<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hongsup Shin">
<meta name="dcterms.date" content="2025-02-01">
<meta name="description" content="The mainstream hardware research for bug discovery uses reinforcement learning (RL) algorithms, but productionizing RL applications still face many challenges. As an alternative, I propose a deployment-friendly approach using learning-to-rank algorithm that improves bug discovery rate.">

<title>Learning-to-rank for hardware bug discovery – Hongsup Shin</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Learning-to-rank for hardware bug discovery – Hongsup Shin">
<meta property="og:description" content="The mainstream hardware research for bug discovery uses reinforcement learning (RL) algorithms, but productionizing RL applications still face many challenges. As an alternative, I propose a deployment-friendly approach using learning-to-rank algorithm that improves bug discovery rate.">
<meta property="og:image" content="https://hongsupshin.github.io/posts/2025-02-01/Fig1.png">
<meta property="og:site_name" content="Hongsup Shin">
<meta property="og:image:height" content="584">
<meta property="og:image:width" content="1022">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Hongsup Shin</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../publications.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hongsupshin"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hongsupshin/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Learning-to-rank for hardware bug discovery</h1>
                  <div>
        <div class="description">
          The mainstream hardware research for bug discovery uses reinforcement learning (RL) algorithms, but productionizing RL applications still face many challenges. As an alternative, I propose a deployment-friendly approach using learning-to-rank algorithm that improves bug discovery rate.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ML</div>
                <div class="quarto-category">verification</div>
                <div class="quarto-category">Learning-to-rank</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hongsup Shin </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 1, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="ml-for-efficient-hardware-verification" class="level2">
<h2 class="anchored" data-anchor-id="ml-for-efficient-hardware-verification">ML for efficient hardware verification</h2>
<p>One of the main challenges of hardware verification is finding bugs efficiently in a design. Since verification is resource-intensive in hardware engineering, ML for hardware verification is an active area of research. The mainstream idea in research is using reinforcement learning (RL) even though it is challenging to productionize RL.</p>
<p>Instead, my team at Arm developed an ML application based on a simpler approach: supervised learning with recommendation (<a href="#fig-1" class="quarto-xref">Figure&nbsp;1</a>). The model uses binary fail/pass labels during training, and predicts the probability of a test candidate being a bug. In production, the application makes batch prediction of a set of candidates and ranks the prediction scores so that engineers run only a subset (top-K) while discovering similar number of bugs.</p>
<div id="fig-1" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Fig1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="450">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Schematics of the default constraint random verification (CRV) flow (left) and the ML application (right) that recommends test candidates based on model prediction.
</figcaption>
</figure>
</div>
</section>
<section id="objectives" class="level2">
<h2 class="anchored" data-anchor-id="objectives">Objectives</h2>
<p>Since verification is an exploration problem, its success is measured by the number of <em>novel</em> bugs discovered. Each verification test returns a binary label: fail or pass. When a test fails, it returns a failure signature (often called a <em>unique fail signature</em>, or <strong>UFS</strong>), which summarizes the cause of the failure. The business objective of bug discovery can be expressed mathematically as maximizing cardinality of the fail signature set <span class="math inline">\(\{s_i\}\)</span> from <span class="math inline">\(K\)</span> test candidates:</p>
<p><span id="eq-ufs"><span class="math display">\[
\left|\bigcup_{i=1}^{K} \{s_i\}\right|
\tag{1}\]</span></span></p>
<p>So far we have been addressing this in a binary classification framework where we use the fail (1) and pass (0) labels. This is based on empirical findings which suggest positive correlation between the number of failures and the number of UFS. In other words, in the binary classifier, we aim to maximize the number of failures given <span class="math inline">\(K\)</span> test candidates:</p>
<p><span id="eq-fail"><span class="math display">\[
\sum_{i=1}^{K} \mathbb{1}_{y_i = 1}
\tag{2}\]</span></span></p>
</section>
<section id="shortcomings-of-binary-classification" class="level2">
<h2 class="anchored" data-anchor-id="shortcomings-of-binary-classification">Shortcomings of binary classification</h2>
<p>Once we deployed the classifier, we unfortunately started observing frequent fluctuations in model performance, often with suboptimal outcomes. After model inspection, we learned that the training process was often dominated by frequently-occurring failures. It turns out the fail signature frequency distribution has a long tail, indicating that only a small number of fail signatures dominate the failure-label space. Therefore, the model learned patterns from a small number of failure signatures.</p>
<p>From business perspective, this is problematic because frequently-occurring failures are already well known to verification engineers. These bugs are also not caused by design but other factors such as testing infrastructures. In other words, when the model focuses on these failures, our application delivers low value and may even risk misclassifying rare (more valuable) failures as passes, failing to prioritize them.</p>
</section>
<section id="learning-to-rank-bugs-by-their-rarity" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-rank-bugs-by-their-rarity">Learning to rank bugs by their rarity</h2>
<p>When I was looking into ways to fix this model behavior and have it focus on rare failures (bugs), I learned about <strong>learning-to-rank (LTR)</strong> algorithms, a family of supervised learning algorithms that learn to generalize the ranking of samples. In a typical LTR dataset, labels are often integers that represent relative relevance of samples in a dataset. Their loss function usually focuses on the top-K elements, and tries to optimize an information retrieval (IR) metric. One of the most widely used IR metric is normalized discounted cumulative gain (NDCG), the normalized version of DCG, which is defined as:</p>
<p><span id="eq-dcg"><span class="math display">\[
\text{DCG}_K = \sum_{i=1}^{K} \frac{2^{rel_i} - 1}{\log_2(i + 1)}
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(rel_i\)</span> represents the relevance label of the <span class="math inline">\(i\)</span>th sample.</p>
<p>Inspired by this, I formulated our objective as a ranking problem. I created a new labeling function that returns integer relevance labels by thresholding fail signature frequency. Failed tests whose signature frequency is less than <span class="math inline">\(M\)</span> times were considered <strong>rare</strong>. Other failed tests were considered <strong>common</strong> failures, and passing tests <strong>irrelevant</strong>. The LTR model learns to rank samples based on these relevance labels. It prioritizes rare fail signatures first, then common failures, and lastly passing tests. An <em>important modeling assumption</em> here is that there might be a learnable pattern among rare failures.</p>
<p>I soon realized that the gap between optimization metric (model loss) and target metric (<a href="#eq-ufs" class="quarto-xref">Equation&nbsp;1</a>) is reduced with the LTR model compared to the binary classifier (<a href="#tbl-comparison" class="quarto-xref">Table&nbsp;1</a>). In addition to not incorporating failure signatures, the binary classifier neither learns ranking directly nor only focuses on top-K during training. The LTR model address both and can return higher cardinality by prioritizing rare signatures.</p>
<div id="tbl-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Loss comparison between the classification and the LTR regarding the target metric (KPI).
</figcaption>
<div aria-describedby="tbl-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Target metric key characteristics</th>
<th style="text-align: left;">Target metric addressed by classifier loss?</th>
<th style="text-align: left;">Target metric addressed by LTR loss?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Cardinality</td>
<td style="text-align: left;"><strong>No</strong></td>
<td style="text-align: left;"><strong>Partially Yes</strong>: Rarity prioritization leads to higher cardinality</td>
</tr>
<tr class="even">
<td style="text-align: left;">Fail signatures</td>
<td style="text-align: left;"><strong>No</strong>: Uses binary labels</td>
<td style="text-align: left;"><strong>Yes</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ranking</td>
<td style="text-align: left;"><strong>No</strong></td>
<td style="text-align: left;"><strong>Yes</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Top-K</td>
<td style="text-align: left;"><strong>No</strong>: Uses all samples</td>
<td style="text-align: left;"><strong>Yes</strong></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="data-augmentation-with-simulated-ltr-groups" class="level2">
<h2 class="anchored" data-anchor-id="data-augmentation-with-simulated-ltr-groups">Data augmentation with simulated LTR groups</h2>
<p>A typical LTR dataset has a <strong>group</strong> (or query) variable that indicates the group membership of a given sample row. During LTR model training, the loss is calculated per group, and its aggregate is used for model updates. It’s like we want to teach a search query model to prioritize similar types of items across multiple different queries. It is technically possible to create a single group that contains the entire training dataset. However, this is much more challenging in terms of both loss optimization and compute. Besides, sometimes it’s not supported by model libraries. For instance, the <a href="https://github.com/microsoft/LightGBM/blob/master/src/metric/dcg_calculator.cpp#L17">lightgbm</a> package limits the group size to 10k.</p>
<p>In the verification training data, there are a few candidate features to group the samples. However, I decided to adopt a simpler approach: bootstrapping. I created an augmented training dataset with <span class="math inline">\(m\)</span> groups where each group contains <strong>ALL rare failures, a subset of common failures, and a subset of passes</strong> where the subsets are bootstrapped independently across all groups. I chose this particular method of guaranteeing every group to have all rare failures because the model should learn to prioritize these over the rest. The details of this process is shown below:</p>
<section id="input" class="level4">
<h4 class="anchored" data-anchor-id="input">Input</h4>
<p><span class="math display">\[
\begin{aligned}
&amp; D_{train}: \text{Original training dataset} \\
&amp; F_{rare} \subset D_{train}: \text{Set of rare failures} \\
&amp; F_{common} \subset D_{train}: \text{Set of common failures} \\
&amp; P \subset D_{train}: \text{Set of passes} \\
&amp; k_{common}: \text{Sampling size of common failures} \\
&amp; k_{pass}: \text{Sampling size of passes} \\
&amp; m: \text{Number of groups} \\
&amp; s: \text{Fixed size for each group where } s = F_{rare} + k_{common} + k_{pass}
\end{aligned}
\]</span></p>
</section>
<section id="output" class="level4">
<h4 class="anchored" data-anchor-id="output">Output</h4>
<p><span class="math inline">\(D_{aug}\)</span>: Augmented training dataset</p>
</section>
<section id="procedure" class="level4">
<h4 class="anchored" data-anchor-id="procedure">Procedure</h4>
<ol type="1">
<li>Initialize <span class="math inline">\(D_{aug} \gets \emptyset\)</span> with size <span class="math inline">\(s\)</span></li>
<li>For <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(m\)</span>:
<ul>
<li><span class="math inline">\(D_{aug} \gets F_{rare}\)</span> (Include all rare failures)</li>
<li><span class="math inline">\(D_{aug} \gets D_{aug} \cup \text{RandomSample}(F_{common}, k_{common})\)</span></li>
<li><span class="math inline">\(D_{aug} \gets D_{aug} \cup \text{RandomSample}(P, k_{pass})\)</span></li>
</ul></li>
<li>Return <span class="math inline">\(D_{aug}\)</span></li>
</ol>
<p>This bootstrapping process naturally results in <strong>bagging (bootstrap aggregation) effect</strong>. The training dataset now has synthetic bootstrapped groups, and the LTR model has to learn to prioritize rare failures over different combinations of common failures and passes.</p>
</section>
</section>
<section id="benchmarking-with-production-datasets" class="level2">
<h2 class="anchored" data-anchor-id="benchmarking-with-production-datasets">Benchmarking with production datasets</h2>
<p>With this new LTR model, I conducted a benchmarking experiment to compare the model performance of the existing binary classifier and the new model. I used about 40 production datasets from a CPU project from last year. For a thorough model comparison, I used the following four metrics:</p>
<ol type="1">
<li>Number of failures (<a href="#eq-fail" class="quarto-xref">Equation&nbsp;2</a>): Binary fail/pass</li>
<li>Number of <strong>rare</strong> failures: failures whose relevance label is the largest <span id="eq-rare"><span class="math display">\[
\sum_{i=1}^{K} \mathbb{1}_{y_i = \max(rel)}
\tag{4}\]</span></span></li>
<li>Number of unique fail signatures (<a href="#eq-ufs" class="quarto-xref">Equation&nbsp;1</a>): Measure of signature set cardinality</li>
<li>Number of <strong>never-seen</strong> fail signatures: Number of signatures that are newly discovered in the test set but NOT observed in the train set, <span class="math inline">\(S_{train}\)</span> <span id="eq-neverseen"><span class="math display">\[
\left|\bigcup_{i=1}^{K} \{s_i\} - S_{train}\right|
\tag{5}\]</span></span></li>
</ol>
<p>These metrics allow us to understand different aspects of model behavior. The number of rare failures (<a href="#eq-rare" class="quarto-xref">Equation&nbsp;4</a>) addresses ranking quality most directly. The number of unique fail signatures is our KPI. The number of <strong>never-seen</strong> fail signatures indicates model generalizability.</p>
</section>
<section id="ranking-performance-comparison" class="level2">
<h2 class="anchored" data-anchor-id="ranking-performance-comparison">Ranking performance comparison</h2>
<div id="fig-2" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Fig2.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Mean test-set ranking performance of the binary classification (“Clf-Fail”) and the LTR (“Rank-FS”) models. Scores are calculated from the top-1000 samples.
</figcaption>
</figure>
</div>
<p>The average performance of the four metrics of the two models shows an interesting pattern (<a href="#fig-2" class="quarto-xref">Figure&nbsp;2</a>). Although the classifier is much better at capturing failures than the LTR model (leftmost), when it comes to metrics that consider fail signatures, it was worse than the LTR model. This supports the idea of the large metric gap of the classifier’s model loss and the target metric (KPI) (<a href="#tbl-comparison" class="quarto-xref">Table&nbsp;1</a>).</p>
<div id="fig-3" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Fig3.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Mean test-set ranking performance of the binary classification (“Clf-Fail”) and the LTR (“Rank-FS”) models across different Ks.
</figcaption>
</figure>
</div>
<p>This performance pattern remained consistent when I varied K (<a href="#fig-3" class="quarto-xref">Figure&nbsp;3</a>). By comparing different Ks, it’s possible to measure efficiency improvement by the LTR model. In the “UFS” and “Never-seen fail signatures” plots, the LTR performance at <span class="math inline">\(K=250\)</span> is similar to the classifier performance at <span class="math inline">\(K=1000\)</span>. This suggests that the LTR model may require <strong>75% fewer tests</strong> to produce the same results.</p>
</section>
<section id="measuring-the-bagging-effect" class="level2">
<h2 class="anchored" data-anchor-id="measuring-the-bagging-effect">Measuring the bagging effect</h2>
<div id="fig-4" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Fig4.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Mean effect of the number of bootstrapped groups (<span class="math inline">\(m\)</span>) during data augmentation on model performance (top-1000).
</figcaption>
</figure>
</div>
<p>In the default LTR model version I tried, I created a training dataset with 30 bootstrapped groups. When I varied the number of groups, I observed the impact of bagging on model performance (<a href="#fig-4" class="quarto-xref">Figure&nbsp;4</a>). As I increased the number of groups, the performance generally improved but it had diminishing return after a certain point.</p>
</section>
<section id="extended-model-comparison" class="level2">
<h2 class="anchored" data-anchor-id="extended-model-comparison">Extended model comparison</h2>
<p>To further understand the model behavior, I created several model variants and measured their performance using the same metrics and benchmark datasets.</p>
<section id="separating-the-impact-of-label-change-and-algorithm-change" class="level3">
<h3 class="anchored" data-anchor-id="separating-the-impact-of-label-change-and-algorithm-change">Separating the impact of label change and algorithm change</h3>
<p>In the LTR model, I made two changes. I created a different labeling system using failure signature frequency, and I changed the algorithm from classification to LTR as well. To separate the impact of these changes, I created two model variants:</p>
<ul>
<li>Multi-class classification with relevance labels (“Clf-FS”)</li>
<li>LTR with binary labels (“Rank-Fail”)</li>
</ul>
</section>
<section id="classification-with-frequency-based-label-encoding" class="level3">
<h3 class="anchored" data-anchor-id="classification-with-frequency-based-label-encoding">Classification with frequency-based label encoding</h3>
<p>To further explore the idea of the impact of the new labeling system I used for LTR, I tried the following two classifier variants:</p>
<ul>
<li>Binary classification without “common” failures (“Clf-FS (No 1s)”)</li>
<li>Binary classification by treating “common” failures as passes (“Clf-FS (1&gt;&gt;0)”)</li>
</ul>
<p>I also played with different signature-frequency thresholds to create binary labels. For instance, if the threshold is 50, all failures whose signature frequency is less than 50 are labeled as 1, the rest (failures whose signature frequency is larger than 50 and all passes) are labeled as 0:</p>
<ul>
<li>Binary classification with varying signature-frequency thresholds (“Clf-FS (Bin, &lt;<span class="math inline">\(N\)</span>)” where different Ns are chosen based on varying quantile values)</li>
</ul>
</section>
<section id="classification-with-bootstrapping" class="level3">
<h3 class="anchored" data-anchor-id="classification-with-bootstrapping">Classification with bootstrapping</h3>
<p>To investigate whether bootstrapping can improve classification performance, I created the following variants as well:</p>
<ul>
<li>Multi-class classification with relevance labels and bootstrapped training data (“Clf-FS (m=30)”)</li>
<li>Binary classification with fail/pass labels and bootstrapped training data (“Clf-Fail (m=30)”)</li>
</ul>
</section>
<section id="the-verdict" class="level3">
<h3 class="anchored" data-anchor-id="the-verdict">The verdict</h3>
<div id="fig-5" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Fig5.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Mean test-set ranking performance of the binary classification (“Clf-Fail”) and the LTR (“Rank-FS”) model variants.
</figcaption>
</figure>
</div>
<p><a href="#fig-5" class="quarto-xref">Figure&nbsp;5</a> shows the final model comparison that includes all classifier variants (gray) and LTR variants (light blue) in addition to the existing binary classifier model (black) and the first (default) LTR model (dark blue). Modifying the label encoding slightly improved the performance of classifiers (see “Clf-FS (Bin, &lt;<span class="math inline">\(N\)</span>)” models in the middle), but their performance was still worse than all LTR variants. Interestingly, the LTR model that used binary labels (“Rank-Fail”) was better than the existing binary classifier’s performance, suggesting that direct ranking optimization result in better performance since our key metrics are ranking metrics.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>One of the biggest lessons from this project was the importance of reducing the gap between the business objective and the optimization objective of ML models especially when it is challenging to accommodate the former. By switching from binary classification to learning-to-rank, we achieved similar bug discovery rate with 75% fewer tests, while better identifying rare and never-seen failure signatures. The LTR model’s success in learning patterns among rare failures suggests there may be underlying commonalities in these rare failures that require further investigation.</p>
<p>The lightweight nature of our approach - using only test settings without design features - makes it particularly practical for verification teams. While not attempting to fundamentally solve the verification problem like RL studies, this approach offers a more deployment-friendly solution that allows verification engineers to focus their efforts on more challenging bugs. Future work could explore incorporating design features, LTR model fine-tuning, and using test output features for ranking.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/hongsupshin\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>