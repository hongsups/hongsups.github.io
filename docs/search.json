[
  {
    "objectID": "posts/2020-09-22-arm-rsh-blog-verification/index.html",
    "href": "posts/2020-09-22-arm-rsh-blog-verification/index.html",
    "title": "Efficient bug discovery with ML for hardware verification",
    "section": "",
    "text": "Imagine designing a highly complex machine. In order to be certain that it functions as its design specifies and does not have any bugs, you would need to test every aspect of the design exhaustively. If the machine is controlled by a set of knobs that can be turned on and off, this verification process can get exponentially complex. For instance, with a machine that has 100 binary on-off knobs, then 2100 tests need to be run to cover all possible combinations. If we assume that a single test takes one second to run, this equates to 1022 years of testing. For present-day microprocessors, it is even more challenging. There can be thousands or tens of thousands of two-state flip-flops in a single microprocessor. Therefore, it is impossible to verify microprocessor designs exhaustively.\nTo work around this problem, hardware verification engineers use a method called random-constraint testing. This is more sophisticated than simple random adjustment of knobs – it is a hybrid approach of manual control and randomization. Engineers can direct the test behavior to a degree by setting constraints for the tests using adjustable knobs. Once constraints are set, the rest of the verification process depends on randomization; the knobs start subsequent processes that are stochastic, which stimulates various parts of the design. This way, engineers can explore the design randomly under the constraints they have set.\nThis method works well when engineers have not explored the design much. As engineers start exploring a design, they find and fix bugs, and the more they explore, the fewer bugs are left to be fixed. Eventually, it becomes very difficult to detect these rare bugs by random probing. Most of the hardware verification effort goes into finding the few remaining bugs in the design. In fact, this process is so time-consuming that 60-70% of the compute time spent on hardware development goes into verification.\nIf the random-constraint testing is not good at specifically targeting bugs in the design, what are the alternatives? My team in Arm Research has been working on this problem since last year. We have analyzed our CPU verification data and successfully trained machine learning (ML) models to solve this problem. We deployed an ML application with these models in collaboration with production engineers at Arm, Mark Koob and Swati Ramachandran. Our application uses ML to flag tests that are likely to find bugs. Verification engineers can feed a large group of prospective tests to our application, and then the application returns a subset of them that are likely to find bugs. This way, engineers can focus on these tests only, and reduce the number of tests to run, which eventually saves compute costs.\nCurrently, our application is being used consistently by Arm verification engineers as a complementary tool to their existing workflow. On average, it has been shown to be 25% more efficient than the default verification workflow in terms of finding bugs, and 1.33x more efficient at finding unique bugs. Through our application, engineers were able to find more unique bugs per test run when compared to their existing verification workflow."
  },
  {
    "objectID": "posts/2020-09-22-arm-rsh-blog-verification/index.html#filtering-prospective-tests-using-ml",
    "href": "posts/2020-09-22-arm-rsh-blog-verification/index.html#filtering-prospective-tests-using-ml",
    "title": "Efficient bug discovery with ML for hardware verification",
    "section": "Filtering prospective tests using ML",
    "text": "Filtering prospective tests using ML\n\nNow let us take a deeper look at how our application works. The main challenge of using ML for hardware verification is that everything revolves around random-constraint testing. Not only is the data collected by this method, but also the testing infrastructure is built to optimize this process. The problem is that random-constraint testing consists of two very different parts:\n\nDeterministic knob-control by verification engineers, and;\nNon-deterministic subsequent processes that are random and intractable.\n\nThis means two things. Firstly, the only source of the usable data is the knobs. Secondly, we cannot guide tests directly to explore new design space because the whole process is non-deterministic.\nLet’s first look at the knob data. Here, each sample is a test that was run in the past. It has several hundred knob values (input) and a binary output; bug or bug-free. In our data, bugs were extremely rare (less than 1%). To address this severe class imbalance, we adopt two approaches. Firstly, we train a supervised learning model that computes a probability of having a bug based on a set of knob values. This model detects tests that may expose bugs similar to the previous ones. Secondly, we train an unsupervised learning model that estimates similarity between a new test and the previous tests. If the similarity is low, the test is likely to be novel. Novel knob combinations can probe unexplored design areas and are more likely to expose bugs. In our preliminary results, we found that these models can detect different types of bugs. Since our main goal is to capture as many bugs as possible, we flag a test as a bug provided one of the two models predicts it will be.\nTo avoid the difficulty in guiding test behavior, we choose a filtering approach. We leave the knob values to be generated randomly from the testing infrastructure and filter them afterwards based on ML prediction scores. To do so, we need to provide the ML models with a large group of knob values (test candidates) first. Luckily, this process is computationally cheap. Then, the ML models compute prediction scores (the probability of having a bug) of the candidates. Based on the scores, we select a subset of the candidates that are more likely to find bugs than others."
  },
  {
    "objectID": "posts/2020-09-22-arm-rsh-blog-verification/index.html#deploying-ml-within-existing-random-constraint-testing-infrastructure",
    "href": "posts/2020-09-22-arm-rsh-blog-verification/index.html#deploying-ml-within-existing-random-constraint-testing-infrastructure",
    "title": "Efficient bug discovery with ML for hardware verification",
    "section": "Deploying ML within existing random-constraint testing infrastructure",
    "text": "Deploying ML within existing random-constraint testing infrastructure\nNow that we have trained ML models ready, can we completely replace our existing random-constraint testing flow with our application? The answer is no. The filtering approach, even with the unsupervised learning model, does not completely solve the exploration problem. That is why the existing flow (random-constraint testing) should remain. The random probing can still be useful for exploration to a degree, and can provide new training data for model update. Thus, we propose two parallel pathways; one with the default randomized testing and the other with ML models, where an additional set of test candidates are provided and then only the tests flagged by the models are filtered and run. This way, it is possible to continue collecting novel data from the default flow for exploration while exploiting the data from previous tests via the ML application.\nSome may think that our job is done when the models are delivered, and deployment finally happens. This is not true. For the models, a new journey begins when they are deployed in the production environment, because it is likely that unexpected events are waiting for them. For instance, we have learned that design and testing infrastructure go through daily changes, so test behavior and data generation process may change frequently. This means the models deployed in the beginning do not guarantee good performance as time goes by.\nTo address this, we have conducted research into the optimization of model retraining. We identified how often models need to be retrained and how much data should be used for training. Using this information, we’ve built a retraining module that is automatically invoked periodically or upon verification engineers’ request. During the retraining process, we compare a variety of different models and tune their hyperparameters. This allows for flexibility across changes in data generation processes and various microprocessor designs."
  },
  {
    "objectID": "posts/2020-09-22-arm-rsh-blog-verification/index.html#towards-end-to-end-ml",
    "href": "posts/2020-09-22-arm-rsh-blog-verification/index.html#towards-end-to-end-ml",
    "title": "Efficient bug discovery with ML for hardware verification",
    "section": "Towards end-to-end ML",
    "text": "Towards end-to-end ML\nWhen we talk about ML, the focus is often on algorithms. As I mentioned, however, when it comes to building ML products, the algorithms are only a small part of a much larger pipeline. Especially with scalable deployment in consideration. Starting from data acquisition all the way to deployment and assessment, every step requires attention of data scientists and ML researchers. It is because developing ML products is different from typical software development. A lot of processes are interconnected and data-dependent, which makes them more challenging to test and evaluate.\nAdmittedly, we initially approached the development of this ML application as a “throw it over the wall” type of engagement. After developing the core ML modules (data preprocessing and models), we delivered them to production engineers without much engagement afterwards. After the application was deployed in several projects, we occasionally needed to intervene to deal with unexpected model behavior. We soon realized that not engaging in the deployment process makes it very difficult to solve any issues that happen after model deployment.\nRecently, in collaboration with our colleagues, it has been made easier to make direct contributions to packages. They have developed a Python package for data science and ML, which has become a core part of our ML application. This allowed us to directly contribute to the ML application easily, and also enabled scalable Python in Arm’s internal compute platform. Our new team member, Kathrine (Kate) Behrman has already built a new module to fix an existing problem in model validation. With this momentum, we are more engaged in deployment to make our ML application perform better and more reliable, while exploring new research ideas.\nThrough this process, we learned that getting more involved in ML product development provides many benefits to researchers. First, it makes tech transfer much easier because making direct contributions to deployed products is more frictionless. This also means that we can test our new research ideas easily and measure the effect quickly. In addition, it helps us fix problems efficiently because we have better understanding of how the models are served in deployment. ML products benefit from an evolutionary approach because there is no absolute guarantee that the data or surrounding infrastructure stays the same. Finally, this process naturally brings automation and scalability, which makes our work easily applicable and more impactful."
  },
  {
    "objectID": "posts/2020-09-22-arm-rsh-blog-verification/index.html#next-steps",
    "href": "posts/2020-09-22-arm-rsh-blog-verification/index.html#next-steps",
    "title": "Efficient bug discovery with ML for hardware verification",
    "section": "Next steps",
    "text": "Next steps\nWe are currently working on launching a new version of our ML application to accommodate a new CPU project that has launched recently. We expect that the new version can be used in the early stage of verification, which is new territory to us. At the same time, we are exploring various research areas. For example, explainable AI for feature assessment and counterfactuals, various unsupervised learning techniques to target novel bugs more effectively, methods to design ML-friendly test benches, and other verification-related problems such as assessing coverage. We are also putting efforts into standardizing our workflow and automating existing features. We anticipate that our work will bring more consistent and reliable model performance over time. We also expect that it will showcase a successful tech transfer example for ML deployment that can be applied to solving other engineering problems inside Arm with ML.\nLearn more about our research in our paper presented at DAC (Design Automation Conference) 2020, and please do reach out to me if you have any questions!"
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html",
    "href": "posts/2020-03-01-acm-fat-2020/index.html",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "",
    "text": "ACM Conference on Fairness, Accountability, and Transparency (ACM FAT*) is a computer science conference that aims to bring together “researchers and practitioners interested in fairness, accountability, and transparency (FAccT) in sociotechnical systems” to nurture multidisciplinary approach and collaborations. Fortunately, I was able to attend the conference every year since 2018 when it first started. I attended the entirety of the conference, which includes tutorials, main conference, and the Critiquing and Rethinking Accountability, Fairness and Transparency (CRAFT) session. (FYI, The conference affiliated with ACM in 2019 (Atlanta, GA) and changed its name to ACM FAccT immediately following this year’s conference in Barcelona, Spain.)"
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html#whats-new",
    "href": "posts/2020-03-01-acm-fat-2020/index.html#whats-new",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "What’s new",
    "text": "What’s new\nCompared to the last year’s event, the organizers made several changes to reflect that FAccT topics require a more holistic view rather than a pure computer science one. In addition to the computer science (CS) track, they created tracks dedicated to social sciences and humanities (SSH), and legal scholarship (LAW). They have also added the CRAFT session, inspired by the NeurIPS 2018 Workshop, “Critiquing and Correcting Trends in Machine Learning” (CRACT). The session focused on critiques of the field of FAccT, such as its blind spots, omissions, or alternative possibilities that take a more holistic approach. This is to give voice to those who suffer the impact, daring formats and beyond academia."
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html#tutorials",
    "href": "posts/2020-03-01-acm-fat-2020/index.html#tutorials",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "Tutorials",
    "text": "Tutorials\nThis year, the tutorials are divided into four categories (hands-on, translation, implications, and invited). The hands-on tutorials included explanatory tools (Google’s What-If and IBM’s “AI Explainability 360”) and University of Colorado’s fairness-aware recommendation package called librec-auto. These are useful tools, but I assumed I could learn about these tools on my own from their websites. Instead, I attended translation and implications tutorials where the former includes topics on ethics, law and policy, and the latter is about case studies of FAccT topics in industry. Interestingly, some of the translation tutorials focused on positionality, the social and political context that influences, and potentially biases, a person’s unique but partial understanding of the world. Here is the summary of the tutorials I attended.\n\nExplainable AI in Industry: Practical Challenges and Lessons Learned\nThis tutorial discussed several techniques on how to provide explainability in ML models and introduced several case studies. Generally, there are two approaches to achieve explainable AI. We can build an interpretable model such as linear models or decision trees, or we consider coming up with post-hoc explanation for a given model. We focused on the latter. The post-hoc explanation approach can be rephrased as the “attribution” problem. In other words, we want to know why a model makes a certain prediction and attribute the model decision to the feature of an input. The main technique we discussed, integrated gradients1 address this aspect by interpreting explainability as feature gradient. For instance, if I changed a feature X, and then the target Y has changed, we may consider the change of X as an explanation to change of Y. Integrated gradients are model-agnostic interpretation of this idea. Here, we look for feature regions where model performance changes dramatically (i.e., large gradient) and integrate them to create an explanation. One application example was diabetic retinopathy to predict severity of the disorder in retinal images. The method was able to provide explanation to deep learning model prediction by locating retinal legions. Although this is useful for its model-agnostic approach, it still lacks global explanation.\n\n\nLeap of FATE: Human Rights as a Complementary Framework for AI Policy and Practice\nEven though AI governance has emerged as a hot topic, many are disappointed by the current ethical framework for AI-related problems. The speakers suggest that we can use human-rights based approach to bring more rigorous and specific groundwork for AI governance. Existing treaties or resolutions such as Universal Declaration of Human Rights (UHDR) or International Covenant on Civil and Political Rights (ICCPR) are good examples. Adopting a human rights perspective for AI governance provides several benefits. First, these are well-established universal standards that have been existing for many decades. Second, they have more currency than ethics, and thus provide better accountability especially when these are violated (i.e., violation to human rights). Since these resolutions are much more specific than ethics, it is possible to make human rights impact assessment of AI technology and companies who own it.\n\n\nTwo computer scientists and a cultural scientist get hit by a driver-less car: Situating knowledge in the cross-disciplinary study of F-A-T in machine learning\nThis tutorial focused on positionality and attempted to give audience first-hand experience of how researchers’ perspectives and approaches can differ across many disciplines. After a brief introduction, we were split into several groups and read three papers all from different disciplines such as computer science, social science and philosophy. The computer science paper was about identifying racial bias in online comments. The social science paper discussed intricate relation between racial minority and their particular usage of certain words in English. In addition to using several questionable methods, the authors from the first paper clearly lacked domain knowledge. Thus, the conclusion of this paper inevitably made a blanket statement on the topic, which was inaccurate and ignorant. It was a good hands-on exercise on the importance of domain knowledge and collaboration across multiple disciplines, which is usually required in FAccT-related studies."
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html#main-conference",
    "href": "posts/2020-03-01-acm-fat-2020/index.html#main-conference",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "Main conference",
    "text": "Main conference\nThere were three keynote talks and 15 sessions of talks for accepted papers. These sessions are grouped into the following topics:\n\nAccountability\nExplainability\nAuditing/Assessment\nFairness\nEthics and Policy\nValues\nData collection\nCognition and education\nSensitive attributes\n\n\nKeynote\nThey were given by Ayanna Howard from Georgia Institute of Technology, Yochai Benkler from Harvard Law School, Nani Jansen Reventlow from Digital Freedom Fund, which supports partners in Europe to advance digital rights through strategic litigation. Howard’s talk was about human bias in trust in AI. She provided examples from her experiments using robots and how human participants interacted with them. In one of her experiments, she found that humans can overly trust AI decision (a robot’s decision in the experiment). She mentioned that understanding cognitive bias is crucial when designing an AI system. Benkler’s talk was on the role of technology in political economy. He discussed how technology has been contributing to the increase of productivity but also aggravating the concentration of wealth, which led to more severe economic inequality. Finally, Reventlow discussed how we can sometimes use litigation strategically to make industry and government more accountable. She mentioned that the best way to build a strategic litigation case is to make the litigation itself embedded in a broader case and to bring various stakeholders such as activists and policymakers together.\n\n\nAccountability\nThe first paper2 criticizes that the term “algorithmic accountability” is inherently vague and thus it needs specification in terms of “the actor, forum and forum, the relationship between the two, the content and criteria of the account, and finally the consequences which may result from the account.” Raji et al.3 also mentioned that due to this vagueness, there is a gap in algorithmic accountability. The authors showed ways to overcome this by illustrating how other industries such as aerospace, medical devices, and finance conduct audit and carry out governance. Finally, Katell et al.4 gave an actual example where researchers co-developed algorithmic accountability interventions (“Algorithmic Equity Toolkit”) by adopting participatory action research methods in collaboration with American Civil Liberties Union of Washington (ACLU-Washington).\n\n\nExplainability\nThere were many interesting papers in this section. Sokol and Flach5 provided convenient yet exhaustive explainability fact sheets for project assessment. Malgieri and Kaminski6 criticized that even though explainability is addressed in GDPR, there is no consensus on what it entails. They also made an excellent point that explanation does not necessarily provide justification on fairness. Barocas et al.7 criticized the faulty assumptions often made when models provide explainability or actionability. For instance, to change the model prediction of an individual, the model may make suggestion to the person to make more money, which is not exactly easily actionable. They suggest that there should be fiduciary obligations on explainability, and we should study how people actually respond to the explanations. Lucic et al8 created Monte Carlo Bounds for Reasonable Predictions (MC-BRP), which generates feature values that would result in a reasonable prediction (i.e. a range of predicted values), based on the n most important features, and general trends between each feature and the target variable, such as whether the target value increase with a certain feature. Bhatt et al.9 conducted a survey on how industry handles explainability in deployment. Their results showed that explainability is mostly used for debugging and the goals of explainability are not clearly defined. Some ML engineers they talked to also addressed that there is technical challenge in real-time deployment for explainability. Hancox-Li10 discussed so-called “Rashomon effect” where multiple models of similar performance provide different explanations, which makes difficult for stakeholders to choose which explanation to believe.\n\n\nAuditing/Assessment\nBlack et al.11 presented a model-agnostic tool that can be used in the exploration stage to identify discriminatory behavior in ML systems. Their concept is roughly based on counterfactuals but they addressed that coming up with a right causal diagram is often challenging. Instead, they used a technique called an optimal transport map, which transforms one probability distribution into another, while minimizing a given cost defined over their respective supports. For example, we might use an optimal transport map from the distribution of men to women in order to obtain a (female, male) pair of inputs with which to query the model. If the model’s output differs for these two people, then it may be evidence that the model discriminates on the basis of gender. Rodolfa et al.12 did a case study in collaboration with a local government to show how to balance equity and model performance depending on which fairness metric to choose and how it affects certain demographics. Lum13 audited a pre-trial risk assessment tool from San Francisco, CA, and found that overbooking (i.e., booking charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision. Sánchez-Monedero et al.14 criticized that some automated hiring systems developed in the US were used in the UK and other European countries where the transparency and data protection law background is quite different from the US. Borradaile et al.15 investigated how the Corvallis (Oregon) Police Department has used social media to monitor certain users and found that they have outsourced the task and racial bias existed among those who were monitored.\n\n\nFairness\nSlack et al.16 developed Fair-MAML, which demonstrates K shot learning for cases where the number of training samples is extremely small. Barabas et al.17 introduced a concept of “studying up” commonly used in the field of anthropology, which refers to studying “the relative upper hand” in terms of the amount of agency and authority they have in a given context such as established legal systems or law enforcement. Pujol et al.18 showed a study of conflict between differential privacy and fairness, and suggested to customize privacy implementation for each assignment instead of using differential privacy for all tasks. Liu et al.19 looked at the complicated interplay between the deployed model for automated decision making in hiring, and individuals who are affected by the model. Because the model’s decision rewards people disproportionately, people tend to change their behavior in response to how these decisions are made. The authors showed that subsidizing the cost of investment create better equilibria for the disadvantaged group. Yang et al.20 examined the ImageNet dataset and found problematic behavior in how “person” category was constructed, for instance, due to lack of diversity in images and bias in annotation. Binns21 investigated the notion of individual and group fairness being in conflict and clarified it being based on a misconception, which requires resolution from a broader context and by focusing on the sources of unfairness.\n\n\nEthics and Policy\nWashington and Kuo22 found that AI ethics codes from corporations conflated consumers with society and were largely silent on agency. They introduced the concept of digital differential vulnerability to explain disproportionate exposures to harm within data technology and suggest recommendations for future ethics codes. Bietti23 discussed a current trend in tech industry where self-regulation or hands-off governance prevails and “ethics” is increasingly identified with technology companies’ self-regulatory efforts with shallow appearances of ethical behavior, so-called “ethics washing.” Abebe et al.24 states that computational research has valuable roles to play in addressing social problems.\n\n\nValues\nDotan and Milli25 the rise and fall of certain model types. They argue that the rise of a model- type is self-reinforcing and the way model-types are evaluated encodes loaded social and political values such as centralization of power, privacy, and environmental concerns. Venkatasubramanian and Alfano26 criticizes the current conceptualization of algorithmic recourse, the systematic process of reversing unfavorable decisions by algorithms and bureaucracies across a range of counterfactual scenarios, and suggests both stakeholder and expert panels should establish acceptable action sets (i.e., not one-way), make a role for fiduciaries who are charged to act on behalf of those they represent, and handle the changes over time (i.e., ongoing engagement).\n\n\nData collection\nIn this section, various authors criticized that the current data collection practices in industry and governments lack transparency and standardized approach. Geiger et al.27 investigated human- annotated datasets widely used in the ML community and found that the so-called benchmark datasets have questionable reliability in the first place. Marda and Narayan28 investigated predictive policing system in New Delhi to discover the lack of public accountability mechanisms, biases that are present within Delhi Police’s data collection practices and how these translate into predictive policing.\n\n\nCognition and education\nBates et al.29 collected data from MSc Data Science teaching team based at University of Sheffield to reflect on their experiences of working at the intersection of disciplines such as FATE (i.e., FAT + ethics). Based on their findings, they suggest to create empathetic learning spaces for interdisciplinary teaching teams and collaborate with decolonization experts to avoid Eurocentrism with data science competency in mind.\n\n\nSensitive attributes\nBogen et al.30 brings up the topic of data collection on sensitive attributes such as race and gender specifically for interventions for antidiscrimination. They illustrated case studies from several industry domains such as credit, employment, and healthcare, and found that the practices widely varies across the domains and also within in certain cases, such as whether the companies are bound by legal restrictions or they can collect self-reported identification as data. The paper encourages various stakeholders to actively help chart a path forward that takes both policy goals and technical needs into account."
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html#craft-sessions",
    "href": "posts/2020-03-01-acm-fat-2020/index.html#craft-sessions",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "CRAFT sessions",
    "text": "CRAFT sessions\nCRAFT stands for Critiquing and Rethinking Accountability, Fairness and Transparency (CRAFT) session. These are designed to bring more diverse voices on FAT* topics. I attended the following sessions.\n\nWhen Not to Design, Build, or Deploy\nGiven the recent push for moratoria on facial recognition, protests around the sale of digital technologies, and the ongoing harms to marginalized groups from automated systems such as risk prediction, a broader discussion around how, when, and why to say no as academics, practitioners, activists, and society, seems both relevant and urgent.\nSome of the notable topics the panel discussed include more case studies on the unnecessary surveillance driven by AI systems, raising tech workers’ agency (accompanied with ethics training for engineers), and making interventions at various points in a production pipeline, and so on. When it comes to how we would formalize the effort in terms of policy, some mentioned that engineers can potentially make released models more difficult to replicate, societies can apply similar regulations to AI as they do to any weapons, and governments can bring legal enforcement on the issue.\n\n\nCentering Disability Perspectives in Algorithmic Fairness, Accountability & Transparency\nIt is vital to consider the unique risks and impacts of algorithmic decision-making for people with disabilities. The diverse nature of potential disabilities poses unique challenges. Many disabled people choose not to disclose their disabilities, making auditing and accountability tools particularly hard to design and operate. Further, the variety inherent in disability poses challenges for collecting representative training data in any quantity sufficient to better train more inclusive and accountable algorithms.\nSpeakers mentioned that disability perspectives combine AI systems design and accessibility issues. This is similar to the lack of diversity issue in training data in face recognition. If deployed AI systems cannot recognize disabled people. This requires ML engineers to consider whom their models might fail for and consider inclusive and viable solutions in advance. The disabled community also has intersectionality because how disabled are represented in the media and society do not reflect the diverse spectrum of the disabled community. This is also related to that various demographic groups have different socioeconomic and genetic backgrounds that might affect the probability of them to become disabled."
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html#reflection-and-suggestions",
    "href": "posts/2020-03-01-acm-fat-2020/index.html#reflection-and-suggestions",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "Reflection and suggestions",
    "text": "Reflection and suggestions\nOverall, it was nice to see the conference growing rapidly every year. I appreciated the organizers’ move to create separate tracks for social science and law, to encourage more holistic views in research. I enjoyed the CRAFT sessions very much. Due to the interdisciplinary nature of the FAT* topics, sometimes I sensed that the one-way communication of the talk format has severe limitations on how we move further from here. The CRAFT sessions were the attempts to mitigate this issue and also bring the ground workers such as activists, lawyers, policy makers, industry data scientists together.\nI also see there are room for improvements. First of all, every talk was strictly limited to 8 minutes, which was too short. Especially when it comes to computer-science papers, some speakers showed many math equations on their slides, which is probably not the best strategy for the 8-min talk. Some speakers did a great job of laying out the background knowledge but not all of them. If we are truly interested in multidisciplinary effort, improving the communication is crucial.\nSpeaking of communication, at the very end of the conference, there was a townhall meeting and the Q&A was extremely short. I think this should be more focused because we are a group of various backgrounds and to make impactful changes from our research, it is important to open the floor for more discussions. This can be potentially mitigated by organizing birds-of- feather (BOF) sessions or group-lunch for certain topics. The conference is growing fast, which means we will have a lot of newcomers every year. It would be better for them to have easy-to- network venues.\nThe conference this year used an anonymous board to submit questions online and did not use their Slack channel much. I understand the need for anonymity but the Slack channel was extremely useful in terms of establishing networks between researchers from different domain and socializing. Tutorial preparation could have also been announced on the Slack channel as well like last year. There were some tutorials where the attendees didn’t get enough information about the preparation and it was a missed opportunity.\nThis issue was brought up last year but the conference may want to attempt to create a shared knowledge base to educate the attendees to have everyone on the same page. There are many definitions of key terms such as fairness, and there are important historic case studies researchers talk about all the time. It would be nice to have standardized material that is open to all attendees before the conference so that we can all well-informed and follow the conference material more easily.\nThis might be a radical and debatable idea, but if the conference is truly interested in tackling practical and more current issues related to the FAT* topics, they might consider bringing more industry people, especially the ones who are often criticized by various researchers to make them more accountable and help them create a better solution, instead of ostracizing them. Not all companies may participate but considering the urgency of the issue, it wouldn’t do any harm for us to reach out to industry and bring them to the table. This will also provide opportunities for researchers as well if the industry stakeholders share their difficulties of implementing good AI governance in practice, and the researchers can help tackling them."
  },
  {
    "objectID": "posts/2020-03-01-acm-fat-2020/index.html#references",
    "href": "posts/2020-03-01-acm-fat-2020/index.html#references",
    "title": "Critiquing and rethinking at ACM FAT 2020",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html",
    "title": "Tutorials at FAccT 2021",
    "section": "",
    "text": "Last year during the SciPy 2020 conference, I participated in a mentoring program and I’ve got to know a fellow data scientist, Henrik Hain. He diligently uploaded a daily update during the conference, which was impressive. Inspired by him, I’ve decided to follow his practice and to write notes during my attendance in the FAccT 2021 this year."
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html#conference-overview",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html#conference-overview",
    "title": "Tutorials at FAccT 2021",
    "section": "Conference Overview",
    "text": "Conference Overview\nThe conference is held virtually this year for the obvious reason. It’s on a platform called circle.so. Everything happens in one platform; video streaming, general announcement, community boards, internal messaging, etc., which is pretty convenient.\nAs an international virtual conference, the schedule is a bit brutal. For my time zone (UTC-6), the schedule starts at 6 am and lasts until 4-7 pm in the evening. Talks in the main session are pre-recorded and thus attendees can watch at their convenience but some sessions occur in real time. Luckily, the conference organizers set up dedicated watching times for the main session, which I really appreciate.\nToday was about tutorials. I attended the following: - Causal Fairness Analysis - Explainable ML in the Wild: When Not to Trust Your Explanations - Responsible AI in Industry: Lessons Learned in Practice - A Behavioral and Economic Approach to Algorithmic Fairness & Human-Centered Mechanism Design - How to Achieve Both Transparency and Accuracy in Predictive Decision Making: an Introduction to Strategic Prediction\nEach tutorial was about 90 minutes, which was a bit short for me. Since FATE (fairness, accountability, transparency, and ethics) topics are highly diverse, it makes sense to have many different tutorials. But because they were so short, they weren’t much of a deep-dive and an overview (at least for the ones I attended)."
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html#causal-fairness-analysis",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html#causal-fairness-analysis",
    "title": "Tutorials at FAccT 2021",
    "section": "Causal Fairness Analysis",
    "text": "Causal Fairness Analysis\nThis tutorial was about conducting fairness analysis through causal framework by using structural causal models (SCMs). The speaker (Elias Bareinboim) started the talk by mentioning that the burden of proof is on the plaintiff in discrimination lawsuit, meaning they have to prove the causal connection if they were treated unfairly. I found this emphasis and motivation from legal lense quite refreshing.\nThe material was relatively easy to follow in the beginning where Elias explained SCM models and how we evaluate them with given empirical data. However, it got a bit challenging to follow where we started combining fairness and causal models, especially given that fairness metrics are so diverse.\nFor me the most interesting part was where Elias compared various counterfactual scenarios. I’ve always assumed that the causal DAGs will not change when we switch the group membership to simulate counterfactuals, but obviously there is no guarantee. It’s possible that we can have indirect and spurious effects for counterfactual scenarios and he explained that we need to subtract those effects from direct effects."
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html#explainable-ml-in-the-wild-when-not-to-trust-your-explanations",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html#explainable-ml-in-the-wild-when-not-to-trust-your-explanations",
    "title": "Tutorials at FAccT 2021",
    "section": "Explainable ML in the Wild: When Not to Trust Your Explanations",
    "text": "Explainable ML in the Wild: When Not to Trust Your Explanations\nThis tutorial consisted of three parts: overview of explainable AI (XAI) methods, their limitations and ethical/practical challenges. I found the second part most interesting.\nThe speaker (Chirag Agarwal) introduced four aspects of XAI limitations: faithfulness, stability, fragility, and evaluation gap. Faithfulness refers to whether explanations change when models changes (and perhaps also the Rashomon effect). Stability refers to whether post-hoc explanations are unstable with respect to small non-adversarial input perturbation. For instance, some evidence shows that LIME explanations may change if we change random seed in certain ML algorithms. We also wouldn’t want our models to chance explanations based on hyperparameters.\nFragility is about whether explanation changes according to data drift in input space. This is closely related to adversarial attack on explanations (i.e., whether small perturbation can change explanation without changing prediction). Finally, in general, it is very difficult to properly evaluate XAI methods and currently there is no ground truth for evaluation.\nPerhaps because of these problematic features, the case studies presented in the following session felt very nuanced and complicated. As I’ve seen in other responsible AI talks, XAI methods are closely related to human-in-the-loop systems and the level of trust in human end users."
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html#responsible-ai-in-industry-lessons-learned-in-practice",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html#responsible-ai-in-industry-lessons-learned-in-practice",
    "title": "Tutorials at FAccT 2021",
    "section": "Responsible AI in Industry: Lessons Learned in Practice",
    "text": "Responsible AI in Industry: Lessons Learned in Practice\nThis talk consisted of two parts: overview of responsible AI tools and related case studies. These tools are useful for model monitoring/inspection, generating explanations, fairness mitigation, error analysis, and counterfactual analysis. I was surprised that there are already several open-source tools available such as InterepretML, Fairlearn, What-If Tool.\nThe live demo from Microsoft was impressive but it was too fast to follow (I personally think Jupyter notebook isn’t the best method for presentation if we have to go back and forth a lot). Also I was not sure whether the examples were from deployed projects or toy datasets especially since the speaker talked about fairness mitigation and I was curious about how the full cycle of mitigation process looked like.\nThe case study from LinkedIn at the end was interesting (especially since they presented similar material last year at the same conference) but it felt somewhat disconnected for the same reason I just mentioned above; I wasn’t sure how human end users were involved in the fairness mitigation process."
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html#a-behavioral-and-economic-approach-to-algorithmic-fairness",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html#a-behavioral-and-economic-approach-to-algorithmic-fairness",
    "title": "Tutorials at FAccT 2021",
    "section": "A Behavioral and Economic Approach to Algorithmic Fairness",
    "text": "A Behavioral and Economic Approach to Algorithmic Fairness\nThis talk was about looking at the fairness problem from economic perspective. Different from the traditional computer science approach, this talk suggested that economic approach presents the fairness problem in the form social welfare functions where social planner can optimize efficiency (expected outcome of interest among groups) and equity.\nWhat was interesting was that the optimal algorithm isn’t just about prediction function but also about admission rule, meaning how social planner can use the predictions to make decisions. Normally, these admission rules are threshold-based; we make a decision on certain group of individuals based on certain thresholds, which is affected by equity preference of social planner. Another interesting aspect was that this equity preference can even affect the prediction function because decision makers who prefer discrimination may want to use additional features from data to discriminate protected groups."
  },
  {
    "objectID": "posts/2021-03-04-facct-2021-tutorial/index.html#how-to-achieve-both-transparency-and-accuracy-in-predictive-decision-making-an-introduction-to-strategic-prediction",
    "href": "posts/2021-03-04-facct-2021-tutorial/index.html#how-to-achieve-both-transparency-and-accuracy-in-predictive-decision-making-an-introduction-to-strategic-prediction",
    "title": "Tutorials at FAccT 2021",
    "section": "How to Achieve Both Transparency and Accuracy in Predictive Decision Making: an Introduction to Strategic Prediction",
    "text": "How to Achieve Both Transparency and Accuracy in Predictive Decision Making: an Introduction to Strategic Prediction\nThe whole idea of strategic prediction is very interesting because it’s about human users trying to game the system if they start understanding the rule of the game. For instance, if students learn about the criteria universities use for their admission process, they will try to find ways to cross that threshold so that they can be admitted. This tutorial gave an overview of this phenomenon from the perspective each stakeholder group in the process; institution (algorithm designers), individual (data provider), and society (all people as a whole).\nInstead of doing a deep dive on a specific topic, the tutorial presented various topics in the domain. Recourse and incentivization was one of them. If the deployed algorithm uses a feature that can harm individuals because the feature incentivizes them to behave in a certain way, automated harm at a massive scale will be expected. Another way to look at the strategic prediction is through causality. If we find a true causal relationship between features and model predictions, it might be able to utilize the benefit of strategic prediction from the institution’s perspective and to encourage improvement without encouraging gaming, which will be crucial for modellers."
  },
  {
    "objectID": "posts/2021-03-09-facct-2021-main2/index.html",
    "href": "posts/2021-03-09-facct-2021-main2/index.html",
    "title": "FAccT 2021. Automated decision-making, causal accountability, and robustness",
    "section": "",
    "text": "One of the reasons I try to attend the FAccT conference is because I always get inspired by novel approaches from this community to quantify and measure social concepts such as fairness. However, this approach has its own demise. It’s possible that these attempts get lost in the weeds while discussing abstract and theoretical aspects of the models which can easily alienate researchers from the real world. Today’s keynote by Katrina Ligett from Hebrew University addressed this issue.\nDuring the talk, she discussed the pros and cons of using mathematical models in general, but also in the context of studying FAccT-related topics. Many positive aspects of mathematical models she mentioned were exactly the same reasons why I have been drawn to this area; mathematical models are explicit about their assumptions such as what’s included and what’s not, usually come with provable guarantees, and provide solid foundations for future research. Mathematical models can also easily become a fig leaf by letting bad actors hide behind the models’ specificity and complexity. People also have a misperceived notion on mathematical models that the models are objective (and thus have authority).\nShe talked a lot about this duplicity during the Q&A. It was nice to hear from Katrina that theoreticians have tendency to be drawn to intellectually stimulating problems and sometimes lose touch with the main purpose that our models serve. She also talked about how to bridge the gap between people who build mathematical models and people who do not have the background but are impacted by the models. She first talked about the importance of translational type of work which explains theoretical concepts in real-world meanings. This way, it is going to be much easier to critique models’ basic assumptions where many problems begin.\nShe ended the talk in a positive note by saying that even with the flaws of mathematical models, our approach should not be banning mathematical models but rather encouraging modelers to acknowledge the shared responsibility within the community they serve, to take advantage of the rigorousness built in mathematical approaches to carefully examine the problems, and to constantly critique the models."
  },
  {
    "objectID": "posts/2021-03-09-facct-2021-main2/index.html#automated-decision-making",
    "href": "posts/2021-03-09-facct-2021-main2/index.html#automated-decision-making",
    "title": "FAccT 2021. Automated decision-making, causal accountability, and robustness",
    "section": "Automated Decision-Making",
    "text": "Automated Decision-Making\nReviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems presents an easy-to-follow and straightforward guide for automated decision-making (ADM) system review. Being able to review something means that we can hold companies easily accountable for the consequences from ADM systems, which already have serious issues of lack of accountability. The authors borrowed the idea from administrative law’s approach to reviewing human decision-making, and presents a practical and holistic framework. The suggested idea consists of various parts such as commissioning, model building, decision-making, investigation, group deliberation, and assessment. I liked their idea originated from law because this will help legislators regulate ADMs.\nSpeaking of regulation, An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists was an excellent paper in that it provided an example of algorithmic equity toolkit designed for diverse stakeholders. I assume it was possible because the authors come from diverse background; computer science, social and political science, and activism. The toolkit aims to increase public participation for AI-policy action and consists of various formats including flowcharts, maps, and questionnaires. With this diverse methods, it tries to deliver comprehensive of information of AI technology such as the type, functionality, and its potential impact. This looks like a handy way to facilitate communication between many different stakeholders such as communities, government officials, and even technologists."
  },
  {
    "objectID": "posts/2021-03-09-facct-2021-main2/index.html#accountability-and-recourse",
    "href": "posts/2021-03-09-facct-2021-main2/index.html#accountability-and-recourse",
    "title": "FAccT 2021. Automated decision-making, causal accountability, and robustness",
    "section": "Accountability and Recourse",
    "text": "Accountability and Recourse\nDesigning Accountable Systems suggests that accountability requires a causal model because if something happens and we need to hold someone accountable for the consequences. The authors bring an interesting distinction between forward- and backward-looking causalities. When we build causal models, we use the forward-looking approach such as “A causes B”. However, when we need to use causality for accountability, we lean on the backward-looking approach such as “the fact that A happened last year causes B to die yesterday”. Thus, the authors use Structural Causal Models to evaluate a system’s design for accountability.\nIn a similar context, Algorithmic Recourse: from Counterfactual Explanations to Interventions also emphasizes the importance of causal models. Many previous studies on counterfactuals as a means of algorithmic recourse overlooked how to achieve the counterfactual scenarios. Without addressing the feasibility, algorithms may provide nonsensical counterfactual explanations such as “in order to get a loan, change your ethnicity”. This study shows that causal models can mitigate this issue because causal explanations inherently avoids nonsensical explanations from the beginning.\nFinally, Epistemic Values in Feature Importance Methods: Lessons From Feminist Epistemology looks at explainable AI (particularly feature importance) through the lense of feminism studies, which was highly enlightening. The authors say explanations and epistemology (study of human knowledge) are inseparable; useful explanations have epistemic values. And providing AI explanations in a feminist way is means we focus on understanding of situated knowledge and avoid totalization. To elaborate, explanations should respect different perspectives, particularly from marginalized individuals, and encourage active conversation about the knowledge. If I look back on how I used feature importance as a tool for explainable AI, it is true that it’s usually a one-way communication that was static and lacked communication with stakeholders. The authors suggest that it’s crucial to evaluate explanation in context and keep it interactive. More importantly, since explanations can often be used as an authoritative way, it’s important to encourage users to avoid over-trusting one interpretation, which was such a thoughtful insight."
  },
  {
    "objectID": "posts/2021-03-09-facct-2021-main2/index.html#robustness",
    "href": "posts/2021-03-09-facct-2021-main2/index.html#robustness",
    "title": "FAccT 2021. Automated decision-making, causal accountability, and robustness",
    "section": "Robustness",
    "text": "Robustness\nIt’s well known that proxy features can contain information about sensitive attributes such as race and gender. Often people try to remove these spurious features from the dataset. In fact, this practice is not just limited to applications that use datasets with sensitive attributes. ML practitioners drop correlated or spurious features to build more robust models. Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately shows that removing these features can lead to performance drop. The authors mitigated the problem by using a technique called Robust Self-Training. The authors of this paper also investigated how the performance drop is manifested among different groups in the data.\nFairness Through Robustness: Investigating Robustness Disparity in Deep Learning is an interesting paper that they define the group fairness in terms of model vulnerability against adversarial attacks. They first address that in deep learning models, it is possible for a certain subgroup to be more vulnerable to those attacks, and thus the model is less robust for particular subgroups. You can easily picture this as a classification with a decision boundary. If the model is not robust and changes its decision boundary when the attack occurs, samples that are closer to the original boundary may be more likely to be misclassified with the attack. The authors identified that for deep learning models, this is a complex matter because it is about data drift and the learning process of the model. However, as a first attempt, they added a simple regularization term to the model to equalize the robustness bias among the subgroups in some examples."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html",
    "title": "Tech volunteering tips for nonprofits",
    "section": "",
    "text": "Throughout my career, I have met many tech workers interested in using their technical skills in more meaningful ways. When I started my industry career, I had similar thoughts and luckily I have been able to volunteer at various non-profits for several occasions. Some may say volunteering will require much less effort than paid labor because the work is free but because of that exact reason, more caution is required. Based on my experience, I would like to share some tips especially regarding using ML and data science skills for tech volunteering.\nI gave a talk at Austin Python Meetup in Apr, 2021 About this. If you’re interested, check out the video below."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#general-advice",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#general-advice",
    "title": "Tech volunteering tips for nonprofits",
    "section": "General advice",
    "text": "General advice\nFirst, before delving into specific tech-skill tips, let’s talk about more general aspects of tech volunteering.\n\nLocal than national or international\nThe first thing you’ll have to do is to choose a nonprofit. If you’ve been interested in data science for social good, you are already familiar with an organization such as DataKind. I’ve worked with DataKind a couple of times for small projects and normally you go through a detailed application process to become a volunteer. Since it’s a popular international organization, the application process is competitive and it’s possible that you will get rejected, which also happened to me several time.\nFamous organizations are all good but I’d like to suggest you look around and go for local nonprofits. They often lack resources to afford tech workers which means that you can easily work with them and your work will make a bigger impact. I’ve joined Texas Justice Initiative (TJI) through a local nonprofit Open Austin, a Code for America Brigade which advocates for open government and civic technology. I went to their meetup a couple of times and joined their Slack channel, where I met our TJI director, Eva.\n\n\nLong-term than short-term\nDepending on the situation, you may work on one-off type projects or more long-term ones. After working with TJI for over a year, I am learning that I get to work on more diverse and in-depth projects since I’ve been here more than a year. For instance, now that I have better understanding of backend, I am paying more attention to automating many backend processes. Plus, with newly accumulated data, I also get to update data journalism reports and launch new investigation.\n\n\nDo your homework\nIf you’re a first-timer and your nonprofit-of-interest doesn’t have any existing volunteers, you might feel intimidated. Then find an organization that already has tech volunteers. When I was reading about TJI before I joined, I found that there are a dozen tech volunteers already, which gave me a sense of reassurance. Most nonprofits disclose funding sources and board members information on their websites. This will give you a better idea on whether your values and what they represent are aligned.\n\n\nReasonable expectations\nNonprofits can be like any other workplaces; there will be some you enjoy working with and some you don’t, and there will be times when things are going really well or falling apart. Thus, having realistic and healthy expectations is important."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#ml-and-statistical-inference",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#ml-and-statistical-inference",
    "title": "Tech volunteering tips for nonprofits",
    "section": "1. ML and statistical inference",
    "text": "1. ML and statistical inference\nNow let’s talk about specific tech-skill opportunities. Let’s start with ML and statistical inference first.\n\nPredictive modeling\nFor my police shooting report, I purposefully did not use any ML models because I am aware of how ML tools can generate unintended harm easily when used prematurely (or at all). Whenever I talk to ML practitioners, they often love to devour any datasets and create predictive models. This is extremely dangerous.\nLet’s assume that for whatever reason, we decided to build a predictive model by using the police shooting data. One can potentially use the severity column (whether a person was killed or injured during an incident) as a target and use the rest of the data as predictor variables. Given that most of our data has demographic and location information, this can easily become a bias-reproducing machine.\nBesides, if these models are deployed in public, it’s possible that the models can be used by bad actors to exploit community members who are already suffering from these tragic incidents. For instance, insurance companies might be able to use a model like this and predict how likely people of certain demographics are more likely to be killed by police. Then they can increase health insurance premium for these people.\nBefore even starting a project, I strongly recommend considering and evaluating the risk and ethical concerns first. If the risk is too high, then you shouldn’t build a model. Note that this thoughtful examination requires a diverse set of opinions. Thus, please open the discussion to other members in the organization (and even further) and try to thoroughly examine the risk.\n\n\nOn significance testing\nSome data scientists like computing p-values from their analyses but first and foremost, p-values have many issues (see the official statement from American Statistical Association cautioning the use of p-values). Plus, given that they are usually not the domain experts, it is likely that the hypotheses are not well thought out. Moreover, once you make value statements based on thresholding by using p-values, it is very likely that you will contribute to propagation of misinformation because these statements (e.g., “A is significantly different from baseline”) can be easily cherry-picked and propagated.\n\n\nThen what can I do?\nI am not telling you to drop any statistical inference or ML completely. Rather, I am asking you to recognize the responsibility that comes with using ML techniques. In fact, there are many research papers that use ML in clever ways. For instance, in Gonen and Goldberg’s 2019 paper, Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them, the authors used predictive modeling to show that (gender) de-biased word embeddings still contain information about gender, which proves that the de-biasing technique doesn’t work. In short, we need to be more thoughtful and creative when it comes to ML."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#data-governance",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#data-governance",
    "title": "Tech volunteering tips for nonprofits",
    "section": "2. Data governance",
    "text": "2. Data governance\nIf ML requires more cautious approach, an easier project you can tackle as a data person is to help a nonprofit establish a good data governance practice. Data governance is a practice of documenting what the data contains and how it should be used. It’s a great opportunity to learn about data and help the nonprofit utilize the datasets responsibly.\nThere is an excellent paper, Datasheets for Datasets (Gebru et al., 2018) that can guide you through this process. The paper has an exhaustive list of questions that you can answer to understand a dataset and its potential problems and impact. At TJI, I’ve gone through this process and published a Datasheet on TJI’s custodial death dataset focusing on the dataset’s composition. This was a great exercise for all of us because we were able to discuss some potential caveats such as sampling bias of our dataset. Another template you can follow is Data Protection Impact Assessment from European Union’s General Data Protection Regulation (GDPR). This also asks you to answer questions to understand the impact and ethical concerns that come with the data.\nFinally, once you’ve created data governance documentation, make sure you provide it to users when they consume the dataset so that they are also aware of the information. This creates a chain of responsibility."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#data-journalism",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#data-journalism",
    "title": "Tech volunteering tips for nonprofits",
    "section": "3. Data journalism",
    "text": "3. Data journalism\n\nInteractive dashboards need a context\nInteractive dashboards are useful in that audience can play with the data directly. However, it’s likely that you can’t have comprehensive knowledge on what users might see when they start slicing the data in every possible way. Plus, you still need context to provide a narrative. Personally, I prefer data journalism reports with static images because I can control what audience can see and I can minimize the chance of them misunderstanding the data.\n\n\nFind a reference point\nProviding a context is extremely important. If I don’t know where something stands compared with others, how would I know whether it is better or worse? Luckily, this comparison can be easily done by comparing your data with other reference datasets. In Part 1, I’ve used the US census data and public health data for this. You can see this approach all the time in data journalism reports from major news organizations such as New York Times. Comparing your data with a reference data helps you ask more interesting questions to understand the difference.\n\n\nGet a domain expert review\nIt’s likely that tech workers are not the domain experts. In this case, make sure your work is seen by them before you publish. You can start by sharing your work with other members in the nonprofit. You can also reach out to academics who are most likely very interested in this type of work. For my report, Eva helped me reach out to Howard Henderson of Texas Southern University’s Barbara Jordan-Mickey Leland School of Public Affairs and Center for Justice Research, and Kevin Buckler of University of Houston-Downtown’s Department of Criminal Justice for a review.\n\n\nLeave out any questionable information\nYou might want to use everything that’s in a dataset to write a report. This is not necessarily the best idea, unfortunately. In our police shooting dataset, there is a column called deadly_weapon which shows whether the person shot by police possessed a deadly weapon. It turns out that this information is highly contentious because whether an object is perceived as deadly varies widely. Based on studies and news reports, a BB gun, a butter knife, or a candlestick can be perceived as a deadly weapon. After deliberation, we’ve decided not to use this information because we thought this categorization misrepresents true information. We plan to revisit this after collecting information about actual objects used in the incidents.\n\n\nResearch opportunities\nDatasets gathered by local nonprofits are so specific and unique that they provide interesting research opportunities, which becomes great material for a data journalism report. Currently at TJI, several volunteers and I have been working on identifying systematic pattern in interaction between officers and civilians in police shooting incidents. Jiletta Kubena, a criminologist and a TJI volunteer, has been studying survey results on various populations, such as people in custody, of those who’ve been impacted by COVID-19.\n\n\nTech setup\nYou will be creating same types of plots over and over with small changes depending on how your investigation goes. Thus, it’s better to spend time in the beginning to create modules for plotting. At the same time, ask if your nonprofit already has a specific visual style guide that you can follow. This helps visual consistency."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#sustainable-workflow",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#sustainable-workflow",
    "title": "Tech volunteering tips for nonprofits",
    "section": "4. Sustainable workflow",
    "text": "4. Sustainable workflow\nWhat kind of tech tools and workflow gets to be implemented in a nonprofit heavily depends on their volunteers. However, the volunteer-dependent workflow is quite fragile because volunteers in general have weaker commitment because they are not always readily available. Thus, it’s quite important to help a nonprofit build a sustainable workflow.\nImplementing CI/CD flow is a must and you can even reuse your notebooks to create analytics reports with updated data by using a package such as Papermill. If you have a heterogenous dataset, you can utilize scikit-learn’s ColumnTransformer to build a consistent and robust data preprocessing pipeline. Finally, you can also set up a data validation check by using a tool like Pandera. This will help you monitor and flag any unexpected data drift.\nA sustainable workflow is also more than just the tech side. Recently, at TJI, we’ve implemented a formal review process to improve the quality of our work. We used to review manuscripts on Google Docs but now we review them on GitHub as a pull request. Nick Holden, a software engineer and a TJI volunteer, connected our content management system (CMS) to this process so that people who are not familiar with GitHub can easily create a manuscript. Writing a plain text document on the CMS automatically creates a pull request and other members at TJI can review and exchange feedback transparently."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#community-building",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#community-building",
    "title": "Tech volunteering tips for nonprofits",
    "section": "5. Community building",
    "text": "5. Community building\nAs a volunteer, you can always work by yourself on a one-off project. Even if that’s the case, having a sense of community makes your work more enjoyable and prevents fast volunteer turnover. In this sense, community building is quite important.\nCommunity building can be socializing but it’s also about building robust infrastructure because it reduces unnecessary stress. Given that volunteer-dependent workflow is fragile, and robustness, accountability, and transparency matter even more. As a tech worker, you can help nonprofits adopt good practices from tech workflow such as automation, code review, software engineering practices, and so on. But when bringing a new tech tool, please try to use open source software as much as possible. I’ve seen a case where a tech worker brought their company’s product and left, which made the nonprofit’s entire backend hugely dependent on a niche commercial product. Sometimes nonprofits are approached by private partners. With your corporate experience, you can help the nonprofit protect their asset too.\nImplementing a good practice for collaboration is equally important. You can start by encouraging other volunteers to showcase and highlight their work. I’ve suggested a dedicated blog where all volunteers can write about their work in their own words. This accelerates knowledge transfer within the organization and helps other nonprofits too.\nFinally, I want to emphasize that you should watch out for volunteer burnout. It happened to me multiple times unfortunately. I got super excited and passionate about a subject matter and I ended up with exhausting all my energy in a short period of time. So please pace yourself and take breaks if needed. But when you do so, instead of ghosting please tell others so that they have reasonable expectations while you’re gone."
  },
  {
    "objectID": "posts/2021-05-25-ds-volunteering-part2/index.html#my-personal-experience",
    "href": "posts/2021-05-25-ds-volunteering-part2/index.html#my-personal-experience",
    "title": "Tech volunteering tips for nonprofits",
    "section": "My personal experience",
    "text": "My personal experience\nSome may think volunteering would be using your existing skills without learning. That was never the case for me. It’s been amazing to have opportunities to work with different types of data that I don’t get to see at work. I’ve also got to work on various projects from exercising data assessment practice and writing data journalism reports, to creating custom data visualization modules and transferring ML Ops practices to the existing workflow. I’m looking forward to exploring research opportunities with causal modeling and explainable AI with our dataset in the future.\nBut more importantly, I’ve been lucky to work with our director Eva and all the volunteers who have been kind, cordial, and open to new ideas. It was also fantastic to work with people from different disciplines and to learn from these domain experts. This helped me expand my network and meet other nonprofit organizers, academics, and tech workers. Finally, I’ve learned a lot about local social issues surrounding my neighborhood and my city, which have significant impact on our community members. I am looking forward to many other interesting opportunities in TJI.\nI hope these tips help you start exploring volunteering opportunities. Finally, you are more helpful than you think! So don’t worry about whether you have the right skill sets or not. They will appreciate your knowledge and there will be many things you can contribute to. So if you find any interesting organizations, don’t be afraid to reach out. You will help other people and learn a lot in the process. Good luck!"
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html",
    "title": "Police shooting in Texas 2016-2019",
    "section": "",
    "text": "I have met several tech workers who were interested in using their technical skills in more meaningful projects particularly for social good. As a tech worker myself, I’ve been volunteering at several organizations about two years now. Among those, I have been volunteering at Texas Justice Initiative (TJI). In this post, I’d like to share some of my volunteering at TJI, particularly on police shooting in Texas.\nAs a tech worker, I’ve been volunteering at several organizations about two years now. Among those, I have been volunteering at Texas Justice Initiative (TJI) consistently over a year where I published a data journalism report on police shooting in Texas with our TJI director, Eva Ruth Moravec. Texas Justice Iniative (TJI) is a criminal justice nonprofit in Austin, TX. It was founded in 2016 by Eva Ruth Moravec, a journalist, and Amanda Woog, a researcher. The main goal of TJI is to create data portal for criminal justice in Texas. TJI mostly relies on tech volunteers. We currently have 11 active volunteers, including myself.\nIn this post, I’d like to share some of my volunteering at TJI, particularly on police shooting in Texas. In the follow-up post, I will share some tips on tech volunteering at nonprofits, especially focusing on how to use machine learning and data science knowledge."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#police-shooting-in-the-us-is-a-globally-recognized-public-health-issue",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#police-shooting-in-the-us-is-a-globally-recognized-public-health-issue",
    "title": "Police shooting in Texas 2016-2019",
    "section": "Police shooting in the US is a globally recognized public health issue",
    "text": "Police shooting in the US is a globally recognized public health issue\nIt’s 2021 now and almost everyone in the US is aware that there is an epidemic of police brutality all over the country. Academic researchers now agree that police contact is a significant contributing factor to health inequality and particuarly to early mortality for people of color. The police brutality in the US is also internationally recognizied such as in the 2014 report by the UN Committee Against Torture: > The Committee is concerned about numerous reports of police brutality and excessive use of force by law enforcement officials, in particular against persons belonging to certain racial and ethnic groups, immigrants and LGBTI individuals, racial profiling by police and immigration offices and growing militarization of policing activities.\nIf the issue of police brutality is this severe, one would expect that there might be a national database of police shooting incidents. Unfortunately, the US goverment does not maintain a national database of police shooting incidents. Rather, most efforts to collect police shooting incidents were done by journalists. D. Brian Burghart’s Fatal Encounter and Washington Post’s Fatal Force are famous examples. Some local goverments have pulic data portals for police shooting but they are mostly at a city level and it’s rare to find a state-level database."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#in-texas-law-enforcement-agencies-must-report-officer-involved-shooting-ois-incidents",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#in-texas-law-enforcement-agencies-must-report-officer-involved-shooting-ois-incidents",
    "title": "Police shooting in Texas 2016-2019",
    "section": "In Texas, law enforcement agencies must report officer-involved shooting (OIS) incidents",
    "text": "In Texas, law enforcement agencies must report officer-involved shooting (OIS) incidents\nThe state of Texas collects officer-involved shooting (OIS) data from all law enforcement agencies in the state. The term officer-involved shooting refers to two different types of incidents: 1. Shooting by officers. In this case, people harmed during a shooting incident are civilians. 2. Shooting of officers. Here, those harmed during an incident are police officers.\nThis was possible thanks to the legistation in 2015 (HB 1036) which requires all officer-involved shootings to be reported to the Office of Attorney General (OAG) of Texas. In 2017, another bill was passed, which now requires OAG to investigate missing reports and to fine law enforcement agencies if the reports were delayed. This data is is available in public in the OAG website. Every report in the data is saved as a PDF file, which makes it challenging to digest."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#data-collection-takes-a-huge-amount-of-effort",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#data-collection-takes-a-huge-amount-of-effort",
    "title": "Police shooting in Texas 2016-2019",
    "section": "Data collection takes a huge amount of effort",
    "text": "Data collection takes a huge amount of effort\nMany ML practicioners and data scientists are usually detached from data collection process, which makes it easy for us to overlook the importance of the effort and cost of data collection. Thus, in this section, I’d like to describe the data collection process for the OIS data briefly to show you that data doesn’t just appear for free.\nFirst and foremost, for a government dataset like this, legislations and polices should be established as a consorted effort to collect data formally. Now every law enforcement agency has to file a one-page report (see the example here) whenever they identify an officer-involved shooting incident in their jurisdiction. This OIS report contains many pieces of valuable information. To list a few: - Date and location of the incident (location as detailed as street address) - Demographic information (gender, race, and age) of the person who was shot by police and the police officer who shot the person - Severity of the incident (binary; injury or death) and whether the person shot possessed a deadly weapon - Whether multiple officers were involved, whether the officer was on duty, and so on.\nTo acquire this data, every month, TJI submits an open records request via OAG’s online portal. Luckily, we get a tabular data (not the PDF files) in a csv file format and our director Eva manually inspects and fixes errors. In this process, we contact the agencies and ask for clarification for errors. Finally, the data is added to the existing data and the data on our website is updated. You can download the OIS data on our website."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#the-oags-annual-report-lacks-insight.-can-we-do-better",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#the-oags-annual-report-lacks-insight.-can-we-do-better",
    "title": "Police shooting in Texas 2016-2019",
    "section": "The OAG’s annual report lacks insight. Can we do better?",
    "text": "The OAG’s annual report lacks insight. Can we do better?\nSince the legislations were passed, the OAG publishes an annual report to provide a summary of all OIS incidents in Texas in a given year. The two-page 2020 report has only three paragraphs where they discuss the data:\n\nFrom January 1, 2020 until December 31, 2020, there were one hundred ninety-four (194) separate incidents statewide involving peace officer shootings with a firearm that caused injury or death. Those incidents resulted in one hundred ten (110) deaths and eighty-four (84) injuries to individuals. Additionally, twenty-nine (29) peace officers were injured and six (6) were killed.\nOf the individuals (non-peace officers) who were either injured or killed in these incidents, sixty-seven (67) were Hispanic, sixty-three (63) were Caucasian, sixty-one (61) were African-American, one (1) was Asian or Pacific Islander, one (1) was not available and one (1) was of another nationality or race. One hundred seventy-six (176) of these incidents involved individuals who were reported to be carrying a deadly weapon; eighteen (18) did not.\nThe reason for the officers’ involvement are broken down as follows: one hundredfour (104) Emergency Calls or Requests for Assistance; twenty-five (25) involving Execution of a Warrant; twenty (20) Hostage, Barricade and Other Emergency Situations; thirty-eight (38) Traffic Stops; and forty-one (41) Other Uncategorized Situations.\n\nThis summary from the OAG report has several issues: 1. The data is aggregated at the state level. Texas has over 250 counties and that granular information is nowhere to be found. 2. There are no intersectional analyses. There are counts based on race and the reasons for officer involvement but that’s all. 3. There are no multi-year analyses. They have been collecting the data since 2015 but year-to-year comparison doesn’t exist. 4. There is no disclosure on data preprocessing. When analyzing a dataset, one has to make numerous decisions how they handle the data such as data exclusion, imputation, discretization, aggregation, and so on. For public data, preprocessing information is a key to transparancey and accountability. 5. There is missing information. The OIS report has information such as age demographics, report dates, on-duty-ness of officers, etc. However, none of that information is addressed in the report.\nTo address these points, I’ve decided to analyze the Texas police shooting data in the following way: 1. Comparison with reference datasets. If we want to make a value assessment on the OIS dataset, we need a target that we can compare with. To do this, I used US Census data and mortality data from Texas Department of Health Services. 2. County-level analyses. Texas nas numerous counties and some are quite different from others. This aspect should be reflected. 3. Various intersectional analyses. We can find many different ways to create interesting intersections of the data. Given the serious racial bias in police brutality, I made sure we look at intersections that contain race demographics. 4. Multi-year analyses. This way, we can idenfity whether there is any temporal trend in the analyses. 5. Reproducible and publically available data preprocessing information. I published all the code, visualizations, raw & intermediate datasets, and Jupyter notebooks on our GitHub repository. 6. Analyses on age demogrphics. This was never addressed in the OAG’s annual reports, and I made sure we utilize this information.\nIn this post, I am going to focus on several major findings from my original report (but updated with 2020 information) focusing on the race demogrphics."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#overview-of-the-ois-data",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#overview-of-the-ois-data",
    "title": "Police shooting in Texas 2016-2019",
    "section": "Overview of the OIS data",
    "text": "Overview of the OIS data\n\n\nCode\n\ndf_civilian = pd.read_pickle(\n    'https://github.com/texas-justice-initiative/officer_involved_shooting/blob/master/Data/Preprocessed/civilian_preprocessed_20162020.pkl?raw=true'\n)\ncolumns_to_show = ['date_incident', 'incident_county', \n                   'civilian_gender', 'civilian_age', 'civilian_race', 'civilian_died',\n                   'Traffic Stop', 'Emergency/Request for Assistance', 'Execution of a Warrant',\n                   'Hostage/Barricade/Other Emergency', 'Other'\n                  ]\ndisplay(df_civilian[columns_to_show].tail().reset_index(drop=True))\n\n\n\n\n\n\n  \n    \n      \n      date_incident\n      incident_county\n      civilian_gender\n      civilian_age\n      civilian_race\n      civilian_died\n      Traffic Stop\n      Emergency/Request for Assistance\n      Execution of a Warrant\n      Hostage/Barricade/Other Emergency\n      Other\n    \n  \n  \n    \n      0\n      2020-12-14\n      HARRIS\n      MALE\n      34.0\n      WHITE\n      DEATH\n      0\n      1\n      0\n      0\n      1\n    \n    \n      1\n      2020-12-14\n      HARRIS\n      MALE\n      30.0\n      BLACK\n      INJURY\n      0\n      0\n      0\n      0\n      1\n    \n    \n      2\n      2020-12-15\n      BRAZORIA\n      MALE\n      80.0\n      WHITE\n      DEATH\n      0\n      1\n      0\n      0\n      0\n    \n    \n      3\n      2020-12-25\n      HARRIS\n      MALE\n      31.0\n      BLACK\n      DEATH\n      0\n      1\n      0\n      0\n      0\n    \n    \n      4\n      2020-12-31\n      JEFFERSON\n      MALE\n      39.0\n      WHITE\n      DEATH\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n\n\n\nThis is a snapshot of a preprocessed OIS dataset (only a small portion of all columns shown). In general, the data has information about the time and location of the incident, demographics, and incident causes (the last 5 columns that are one-hot encoded). The original report has a number of intersectional visualizations but here, I would like to share one example to give you an idea.\nThis heatmap below shows the number of civilians shot by police by race and age groups at the state level. The numbers in the parentheses represent the total in the corresponding category and the gray cells represent 0s.\n\n\nCode\n\nplot.plot_heatmap_age_race_year(\n    df_civilian, \n    figsize=(14, 3.5), \n    cmap='Blues',\n    title='Number of Civilians Shot in OIS Incidents (All Counties) by Age Groups'\n)\n\n\n\n\n\nThis simple intersection of race and age already reveals something important that was hiddne in the OAG’s reports; almost every year, young children were shot by police."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#racial-disparity-in-police-shooting-in-texas",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#racial-disparity-in-police-shooting-in-texas",
    "title": "Police shooting in Texas 2016-2019",
    "section": "Racial disparity in police shooting in Texas",
    "text": "Racial disparity in police shooting in Texas\n\n\nCode\n\ndf_census = pd.read_pickle('https://github.com/texas-justice-initiative/officer_involved_shooting/blob/master/Data/Interim/census_county_race_2010.pkl?raw=true')\n\n# highlighting the top 5 counties in terms of the no. civilians shot\ntop5_locs = df_civilian['incident_county'].value_counts()[:5].index.values\n\ntemp = df_civilian.groupby(['incident_county'])['date_incident'].count().sort_values(ascending=False)\ncounty_names_by_incidents = temp.index\npop_size = df_census.sum(axis=1).loc[county_names_by_incidents.values]\n\n\n\n\nCode\n\nfig, ax = plt.subplots(figsize=(5, 4))\nax.plot(pop_size, temp, 'o', color='k', markersize=5, alpha=0.25)\nax.set(xscale='log', xlabel='Population Size', ylabel='Civilians Shot', title='General population vs. OIS data by County', xlim=[1e3, 1e7*3], ylim=[-5, 200])\nfor county_name in top5_locs:\n    ax.annotate(county_name.capitalize(), (pop_size.loc[county_name]+3e5, temp.loc[county_name]+1), fontsize=11, color='crimson')\n\n\n\n\n\nWhen I broke down the OIS data at the county level, I found that the number of civilians shot by police is positively correlated with the population size. Interestingly, most counties have a very small number of incidents and more than half of the data come from the top 5 most populous counties (annotated in the figure); Harris, Bexas, Dallas, Tarrant, and Travis.\nThe race composition of all the civilians shot by police in our data shows that the proportion of each race group, white, Black and Hispanic is comparable.\n\n\nCode\n\nplot.plot_pie(df_civilian, 'civilian_race', colors=cols_race, remove_labels=True, \n              title='Race Composition in the OIS Data\\n(All Counties)',\n              figsize=(4, 4), \n              fontsize=13,              \n              bbox_to_anchor=(1.5, 0.1))\n\n\n\n\n\nHowever, if we focus on the top 5 counties, it is now clear that Black people are shot more often (39.3%) than white (25.1%) and Hispanic (32.2%).\n\n\nCode\n\ndf_civilian_top5 = df_civilian.loc[df_civilian['incident_county'].isin(top5_locs), :]\nplot.plot_pie(df_civilian_top5, 'civilian_race', colors=cols_race, remove_labels=True, \n              title='Race Composition in the OIS Data\\n(Top 5 Counties)',\n              figsize=(4, 4),\n              fontsize=13,\n              bbox_to_anchor=(1.5, 0.1))\n\n\n\n\n\n\nBlack people are overrepresented in police shooting incidents\nAt this point, one might think that this is simply because the top 5 counties have larger Black populations than other counties. It’s true that in Texas, these top 5 counties with metropolitan areas have more Black people. If this hypothesis is true, this disproportionate distribution across race groups should be reflected in the census data.\n\n\nCode\n\nplot.plot_scatter_compare_race_incident_vs_population_pulled(\n    df_civilian, \n    df_census,\n    n_county=5,\n    annotate=True,\n    xylim=0.8,\n    figsize=(4, 4),\n    title='Racial Bias in Police Shooting\\n(Top 5 Counties, 2016-2020)')\n\n\n\n\n\nUnfortunately, I found that that was not the case. The plot above shows race representation of general population (US census, x axis) and of the police shooting incidents (OIS data, y axis). The dotted diagonal line shows equal representation between the census and the police shooting data. Any data points above the line indicate overrepresentation in the OIS data compared with the general population data (census).\nThe four colored dots represent the race groups in our data; Black (B), Hispanic (H), white (w), and others (O). The annotated numbers next to the dots show the rate of overrepresentation in the police shooting data of a race group compared to the census data. This analysis shows that Black people are 2.4 times overrepresented in police shooting compared with general population in these top 5 counties. The figure below shows that this pattern is in fact consistent across all 5 counties although the rate of overrepresentation for Black people is much higher particuarly for Harris and Dallas counties.\n\n\nCode\n\nplot.plot_scatter_compare_race_incident_vs_population(\n    df_civilian, \n    df_census, \n    fontsize=13,\n    title='Racial Bias in Police Shooting (2016-2020)')\n\n\n\n\n\n\n\nBlack and Hispanic people are overrepresented in fatal police shooting incidents\nSince the OIS data has a column that shows severity of incidents in terms of whether victims were killed or just injured, I decided to use the morality data of general population published by Texas Department of State Health Services (DSHS) to examine the racial bias in fatal police shooting incidents.\n\n\nCode\n\ndf_civilian_died = df_civilian[df_civilian['died']==1]\ndf_death_county = pd.read_csv('https://raw.githubusercontent.com/texas-justice-initiative/officer_involved_shooting/master/Data/Raw/Census/mortality_rate_by_county.csv', index_col='County')\ndf_death_county = df_death_county.drop('TOTAL', axis=1)\n\n\n\n\nCode\n\nplot.plot_scatter_compare_race_incident_vs_population_pulled(\n    df_civilian, \n    df_death_county,\n    n_county=5,\n    annotate=True,\n    xylim=0.8,\n    figsize=(4, 4),\n    title='Racial Bias in Fatal Police Shooting\\n(Top 5 Counties, 2016-2020)')\n\n\n\n\n\nSimilar to the comparison with the census data, I found the overrepresentation of Black people in the fatal police shooting incidents (2.1 times overrepresented). In addition, Hispanic people were overrepresented as well (1.6 times). This pattern of overrepresentation of Black and Hispanic people was consistent across all 5 counties:\n\n\nCode\n\nplot.plot_scatter_compare_race_incident_vs_population(\n    df_civilian_died, \n    df_death_county, \n    fontsize=13,\n    title='Racial Bias in Fatal Police Shooting (2016-2020)')\n\n\n\n\n\n\n\nYoung Black men are overrepresented in fatal police shooting incidents\nI found a similar result when I looked at the intersection of race and age demographics. In the figure below, I’ve compared the race composition of mortality data (left, “County Deaths”) and the OIS data (right, “Deaths by OIS”) in various age groups (5 subplots). I used the same color palette as above (red for white, blue for Black, and dark gray for Hispanic). If there is no disproportionate race representation in the fatal police shooting data per age group, the two stacked bars (general population’s mortality and fata police shooting) in a subplot should look similar.\nHowever, it is clear that Black people are overrepresented especially in younger age groups (ages 15-24 particularly). In the ages 15-24, Black people were about 3 times overrepresented in fatal police shooting incidents. This pattern quickly disappears as we move towards older age groups (plots on the right side).\n\n\nCode\n\nrace_list = ['WHITE', 'BLACK', 'HISPANIC', 'OTHER']\n\n# mortality data by age and gender (male only)\ndf_death_age_male = pd.read_csv(\n    'https://raw.githubusercontent.com/texas-justice-initiative/officer_involved_shooting/master/Data/Raw/Census/mortality_rate_by_age_male.csv', \n    index_col='Age').iloc[1:, :]\nage_range_names = df_death_age_male.index.values\n\n# intersection of race and gender\ndf_civilian_died_male_age = df_civilian_died.loc[df_civilian_died['civilian_gender']=='MALE'].groupby(['civilian_age_binned', 'civilian_race'])['date_incident'].count().unstack().fillna(0)[race_list]\n\n# selecting the age groups of interest\ninds_age_binned = np.arange(2, 7)\ndf_civilian_died_male_age = df_civilian_died_male_age.loc[inds_age_binned, :].drop('OTHER', axis=1)\ndf_death_age_male = df_death_age_male.iloc[inds_age_binned, :].drop('TOTAL', axis=1)\n\n# convert counts to proportions\ndf_civilian_died_male_age_pct = preprocess.pct(df_civilian_died_male_age, 1)\ndf_death_age_male_pct = preprocess.pct(df_death_age_male, 1)\n\n# changing the index for plotting\ndf_civilian_died_male_age_pct.index = ['Age {}'.format(s) for s in age_range_names[inds_age_binned]]\ndf_death_age_male_pct.index = ['Age {}'.format(s) for s in age_range_names[inds_age_binned]]\n\n\n\n\nCode\n\nplot.plot_stackedbar_compare_ratio(\n    df_civilian_died_male_age_pct,\n    df_death_age_male_pct,\n    df_civilian_died_male_age.sum(axis=1).astype(int), \n    severity='Deaths', \n    legend=False,\n    figsize=(10, 6)\n)\n\n\n\n\n\n\n\nIncident cause for Black people is often “Other”\nI found another example of racial disparity in how law enforcement agencies labeled the cause of incidents. The OIS report provides a list of five categories as a potential cause of an incident; “Traffic Stop”, “Emergency/Request for Assistance”, “Execution of a Warrant”, “Hostage/Barricade/Other Emergency”, and “Other”. Obviously, the last category, “Other” has the lowest accountability and transparency because it can be anything.\n\n\nCode\n\nincident_causes_list = [\"Traffic Stop\", \"Emergency/Request for Assistance\", \"Execution of a Warrant\", \"Hostage/Barricade/Other Emergency\", \"Other\"]\nincident_causes_list_sorted = df_civilian[incident_causes_list].sum(axis=0).sort_values(ascending=False).index\ndf_civilian_top5 = df_civilian.loc[df_civilian['incident_county'].isin(top5_locs), :]\ndf_civilian_incident_race_top5 = df_civilian_top5.groupby('civilian_race')[incident_causes_list_sorted].sum().loc[race_list, incident_causes_list_sorted]\ndf_civilian_incident_race_top5_pct = preprocess.pct(df_civilian_incident_race_top5, 0)\n\n\n\n\nCode\n\nfig, ax = plt.subplots(1, 1, figsize=(11, 3))\ndf_civilian_incident_race_top5_pct.T[::-1].plot(kind='barh', stacked=True, ax=ax, legend=False, width=0.75, color=cols_race)\nplot.annotate(ax, 'h', 'percent', fontsize=10)\nax.set(xlim=[0, 100])\nax.set_yticklabels([s + ' ({})'.format(n) for s, n in zip(df_civilian_incident_race_top5.columns, \n                                                          df_civilian_incident_race_top5.sum(axis=0).values)][::-1], fontsize=10)\nfig.legend(race_list, ncol=4, bbox_to_anchor=(0.81, 0.09), fontsize=10)\n\nfig.suptitle('Race Demographics by Incident Cause (Top 5 Counties)', fontsize=12, x=0.6, y=1.03)\nfig.tight_layout()\n\n\n\n\n\nWhen incident cause data is broken down by race groups, the “Other” category shows unusually high proportion of Black people. This means that when OIS reports were filed, more than half of the times, the incident cause was identified as “Other” when the victim was a Black person. Again, considering this category’s low accountability and transparency, this racial disparity was another concerning example."
  },
  {
    "objectID": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#conclusions",
    "href": "posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html#conclusions",
    "title": "Police shooting in Texas 2016-2019",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post, I’ve introduced several examples of my data science volunteering work at Texas Justice Initiative and a larger context surrounding it. Using reference data and conducting intersectional analyses, I was able to identify important racial disparity in Texas police shooting data that was never addressed in the state government’s official annual reports. In the follow-up post, I will share some useful tips for tech volunteering (especially in terms of using data science skills)."
  },
  {
    "objectID": "posts/2019-02-01-acm-fat-2019/index.html",
    "href": "posts/2019-02-01-acm-fat-2019/index.html",
    "title": "Fairness in ML at ACM FAT 2019",
    "section": "",
    "text": "ACM Conference on Fairness, Accountability, and Transparency (ACM FAT*) is a “multi- disciplinary conference that brings together researchers and practitioners interested in fairness, accountability, and transparency in socio-technical systems.” The conference originated from a fairness workshop in machine learning workshop at NIPS a few years and last year they had the first inaugural event as an independent conference. This year, they were joined with ACM and it was held in Atlanta, GA in Jan 2019. I attended both the tutorials and the main conference."
  },
  {
    "objectID": "posts/2019-02-01-acm-fat-2019/index.html#general-thoughts",
    "href": "posts/2019-02-01-acm-fat-2019/index.html#general-thoughts",
    "title": "Fairness in ML at ACM FAT 2019",
    "section": "General thoughts",
    "text": "General thoughts\nCompared to the last year’s conference, which I also attended, it seemed that the academic discipline has become more mature; last year, there were many talks focused on showing the examples of biased machine learning in practice and researchers discussed how they can agree upon defining terms such as fairness in mathematical concept. This year, there were many in- depth theoretical studies where researchers built mathematical models to simulate the propagated effect of bias in machine learning (ML). Empirical studies were also presented as well, in which researchers crowd-sourced data to show bias in human judgment especially when they interact with ML algorithms. Unfortunately, due to the recent long-term government shutdown, there weren’t many government officials who were supposed to participate. Hence, it was lacking to see how the discoveries made in academia can be translated into actual policies and legislation. However, it’s important to notice that there is huge growing interest in this field as ML-based algorithmic decision making is widespread in our society."
  },
  {
    "objectID": "posts/2019-02-01-acm-fat-2019/index.html#tutorials",
    "href": "posts/2019-02-01-acm-fat-2019/index.html#tutorials",
    "title": "Fairness in ML at ACM FAT 2019",
    "section": "Tutorials",
    "text": "Tutorials\nFor tutorials, I attended the Translation Tutorial: A History of Quantitative Fairness in Testing and Hands-on Tutorial: AI Fairness 360 Part 1 and Part 2.\nThe tutorials are divided into two categories: translation and hands-on. Since the conference is multi-disciplinary, there are attendees from various backgrounds; computer science, social science, law and policy, etc.\n\nTranslation Tutorial: A History of Quantitative Fairness in Testing\nThe translation tutorials are designed to have everyone more or less be on the same page. This specific translation tutorial I attended seemed to be aimed at computer scientists who are not much aware of the history of research on fairness and automated decision making. This was given by Google researchers Ben Hutchinson and Margaret Mitchell, who showed that the growing current interest on fairness in ML isn’t something new but existed in 1960’s when the civil rights movement started, which made sense.\nThen, the interest was on designing fair standardized tests like SAT or LSAT. Based on correlation studies, researchers then discovered that standardized tests require specific cultural knowledge, which is unfair to racial minorities. There were several views on how to build a fairer model, similar to the various modeling perspectives found in modern studies, such as whether to use sensitive features like race or gender exclusively in models. Also, researchers then pointed out that what we are trying to predict and what we should do are two different questions, which is always mentioned in almost every presentation at this conference.\nBen and Margaret finished the talk by inspiring the audience with various research opportunities that we can learn from the history. For instance, these historical studies started by asking the question of “what is unfair?” instead of the definition of fairness itself. Correlation- based approach and addressing regression problem (instead of classification) in fairness are other examples.\n\n\nHands-on Tutorial: AI Fairness 360\nThe second tutorial I attended (“AI Fairness 360”) was a hands-on one. Here, I learned about IBM Research’s effort to build an open-source Python package that implemented mathematical definition of various fairness criteria and how to apply them to a user’s existing workflow. The package seemed quite comprehensive and had great usability for the following reasons:\n\nThere is no one definition that satisfies all the needs and hence they have implemented a variety of metrics.\nThe package addresses different steps in ML pipeline where the fairness adjustment (i.e., mitigation algorithm) can be made: pre-process (mitigation to training data), in-process (mitigation to model during training), post-process (mitigation to predicted labels). This gives freedom and flexibility in the mitigation process.\nIts syntax is similar to scikit-learn, which make everything easy and familiar. Plus, their API is well-established, and the repository has many notebook examples.\nThe package can have various types of data (tabular, images, etc.)\n\nThis is just a start and of course, there are aspects that can be improved and investigated. First, some mitigation algorithms require tuning. It’ll be interesting to understand how this works on top of the main ML model’s tuning process. Since there are various fairness metrics, it would be nice to directly target those metrics for tuning. When a mitigation algorithm is applied, we sometimes see a compromise in model performance. In this case, it will be interesting to conduct an error analysis to find in which examples the model starts missing predictions."
  },
  {
    "objectID": "posts/2019-02-01-acm-fat-2019/index.html#keynote",
    "href": "posts/2019-02-01-acm-fat-2019/index.html#keynote",
    "title": "Fairness in ML at ACM FAT 2019",
    "section": "Keynote",
    "text": "Keynote\nThere were two keynotes from two very different domains: computer science and law. The computer science one was given by Jon Kleinberg at Cornell University. His talk was about mathematical formulation of a mitigation policy called Rooney Rule. The Rooney Rule is a requirement that at least one of the finalists be chosen from the affected group. His researched showed that it not only improves the representation of this affected group, but also leads to higher payoffs in absolute terms for the organization performing the recruiting. He presented a mathematical proof to prove his point. His model has only three parameters: composition of pool (diversity), level of bias, and abundance of outliers (superstars) and he showed that with certain constraints, Rooney Rule can be used to improve to the utility of the entire group (not just for the minority).\nThe second one was given by Deirdre Mulligan from UC Berkeley. Her talk was on “fostering cultures of algorithmic responsibility through administrative law and design”. She started by citing the case Loomis v. Wisconsin where Eric Loomis challenged the State of Wisconsin’s use of proprietary, closed-source risk assessment software (COMPAS) in the sentencing of himself to six years in prison. His argument was that there was gender and race bias in the software, which was also addressed by many investigative journalists. One of the interesting and disappointing part of this case was that the court addressed that “it is unclear how COMPAS accounts for gender and race…”, meaning the lack of transparency of the software but also the lack of ability to handle these cases in law. To fix this problem, she emphasized that there should be case laws addressing limits and details of math and algorithms, but more importantly, a “contestable” design. This means whenever there is a wrong done to a person by automated decision, they should be able to contest on this algorithmic decision."
  },
  {
    "objectID": "posts/2019-02-01-acm-fat-2019/index.html#main-sessions",
    "href": "posts/2019-02-01-acm-fat-2019/index.html#main-sessions",
    "title": "Fairness in ML at ACM FAT 2019",
    "section": "Main sessions",
    "text": "Main sessions\nIn total, there were 11 sessions and there was a wide range of topics ranging from problem formulation to content distribution and economic models. The following sessions stood out.\n\nFraming and Abstraction\nThe talks in this session here mentioned that problems in ML projects can start from the very beginning where ML practitioners and data scientists start formulating ideas and framing problems. For examples, if what one wants to measure cannot be obtained or measurable, they use a proxy that is correlated to the original metric they wanted to measure. Sometimes data scientists transform a regression problem as classification by discretizing variables. Unfortunately, these decisions are often not well documented even though they happen in many layers in a ML project.\n\n\nProfiling and Representation\nChakraborty at al. presented an interesting idea of handling online trolls in their talk, “Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations.” Since algorithms trained by online user data make often offensive and biased prediction, they came up with a fairer “voting” system by considering that there are a few bad actors (the trolls) and that most users are silent (who don’t case a vote). Here, the main challenge was the latter because they need to infer how a silent user might have voted. To solve this, they used existing personalized recommendation techniques.\n\n\nFairness methods\nIn “From Soft Classifiers to Hard Decisions: How fair can we be?”, Canetti at al. tackles the fairness problem by implementing a deferral process. Instead of making hard yes/no decisions, the model can defer the decision to a group of moderators so that they can gather more information and make more socially-acceptable decisions. Their paper discussion a couple of options of deferrals: applying different thresholds per group for deferral or equalizing the accuracy with equal deferral rate.\nSimilarly, in “Deep Weighted Averaging Classifiers”, Card et al. questions when we should know when not to trust a classifier. On top of traditional model performance, they come up with a separate measure of model credibility which can address whether this model is a right tool. They provided a fun example where they build a model by using the MNIST dataset and then used the trained model on the Fashion MNIST (the former has images of numerical digits and the latter, images of clothes and shoes). The model can still make predictions but clearly this model is not built for the testing data. In this case, the model’s “credibility” is very low.\n\n\nExplainability\nUstun at al. points out that explanability does not mean contestability in their talk, “Actionable Recourse in Linear Classification”. When a model explains why your loan is rejected, it doesn’t mean what you should do about it, especially the explanation is based on immutable features like your race. They suggest that we should have a system and model that provides “ability to obtain a desired prediction from a model by changing actionable input” so that the model’s explanation is actually useful. Their tool provides two answers: 1) what users can do to flip the decision and 2) the proof of no recourse (by checking every option in actionable features). The second answer can be still valuable because then it provides the user with contestable information.\n\n\nEconomic Models\nThere were two dedicated sessions just for economic fairness, new to this year’s program compared to the last year’s. One interesting concept discussed in multiple talks was “strategic manipulation.” It is also called “Stackelberg game”, where candidates might try to change their feature to game the system (the model) and the learner anticipates this and adjust the classifier by changing the decision boundary more stringent, which can also result in changes in candidates’ behavior. Milli et al.’s discussed in “The Social Cost of Strategic Classification” that so far the approach to this problem has institutional-centric view. For instance, when the institution, who build the model, wants to come up with a counter-measure to strategic manipulation, they weigh hard-to-change features such as parents’ income or zip code more highly. Then it becomes extremely high-cost for credit-worthy individuals in the wrong zip code area to obtain a loan, meaning the social burden is now on individuals. Their study found that as the game is played, individual social burden increases and returns more unfair outcome."
  },
  {
    "objectID": "posts/2019-02-01-acm-fat-2019/index.html#summary",
    "href": "posts/2019-02-01-acm-fat-2019/index.html#summary",
    "title": "Fairness in ML at ACM FAT 2019",
    "section": "Summary",
    "text": "Summary\nI enjoy this conference very much not just because of personal interest but also of the fact that studies discussed here emphasize robustness and responsibility of ML algorithms, often ignored in ML community. Plus, the crosstalk between different domains enlighten me because they the topics can be applied to many aspects of ML engineering work. The theoretical studies provide new insights on how to measure abstract concepts like fairness and bias, transferrable to processes like problem statement that are highly important in ML. The empirical studies help theoretical studies grounded and more realistic. Finally, the law and policy side of works emphasizes the real implementation of these studies, which resembles constructing deployment plans and the results with stakeholders."
  },
  {
    "objectID": "posts/2020-11-24-github-manuscript-review/index.html",
    "href": "posts/2020-11-24-github-manuscript-review/index.html",
    "title": "Markdown and GitHub for scientific writing",
    "section": "",
    "text": "Last year, I was fortunate to have an opportunity to do a poster presentation at the Scientific Computing in Python (SciPy) conference. Every presenter has an option to submit a written proceedings and I decided to participate. I’ve never had a chance to present my ML work as a paper in industry, and so I thought it would be a good experience.\nI was a bit intimated when the conference committees said authors should submit their manuscripts as pull requests in their proceedings repository in GitHub. They also said manuscripts should be written in reStructuredText. I have used git for many years but I have limited experience in collaborating with others on GitHub. Plus, I’ve never used reStructuredText (I’ve only used Markdown) and also have never written an academic manuscript in markup languages.\nIn short, I eventually became to really enjoy the process of using markup languages for writing manuscripts and using GitHub for review. I will briefly talk about my experience at the SciPy and talk about why I think you should give it a try."
  },
  {
    "objectID": "posts/2020-11-24-github-manuscript-review/index.html#scientific-writing-in-markup-languages",
    "href": "posts/2020-11-24-github-manuscript-review/index.html#scientific-writing-in-markup-languages",
    "title": "Markdown and GitHub for scientific writing",
    "section": "Scientific writing in markup languages",
    "text": "Scientific writing in markup languages\nThere was a bit of learning curve but it wasn’t too difficult to get used to the reStructuredText syntax because there were some similarities between Markdown and reStructuredText (rst). The conference committee also provided examples. They also provided me with a PDF builder that I could run locally. This helped me preview the final version of the PDF when it’s fully rendered. This was useful especially to decide the location of figures. There was a custom LaTeX builder in their repository too, which was invaluable.\nCompared with existing word processor softwares such as MS Word or Google Docs, using markup languages pose some challenges to the first-time users. First, you need to learn the syntax. There are some limitations on styling because not everything is rendered automatically (e.g., LaTeX) and sometimes you need to play with html and css files to get a more complex page layout.\nHowever, there are many benefits as well:\n\nEvery styling is written explicitly (e.g., ** for bold in Markdown) and thus it is easily discoverable.\nCode snippets can be automatically rendered in a standardized way.\nIt’s easy and fast to publish the manuscript online, which makes it easy to share.\nFinally, with a right document builder, the above-mentioned pain points can be mitigated.\nIt is much easier and convenient to review the manuscript (see below)."
  },
  {
    "objectID": "posts/2020-11-24-github-manuscript-review/index.html#using-github-for-submission-and-review",
    "href": "posts/2020-11-24-github-manuscript-review/index.html#using-github-for-submission-and-review",
    "title": "Markdown and GitHub for scientific writing",
    "section": "Using GitHub for submission and review",
    "text": "Using GitHub for submission and review\n\n\nOnce I finished writing the manuscript, I submitted the main document and image files as a pull request. It was pretty straightforward because I was working alone in my own branch (you can find my pull request here). A few weeks later, two reviewers started commenting on my pull request. The whole process was quite simple and very similar to a typical review process in academia: reviewers left comments on certain parts of the manuscript and I responded. It also reminded me of a code review process. This year, I volunteered at the SciPy 2020 as a reviewer. I reviewed a paper that was submitted as a pull request."
  },
  {
    "objectID": "posts/2020-11-24-github-manuscript-review/index.html#benefits-of-github-for-reviewing-manuscripts",
    "href": "posts/2020-11-24-github-manuscript-review/index.html#benefits-of-github-for-reviewing-manuscripts",
    "title": "Markdown and GitHub for scientific writing",
    "section": "Benefits of GitHub for reviewing manuscripts",
    "text": "Benefits of GitHub for reviewing manuscripts\nWith these experiences, I learned that markup language with GitHub review process provides many benefits for scientific writing, compared with MS Word or Google Docs which are used in typical (academic) review process:\n\n\n\n\n\n\n\n\n\nMarkup + GitHub review\nMS Word or Google Docs + Academic review\n\n\n\n\nDiscover exact changes\nYes\nPossible but can be confusing when many changes occur in one place\n\n\nChange versions easily\nYes\nMay need multiple versions (separate files)\n\n\nTrack communications\nYes\nChallenging because comments happen in the margin, which has a very limited space. Otherwise, they are addressed in a separate space (like a letter).\n\n\nText override or unauthorized edits\nNo (permission needed)\nCan happen if tracking option is off or multiple files exist\n\n\nGroup communication\nYes\nNo. Normally reviewers don’t talk to each other. Reviewers communicate individually with authors.\n\n\nImmediate responsiveness\nHigh (online comments)\nLow. Letters or emails need to be exchanged, which can take months.\n\n\nCommunication transparency\nHigh\nAlmost none (academic review). Normally double-blind. A reviewer can’t normally read other reviews during the review process.\n\n\nPublic transparency\nYes (public repositories)\nAlmost none. Very rare to publish the whole conversation between reviewers and authors.\n\n\nOne place for everything\nYes\nNo. Normally non-document type files exist in a different place.\n\n\n\nIn short, the main benefit of using GitHub for reviewing manuscript comes from that GitHub is an excellent version control tool. It prevents reviews (new changes) and the manuscript (original file) from getting mixed up and allows us to track changes meticulously.\nAnother huge benefit is transparency. If our repository and pull requests are public, the entire review process can be seen by anyone. This way, every decision making process is tracked and stays public, which minimizes potential dispute or abuse between authors and reviewers. Given that academia is not capable of perfect self-governance and some academic disciplines have reproducibility problems, transparent review process like this can become a solution."
  },
  {
    "objectID": "posts/2020-11-24-github-manuscript-review/index.html#final-thoughts",
    "href": "posts/2020-11-24-github-manuscript-review/index.html#final-thoughts",
    "title": "Markdown and GitHub for scientific writing",
    "section": "Final thoughts",
    "text": "Final thoughts\nIt is true that markup languages may not always be perfect for writing a manuscript. Some people are more familiar with traditional word processing softwares where styles are immediately rendered and more style options are available. However, based on my experience with the SciPy conference, especially, being at the both ends of the review process, I can say the benefits of using markup language for scientific writing and using GitHub for review outweighs its caveats. Manuscripts written in markup languages can be easily reviewed in GitHub where we can utilize its excellent built-in tools to achieve better tracking, communication, and transparency."
  },
  {
    "objectID": "posts/2021-03-08-facct-2021-main1/index.html",
    "href": "posts/2021-03-08-facct-2021-main1/index.html",
    "title": "FAccT 2021. AI audit, governance, and trustworthiness",
    "section": "",
    "text": "This year, for the first time, the FAccT conference has more than one track and there are 27 sessions (+80 talks) happening in 3 days. This is a summary of the first day of the main conference. My summary is based on the talks I attended based on my interest."
  },
  {
    "objectID": "posts/2021-03-08-facct-2021-main1/index.html#keynote-health-technology-and-race",
    "href": "posts/2021-03-08-facct-2021-main1/index.html#keynote-health-technology-and-race",
    "title": "FAccT 2021. AI audit, governance, and trustworthiness",
    "section": "Keynote: Health, Technology and Race",
    "text": "Keynote: Health, Technology and Race\nAs a person who’s been attending FAccT every year, I’ve been familiar with the criticism to the FAccT academic community; many studies appeared here are often too abstract or disconnected from the real world and the impacted communities. The organizers are aware of this and they have been making efforts such as organizing CRAFT (Critiquing and Rethinking Accountability, Fairness and Transparency) sessions where they bring more multidisciplinary stakeholders. I considered the first keynote from today, given by Yeshimabeit “Yeshi” Milner, the executive director and co-founder of Data for Black Lives (D4BL), as a signal from the conference saying that they are actively working on this criticism. In this regard, it was an excellent keynote speech to kick off the conference.\nAt the very beginning of the talk, Yeshi said, “we will never achieve fairness, accountability, and transparency without first demanding justice, power, and self-determination.” Again, this directly points out that without a holistic approach where we bring all the stakeholders, achieving justice and making changes isn’t going to happen. Yeshi then talked about racial disparity examples where Black people were discriminated and disproportionately affected in a negative way, especially in healthcare.\nShe introduced the COVID-19 Data Tracker Project where organizers in the D4BL members compiled a dataset by themselves to highlight the stark disparities in health outcomes related to COVID-19. She mentioned that the larger goal here is to “build the political power of Black communities” as well, which is a critical point. She also emphasized that any intervention that does not build political power or agency of the marginalized community is liable to harm rather than to help. Considering that even some tech-for-good projects with good intentions often sideline those who are affected by the system, her remark was poignant.\n“We need to think ‘who do we need to be’” At the end of the talk, she mentioned the importance of reflection, and said we need to resist the urge to feel compelled to keep moving forward, but to pause and reflect on ourselves. This involves identifying what we should unlearn and how we need to create new spaces to bring people who are directly impacted. I think this passionate and moving speech from Yeshi sent out this important message to all attendees, especially to academics and tech workers, like myself."
  },
  {
    "objectID": "posts/2021-03-08-facct-2021-main1/index.html#algorithm-audit-and-impact-assessment",
    "href": "posts/2021-03-08-facct-2021-main1/index.html#algorithm-audit-and-impact-assessment",
    "title": "FAccT 2021. AI audit, governance, and trustworthiness",
    "section": "Algorithm Audit and Impact Assessment",
    "text": "Algorithm Audit and Impact Assessment\nFor tech workers who are interested robust and responsible AI like myself, case studies are like gems. We are always curious about how others do things and want to learn from each other. That’s why I was very happy to see a case study about algorithm audit from Wilson et al., Building and Auditing Fair Algorithms: A Case Study in Candidate Screening.\nResearchers from Northeastern University worked with Pymetrics, a company that uses gamification and AI to assist hiring in other companies. This collaboration itself was a unique instance because most audits happen internally or in a permission-less form where the companies being audited do not cooperate with the auditors. The academic researchers seemed to be quite transparent about the process (e.g., they revealed they were paid but the audit was done independently) and disclosed some problems in Pymetric’s system based on the criteria they used. I don’t know the full details yet but it was nice to hear that Pymetrics started working on solutions to fix these.\nSpeaking of the audit, there was another interesting paper about Amazon’s questionable practice in product recommendation. It’s been known that Amazon created their own private brand and started competing against other brands. The paper, When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces investigates this issue by conducting a systematic audit on Amazon’s recommendation system. They found that Amazon’s private brand has greater exposure in the sponsored ads by comparing related item network (RIN) models between the private brand’s and others’ networks.\nFinally, there was a paper dedicated to algorithmic impact assessment. The authors in the paper, Algorithmic Impact Assessments (AIA) and Accountability: The Co-construction of Impacts, pointed out that often poor impact assessment practices neither mitigate harm nor bring effective algorithm governance. They emphasized that impact assessment should be closely related to accountability, especially for both institutions and people who are affected. The paper showcases impact assessment examples from different domains, which will be useful to design an effective and accountable algorithmic impact assessment procedure."
  },
  {
    "objectID": "posts/2021-03-08-facct-2021-main1/index.html#trust-in-ai",
    "href": "posts/2021-03-08-facct-2021-main1/index.html#trust-in-ai",
    "title": "FAccT 2021. AI audit, governance, and trustworthiness",
    "section": "Trust in AI",
    "text": "Trust in AI\nSpeaking of impact assessment, if we have incredibly thoughtful (which is probably very detailed) impact assessment and documentation of deployed algorithms, does it bring more trust? The paper, The Sanction of Authority: Promoting Public Trust in AI, says, perhaps not. The authors bring up the example of aviation industry. When we board on a plane, do we want to see the maintenance log, history of pilot performance to reassure ourselves? Or, do we normally trust that aviation is “highly regulated, comparatively safe, and any breach of safety will be sanctioned”? The authors suggest that the latter is how public trust is formed. This is an excellent point. Documentation of deployed AI systems can be as bad as terms of service statements, which usually have extremely poor readability and can be easily manipulated. I liked that the authors emphasized both the importance of regulatory system but also of externally auditable AI documentation, which would facilitate the development of AI regulations."
  },
  {
    "objectID": "posts/2021-03-08-facct-2021-main1/index.html#explainable-ai",
    "href": "posts/2021-03-08-facct-2021-main1/index.html#explainable-ai",
    "title": "FAccT 2021. AI audit, governance, and trustworthiness",
    "section": "Explainable AI",
    "text": "Explainable AI\nHow Can I Choose an Explainer? An Application-Grounded Evaluation of Post-hoc Explanations was a case study of comparing various XAI methods (LIME, SHAP, and TreeInterpreter) and how human participants use explanations to perform fraud detection task. I was hoping to see a discussion of comparison and evaluation of various XAI methods in a standardized way but it was more about evaluating the suitability of these methods using case-specific real-human tasks. I’d like to take a deeper look at the paper to see whether the authors mentioned any suggestions on cases where it is challenging to run those tasks."
  },
  {
    "objectID": "posts/2021-03-08-facct-2021-main1/index.html#data-governance",
    "href": "posts/2021-03-08-facct-2021-main1/index.html#data-governance",
    "title": "FAccT 2021. AI audit, governance, and trustworthiness",
    "section": "Data Governance",
    "text": "Data Governance\nMy favorite talk of the day was Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure by Hutchinson et al.\nIt’s not too difficult to meet ML practitioners who are obsessed with state-of-the-art deep learning models and conveniently ignore training data and how their data is preprocessed. These people often have a tendency of even looking down upon data preprocessing process and consider it demeaning. This paper takes a jab at this phenomenon.\nThe authors first talk about data scapegoating. Even though everyone is well aware of “garbage in, garbage out” relationship between data and ML models, often data is overlooked and undervalued. Many ML practitioners depend on benchmark datasets, which “reinforce the notion of data as fixed resources.” In my experience, this tendency intensifies the fanaticism on ML algorithms even though they are just a tiny part of a much bigger picture. With many good off-the-shelf models readily available, frankly speaking, models are rather becoming commodities.\nThey also point out the lack of good governance practice in data maintenance. Understanding ML model performance often requires good and detailed understanding of training data. However, validation and verification of training data is rarely conducted in ML engineering pipelines.\nFinally, the authors didn’t forget to mention the power imbalanced between ML practitioners and data providers. I often feel extremely fortunate that I have experienced the entire end-to-end process of an academic research. I started from scratch and designed a process to collect the data by myself, which required great care and intense labor. In the current ML trend, this step is often conveniently removed and thus creates a bad habit of ignoring labor and responsibility in the data collection and development process.\nI really appreciated the simplicity of the authors’ suggestion on this problem; “acknowledge that datasets are technical infrastructures.” Since many ML practitioners are familiar with tech infrastructures such as software programs, the authors suggest that we simply apply existing practices to datasets; create requirements, design, and tests. Once this formally becomes a part of infrastructure, it also becomes easy to get all the existing stakeholders in the pipeline involved."
  },
  {
    "objectID": "posts/2021-03-10-facct-2021-main3/index.html",
    "href": "posts/2021-03-10-facct-2021-main3/index.html",
    "title": "FAccT 2021. Journalism, data leverage, education, and language models",
    "section": "",
    "text": "The final keynote was given by Julia Anguin, an investigative journalist and a co-founder of The Markup, “a nonprofit newsroom that investigates how powerful institutions are using technology to change our society.” I was already familiar with her body of her because she’s famous for her ProPublica article on racial bias in the risk assessment software, COMPAS. Her talk consisted of two parts. First, she talked about how her organization Markup is run and what methods they use to publish investigative journalism reports. Later, she gave several examples of their work from last year, which was mostly on algorithm auditing.\nMarkup consists of engineers and journalists. It’s interesting to hear her saying that engineers are essentially investigative journalists themselves. They collaborate with journalists in the team but they bring tech expertise to tech reporting. Once Markup has written a report, they have an extensive vetting process (called the “bulletproof” stage) where they actively seek out critiques from various external experts. What’s interesting is that each work product comes with a pair of publications; a news article targeting general public and an extensively detailed methodological write-up. They also have established a practice of publishing datasets and codes so that other people can replicate their analysis and apply those to their own projects.\nWhen she gave examples of their algorithm auditing projects, it was amusing to learn about various novel approaches they have made to probe algorithms. In one project, they simply analyzed search results, but in other projects, they created numerous web accounts to try to reverse-engineer opaque decisions algorithms make. Markup also seemed to put resources into creating tools such as Blacklight that runs privacy tests on virtual browsers for websites to inspect their privacy violations, or Citizen Browser where volunteer citizens use Markup’s app to scrape data from Facebook so that the team can examine various decisions the Facebook app makes.\nDuring the Q&A, one memorable remark she made was all data is political. Whether it’s leaked, accessed through protocols, or publicly available, data collection starts with a certain intention. She said there’s no national database on police violence in the US, which reveals the political will and intention of the US government, which was revealing. To fight this issue, she said it’s really important to know the limitations of data and be very transparent about it.\nShe mentioned that Markup often tries to co-publish pieces with other news companies for distribution reasons. She said sometimes the process is challenging because these organizations do not have technical experts to review their lengthy methodological write-up. This resonated with me most because this is what I’ve been experiencing while volunteering at a local non-profit. Nevertheless, when one of the audience members asked her about how individual tech workers help, she said the most straightforward way to help journalists it via financially supporting them, especially the local news organizations."
  },
  {
    "objectID": "posts/2021-03-10-facct-2021-main3/index.html#data-leverage",
    "href": "posts/2021-03-10-facct-2021-main3/index.html#data-leverage",
    "title": "FAccT 2021. Journalism, data leverage, education, and language models",
    "section": "Data Leverage",
    "text": "Data Leverage\nIn the tech world, there’s a tendency to trivialize data labor. Tech companies also conduct problematic practices to collect data. For the public, who are in a very vulnerable position, what can they do? Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies explores several ways to leverage the fact that we the public are data providers. The defined the data leverage as power to influence a company held by those who implicitly or explicitly contribute data on which companies rely. The large goal of identifying data leverage is to give more agency to people. They explore three options; data strike, data poisoning, and conscious data contribution. Each options has its pros and cons and some have more complex legal landscape to navigate. Personally I found data poisoning most interesting, which is about inputting fake and random data (somewhat similar to adversarial attacks)."
  },
  {
    "objectID": "posts/2021-03-10-facct-2021-main3/index.html#measurement",
    "href": "posts/2021-03-10-facct-2021-main3/index.html#measurement",
    "title": "FAccT 2021. Journalism, data leverage, education, and language models",
    "section": "Measurement",
    "text": "Measurement\nAny attempts to address FAccT topics in mathematical models begin with constructing a measurement; we are trying to measure unobservable and abstract concepts. Measurement and Fairness proposes measurement modeling framework in fairness research in computer science. The authors target fairness specifically because fairness itself is an essentially contested construct; it’s heavily context-dependent and some constructs are conflicting. Besides, in general, determining which measurements and metrics to use requires extreme caution. To mitigate these problems, the authors came up with two criteria; construct reliability and construct validity. The former checks whether similar inputs can return similar measurements. The latter checks whether measurements are meaningful and useful such as whether they capture relevant events, or whether the impact from using the measurements has been addressed."
  },
  {
    "objectID": "posts/2021-03-10-facct-2021-main3/index.html#education",
    "href": "posts/2021-03-10-facct-2021-main3/index.html#education",
    "title": "FAccT 2021. Journalism, data leverage, education, and language models",
    "section": "Education",
    "text": "Education\nSince last year, the conference has been including more papers about ethics education. This year, “You Can’t Sit With Us”: Exclusionary Pedagogy in AI Ethics Education stood out. This survey paper collected more than 250 AI ethics courses in computer science curriculums from more than 100 universities around the world and analyzed the pattern. Sadly, they found a predominant pattern of exclusion in many courses. The authors found this exclusion had many shapes; the discipline not valuing other ways of knowing, lack of collaboration with other disciplines, and lack of interest in learning other’s work. The fact that computer science itself can’t solve AI ethics problems, this seemed very worrisome."
  },
  {
    "objectID": "posts/2021-03-10-facct-2021-main3/index.html#language-models",
    "href": "posts/2021-03-10-facct-2021-main3/index.html#language-models",
    "title": "FAccT 2021. Journalism, data leverage, education, and language models",
    "section": "Language Models",
    "text": "Language Models\nWhat happens when an authoritarian government and AI meet? Censorship of Online Encyclopedias: Implications for NLP Models explores how censorship in training data influence downstream processes in NLP applications using Chinese language models. One of the dataset they looked into was Baidu Baike, a censored language dataset that is often used in Chinese language models. They first checked the word embeddings and examined the position of words such as democracy, surveillance, social control, CCP (Chinese Communist Party) with respect to positive and negative words. They found that democracy often appeared with negative words and the rest in the example were the opposite. They also found similar pattern in a sentiment classification application that uses web news headlines. These results were concerning because as the authors addressed, these applications can be used to monitor public opinion and curate social media posts, essentially as a highly effective propaganda machine in a massive scale.\nFor those who’ve been following the news of Timit Gebru, an AI ethics researcher who was fired by Google, On the Dangers of Stochastic Parrots:Can Language Models Be Too Big? was the paper that was at the core. The paper explores potential risks of the current trend in language modeling where researchers and practitioner pay more attention to larger models. As shown in the paper, these models have billions, sometimes trillions of parameters. The first risk the authors bring up is environmental and financial cost. Studies have shown that training a single BERT base model (without tuning) requires as much energy as a trans-American flight. This gets more problematic if you become aware that most language models serve English-speaking communities and the most impacted communities from climate change are not those. There are more sinister risks too such as training data still lacking diversity (i.e., “big” doesn’t mean “diverse.”) and lacking oversight, which creates harmful effects. Plus, since many efforts and resources in ML community go into building large language models, other research topics are naturally overlooked. The authors suggest simple mitigation strategies - step back and think; evaluate various approaches, ask yourself if we really need large models, and conduct pre-mortem analysis."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "",
    "text": "As a data scientist working in industry, I frequently witness the impact a machine learning application can make. This impact often has a cascade of downstream effects which are inconceivable to a data scientist without enough domain knowledge. Nevertheless, under the widespread motto, “move fast and break things,” in tech industry, ML practitioners tend to care little about the size of their products’ impact. In certain cases, they even overlook scientifically rigorous evaluation of their products. These phenomena have been greatly worrying me ever since I started my career in industry. My concern has deepened due to many recent instances of AI applications reproducing human bias on a massive scale and aggravating existing socioeconomic problems.\nRecently, I found out about the event, The Tech Policy Workshop at the Center for Applied Data Ethics at USF via Dr. Rachel Thomas on social media, as I have been enjoying her blog posts and talks. She is smart, honest, and concerned about the welfare of people and society. Thus, I assumed that the workshop would be beneficial in many ways. The lineup was also very interesting because the speakers had diverse backgrounds. Additionally, the low registration cost was helpful.\nI have been interested in fairness, accountability, and transparency of ML for several years, and thus have been attending events related to the topics. Compared with those events, I found this workshop unique in two ways. First, the organizers made a great effort to gather experts from a wide variety of domains ranging from computer science to policymaking. This makes sense because tech ethics and policy are multidisciplinary issues. Second, by choosing the format of a workshop, instead of a conference, the event was more interactive. There were several hands-on exercises, which encouraged valuable discussions among the attendees. The workshop exceeded my expectations and I felt grateful for the opportunity to attend."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#interactive-ethics-exercise-on-facial-recognition",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#interactive-ethics-exercise-on-facial-recognition",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "Interactive ethics exercise on facial recognition",
    "text": "Interactive ethics exercise on facial recognition\nEvery session was interesting and unique in its own way. However, the main lesson I got out of the workshop came from the ethics session led by Irina Raicu from Santa Clara University. Ethics can be vague, abstract, and even esoteric but Irina made it sound accessible. She compared it to birdwatching; the more we learn about ethics, the more easily we can notice ethical issues in the world. Instead of giving a lengthy philosophy lecture, she taught us various easy-to-understand ethical “lenses”. She then asked us to use these in an exercise based on a real-world case about face recognition. The case was about a dataset1 that ML researchers at IBM created to train a fairer face recognition model.\nI was already familiar with the case. Joy Buolamwini, a ML researcher at MIT, published a study2 that revealed gender and racial bias in the training dataset in many commercial face recognition platforms including IBM’s. After this paper was published, IBM was forthcoming about the issue and they promised to improve their application. It is safe to assume that IBM researchers had good intentions when they published a new training dataset. Unfortunately, we found that this new dataset still had many problems, especially related to data privacy.\nDuring the exercise, we applied different ethical lenses to address which ethical values were pursued or violated. Since we were a group of data scientists, policymakers, and activists, there were a variety of ideas. I admit that in the beginning of the exercise, I was somewhat frustrated because ethics cannot be easily optimized. This meant that we might not be able to reach conclusions easily. However, once we started discussion, I realized that the true value of the exercise was not about finding the right answer quickly, but about evaluating various perspectives especially when multiple ethical values were in conflict. The exercise also taught me that ethical decision making is a highly dynamic process that requires a diverse set of opinions. I started thinking this type of exercise would be beneficial for tech engineers to change the way they think."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#ethics-training-for-tech-workers",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#ethics-training-for-tech-workers",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "Ethics training for tech workers",
    "text": "Ethics training for tech workers\nGranted, as Chris Riley at Mozilla pointed out during his session, teaching ethics to engineers may not be the best way to solve the tech-related problems in our society. After all, ethics focuses on individuals. Typically, socioeconomic problems are solved more effectively through legislation, regulations, and policies. However, several speakers hinted that training engineers to learn about ethical decision making can still be useful."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#tech-industry-has-great-power-and-influence-on-our-society",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#tech-industry-has-great-power-and-influence-on-our-society",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "Tech industry has great power and influence on our society",
    "text": "Tech industry has great power and influence on our society\nAs Prof. Elizabeth E. Joh from UC Davis mentioned, tech industry has a significant amount of power these days. According to her, in certain circumstances, police must even drop their investigation so as not to violate the non-disclosure agreement related to the procurement of technological devices they use. She gave an example of police body-cams produced by the company, Axon, who dominates the market. They make almost every decision on how the device works, how and where data is stored and maintained, and so on. This means the management decision in the tech company can have a significant impact on the general public. Guillaume Chaslot from Algo Transparency delivered a similar message using an example from YouTube. It showed how slow and passive YouTube’s response was on content moderation. This is similar to Facebook’s naïve approach that contributed to the dissemination of disinformation over the internet, which created numerous sociopolitical problems all over the world."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#tech-workers-also-have-power",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#tech-workers-also-have-power",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "Tech workers also have power",
    "text": "Tech workers also have power\nMultiple speakers emphasized that tech workers have great power as well. Recent tech employee walkouts demonstrate that power. Some even affected their companies’ decisions on certain social issues. The news3 about Google recently hiring firm known for anti-union efforts implies that companies now have recognized employee unrest as a threat. ML practitioners who work closely on sensitive datasets can wield even greater power. Kristian Lum from Human Rights Data Analyses Group shared a disturbing example of a ML application used at a government branch. A ML practitioner manipulated the results by hand-selecting model coefficients from multiple versions of a dataset, but this only came to light much later during an audit. Based on my conversations with other data scientists, proper oversight or formalized review of technical work is still missing in many industries. Under these circumstances, the responsibility to provide transparency and accountability falls to individual ML practitioners. Since building a ML application is a complex process, technical debt can aggravate quickly."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#what-i-can-do-as-a-tech-worker",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#what-i-can-do-as-a-tech-worker",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "What I can do as a tech worker",
    "text": "What I can do as a tech worker\nThe fact that ethics is about an individual’s decisions makes me think that there should be something I can do and that I should be aware of the responsibility that comes with my power as a tech worker. I can share a few things I try at work to make a change even though they are small. First, I seek out resources to learn about best practices in ML across the industry and to establish them at work regarding transparency and accountability. My team uses Model Cards4, which summarize how models are trained and how they are supposed to be used, and Datasheets5, which describe how data are collected, used, and processed. Second, to prevent technical debt, I often ask for reviews so that my work is seen by many different stakeholders. Another effort is bringing my work to a public space through publications, seminars, or conferences. For the tech ethics and policy issues, I try to take advantage of any opportunities inside my job (e.g., lunch-and-learn sessions) or outside (e.g., house parties or friends’ gatherings) to raise awareness. Finally, I try to learn more about the topics via events like this workshop."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#what-i-can-do-as-a-citizen",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#what-i-can-do-as-a-citizen",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "What I can do as a citizen",
    "text": "What I can do as a citizen\nI spent a lot of time thinking about what I can do as a tech worker, but I was forgetting something more important. At the end of the workshop, Prof. Elizabeth E. Joh talked about how tech companies wield their massive power in surveillance and policing. An attendee asked her whether there is anything we can do to make a change. Rather than answering his question, she asked him how many times he has previously attended city council meetings of his own city. She said there is still not enough awareness about this problem among the general public nor even momentum to create strong public opinion. That is why we need to raise our voices to address the issue and demand a change. Carrying out my duty as a citizen by speaking up was the one that I have overlooked.\nAt the workshop, I heard from Bay Area government officials about how they try to protect their citizens. Catherine Bracy from TechEquity Collaborative discussed organizations that help protect local communities from tech-related problems. Even though the Bay Area has been facing many problems such as housing and severe income inequality due to tech industry, it also has become the place where movements to fight back are pioneered. I think it has happened here because 1) it is the epicenter of the tech boom, 2) the severity of the problems is extreme, and 3) people are more aware of and sensitive about these issues.\nOther local governments face a very different situation. The Litigating Algorithms report6 by the AI Now Institute mentions that many local governments are drawn to the idea of implementing automated decision processes (ADP) because officials expect ADP to save money. Since the governments do not have affordable access to the right talent, they end up outsourcing the work to cheap vendors who often do poor execution and do not provide enough transparency. The report lists many examples of ADP that went wrong and harmed many people."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#responsibility-as-a-citizen-and-a-tech-worker",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#responsibility-as-a-citizen-and-a-tech-worker",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "Responsibility as a citizen and a tech worker",
    "text": "Responsibility as a citizen and a tech worker\nThis workshop gave me an idea of how to combine my duty as a citizen and my duty as a tech worker. Government officials I spoke to at the workshop said it is extremely difficult for governments to catch up with the tech industry. It makes sense—policymaking is an inherently slow process but tech is all about fast developments and adaptations. To reduce the gap, we can start organizing small groups of tech workers to help nonprofits and local governments navigate the ever-changing tech space more efficiently. In the long run, these groups can form an advisory board or a civilian oversight committee to monitor tech-related issues in local communities such as predictive policing. By then, of course, these groups will include not only tech workers but other stakeholders such as local residents, legal experts, social scientists, activists, and more. This way, tech workers like myself can provide our local communities with technical expertise. At the same time, I will have a better idea of the real impact that my work makes. I optimistically believe that as we tech workers engage with our communities, we will change tech culture positively."
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#final-thoughts",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#final-thoughts",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "Final thoughts",
    "text": "Final thoughts\nIn addition to learning much about tech ethics and policy, I found the workshop particularly special because I met great people from a variety of backgrounds, and I made new friends. I am sure many attendees had the same experience. It’s a rare occasion to have a diverse group of people sharing many concerns about the future in the same place for two days of honest discussions.\nWhen I saw ads for the event, I hesitated whether I should register. I was unsure whether I would belong there because policy is outside of my experience. During the workshop, I confessed this to other attendees. Some said they had similar hesitations. After listening to my confession, Shirley Bekins, a housing activist who sat beside me, said with a big smile, “Of course you should be here!”"
  },
  {
    "objectID": "posts/2020-01-30-cade-workshop-refelection/index.html#references",
    "href": "posts/2020-01-30-cade-workshop-refelection/index.html#references",
    "title": "Reflection on USF tech policy and data ethics workshop",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hongsup Shin",
    "section": "",
    "text": "Tech volunteering tips for nonprofits\n\n\n\n\n\n\n\nvolunteering\n\n\njournalism\n\n\nML\n\n\n\n\nLessons I’ve learned from my own experience by working with various nonprofit organizations such as DataKind and Texas Justice Initiative\n\n\n\n\n\n\nMay 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPolice shooting in Texas 2016-2019\n\n\n\n\n\n\n\ncriminal justice\n\n\nvisualization\n\n\nEDA\n\n\nvolunteering\n\n\njournalism\n\n\n\n\nJupyter Notebook on police shooting analysis in Texas from 2016 to 2019 (done in collaboration with Texas Justice Initiative)\n\n\n\n\n\n\nMay 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFAccT 2021. Journalism, data leverage, education, and language models\n\n\n\n\n\n\n\nconference\n\n\njournalism\n\n\nmeasurement\n\n\neducation\n\n\nlanguage models\n\n\nresponsible AI\n\n\nML\n\n\n\n\nSummary of Day 3 at FAccT 2021. Julian Anguin’s Markup, language models, measurements, and data average\n\n\n\n\n\n\nMar 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFAccT 2021. Automated decision-making, causal accountability, and robustness\n\n\n\n\n\n\n\nconference\n\n\nexplainability\n\n\ncausality\n\n\nrobustness\n\n\nresponsible AI\n\n\nML\n\n\n\n\nSummary of Day 2 at FAccT 2021. Automated decision-making, accountability and recourse, and model robustness\n\n\n\n\n\n\nMar 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFAccT 2021. AI audit, governance, and trustworthiness\n\n\n\n\n\n\n\nconference\n\n\ngovernance\n\n\nexplainability\n\n\nresponsible AI\n\n\nML\n\n\n\n\nSummary of Day 1 at FAccT 2021. Algorithm audit, impact assessment, data governance, trust in AI, and explainable AI\n\n\n\n\n\n\nMar 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTutorials at FAccT 2021\n\n\n\n\n\n\n\nconference\n\n\ncausality\n\n\nfairness\n\n\nexplainability\n\n\nresponsible AI\n\n\nML\n\n\n\n\nFAccT 2021 (virtual) tutorial summary. Causal analysis, XAI, and algorithmic impact\n\n\n\n\n\n\nMar 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\nMarkdown and GitHub for scientific writing\n\n\n\n\n\n\n\ncollaboration\n\n\n\n\nHow to use GitHub to publish and review academic manuscripts for better tracking, communication, and transparency\n\n\n\n\n\n\nNov 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\nEfficient bug discovery with ML for hardware verification\n\n\n\n\n\n\n\nML\n\n\nverification\n\n\n\n\nMy Arm Research blog post about using ML in hardware engineering to make verification more compute-efficient\n\n\n\n\n\n\nSep 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nCritiquing and rethinking at ACM FAT 2020\n\n\n\n\n\n\n\nconference\n\n\nfairness\n\n\nML\n\n\n\n\nSummary of FAT 2020 in Barcelona, Spain. “Critiquing and rethinking” was their new attempt to open up a discussion between multidisciplinary stakeholders\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\nReflection on USF tech policy and data ethics workshop\n\n\n\n\n\n\n\nethics\n\n\n\n\nA reflection piece about the USF tech ethics and policy workshop focusing on data ethics as a tech worker myself\n\n\n\n\n\n\nJan 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\nFairness in ML at ACM FAT 2019\n\n\n\n\n\n\n\nconference\n\n\nfairness\n\n\nML\n\n\n\n\nSeveral key moments from the conference and my thoughts at the FAT (Fairness, Accountability, and Transparency in ML) conference\n\n\n\n\n\n\nFeb 1, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Hi there!",
    "section": "",
    "text": "I’m Hongsup (홍섭) and I am a Principal ML Research Engineer at Arm. Until recently, I also volunteered at Texas Justice Initiative, a criminal justice non-profit in Austin. My background is computational neuroscience (PhD) and behavioral ecology (MSc). Also, I’m a native speaker of Korean. If you have any questions, feel free to reach out to me via email.\nIf you want to learn more about my work, here are some examples:\n\nIEEE System-on-Chip 2022 paper on data-centric ML in hardware engineering\nScipy 2019 paper on ML Ops case study of building ML apps for hardware verification\nPolice shooting analysis in Texas 2016-2019 report (collaboration with Texas Justice Initiative)\nMedium post on simulationg relationship between talent, luck, and meritocracy\nPNAS paper on human visual short-term memory from my PhD thesis"
  }
]