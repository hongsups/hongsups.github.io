<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Hongsup Shin</title>
<link>https://hongsupshin.github.io/</link>
<atom:link href="https://hongsupshin.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.40</generator>
<lastBuildDate>Sun, 12 Jan 2025 06:00:00 GMT</lastBuildDate>
<item>
  <title>Building Effective ML Teams - Lessons from Industry</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2025-01-12/</link>
  <description><![CDATA[ 





<p>After a decade as an ML practitioner in industry, I have noticed an interesting dichotomy in how ML is perceived in corporate settings. Senior stakeholders typically fall into two opposing camps.</p>
<p>The Skeptics are interested in adopting ML, but they overestimate the risks of ML product developtment. They treat standard ML workflows—investigation, experimentation, and evaluation—as if they were experimental research. Proven ML solutions are unnecessarily scrutinized and methodological experiments are considered exploratory and academic.</p>
<p>The Believers view ML as a magical solution for any problem. They fully buy into AI hype and greenlight ML projects without understanding limitations or real business value. They also rush to production even when the solution is half-baked and needs validation.</p>
<p>When it comes to ML discussion, the ML community often talks about state-of-the-art technology but rarely about how ML teams actually get things done. ML teams have to navigate between the two extremes, on the one hand building confidence in ML solutions, and on the other setting realistic expectations about capabilities and limitations. While working with the stakeholders, ML teams must also maintain their autonomy and technical integrity without sacrificing efficiency.</p>
<section id="research-excellence-and-judgment" class="level2">
<h2 class="anchored" data-anchor-id="research-excellence-and-judgment">Research Excellence and Judgment</h2>
<p>Good research judgment is at the core of navigating these extremes. This goes beyond technical expertise and includes: - Intuition for distinguishing experimental approaches from proven solutions - Understanding when to invest in deep research versus applying established tools - Accurately evaluating when a problem is truly solved and whether it is usable and scalable</p>
<p><a href="http://joschu.net/blog/opinionated-guide-ml-research.html">As John Schulman, an AI researcher at Anthropic has previously mentioned</a>, this is about developing research <em>taste</em>. I have seen teams spend months optimizing problems that could have been solved in a simple way with well-studied existing tools. At the other extreme I have seen teams scale solutions without proper evaluation.</p>
<p>Good research judgment is invaluable when working with stakeholders. For Skeptics, it helps create rigorous validation processes that then build trust through small wins and case studies. For Believers, it helps set realistic technical expectations and encourage stakeholders to play an active role in product development.</p>
<p>Good judgment also implies iterating quickly. Teams must constantly assess whether their approach solves the core problem, and if not, pivot quickly to an alternative solution that still delivers value.</p>
<p>Pivoting sounds easy but in practice, it is not. People naturally become attached to their ideas, and they often get anxious about negative consequences of negative results. Therefore, it is important to have good judgment about how long to pursue an idea versus when to finally abandon it. I’ve seen brilliant colleagues try to force solutions to work despite a lack of evidence. True research maturity includes accepting and learning from negative results.</p>
</section>
<section id="fostering-open-culture-with-clear-technical-vision" class="level2">
<h2 class="anchored" data-anchor-id="fostering-open-culture-with-clear-technical-vision">Fostering Open Culture with Clear Technical Vision</h2>
<p>Research excellence and team culture reinforce each other. Strong research judgment often results from meaningful technical discussions. Teams need a culture where open discussion is encouraged and civil discourse prevails. Without a culture of trust and respect, innovative ideas remain unspoken, and suboptimal solutions go uncontested.</p>
<p>This is why teams need technical leaders with clear vision. A sense of working toward common goals boosts morale and nurtures healthy team dynamics. Clear vision in leaders requires them to have good research judgment, which is why having ML practitioners in management proves crucial. When organizations underestimate researchers’ leadership skills, and exclude them from strategic discussions, it creates a gulf between technical and business objectives in ML projects.</p>
<p>Teams built with clear vision develop conviction when communicating with stakeholders. Clients and stakeholders without strong ML expertise often suggest unrealistic solutions. Instead of agreeing to everything, teams and their tech leads must discern the relevant demands while continuously proving value to the stakeholders. An especially critical role for good research judgment is to tell the stakeholders when ML is not needed. Doing so protects teams from overpromising, helps them establish autonomy, and eventually earns them more trust from stakeholders.</p>
</section>
<section id="establishing-technical-standards" class="level2">
<h2 class="anchored" data-anchor-id="establishing-technical-standards">Establishing Technical Standards</h2>
<p>Even teams with strong culture and technical excellence can collaborate inefficiently. A simple solution is to build clear technical standards. When a team shares a common understanding of technical quality, reviewing and critiquing work becomes less burdensome, and product quality improves quickly.</p>
<p>Technical standards also include reproducibility, benchmarking, and rigorous evaluation. While staying current with new ideas is crucial in the fast-moving ML field, it shouldn’t come at the expense of product quality. Without clear standards, comparing different approaches becomes challenging, leading to technical debt.</p>
<p>When developing standards, we should avoid rigid and overly complicated standards, which hinder innovation and slow down experiments. Knowing where to enforce stricter compliance also matters: the cost of neglecting model evaluation standards is higher than ignoring code style.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>After a decade in industry, I’ve learned that technical excellence alone is insufficient for successful ML projects. As ML has become ubiquitous, technical standards have soared, yet truly effective ML teams remain rare. And with ML tools and expertise becoming increasingly democratized, the factors differentiating teams will be clear: research excellence, strong technical vision, open culture, and thoughtful technical standards. Look for these elements to separate teams that merely dabble in ML from those that transform their organizations through it.</p>


</section>

 ]]></description>
  <category>collaboration</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2025-01-12/</guid>
  <pubDate>Sun, 12 Jan 2025 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Building datasets for model benchmarking in production</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2024-10-05/</link>
  <description><![CDATA[ 





<p>At its core, benchmarking serves as a systematic methodology for objectively evaluating techniques or products through carefully curated test samples. In ML, this practice has traditionally been used in academic research, enabling rigorous comparison of various deep learning models. However, its utility extends beyond research, and it becomes an invaluable tool in production environments, similar to A/B testing for new feature evaluation. Here, we can construct benchmark datasets from historical data, tailoring the evaluation process to our own needs in real-world scenarios.</p>
<p>In this post, I am focusing on ML model benchmarking, which primarily evaluates model performance and its robustness, and other factors such as training cost and inference latency. While the landscape of ML benchmarking is dominated by image and text datasets designed for deep learning research, I would like to focus on a less traversed yet equally critical domain: tabular benchmark datasets in production environments. We’ll examine the practical considerations that arise when applying benchmarking to real-world applications.</p>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h2>
<p>When building benchmark datasets, data preprocessing is typically conducted so that the data are ready for model training, removing the computational burden of data preparation. Although this is a common practice, for data-centric AI, experimentation demands raw data for optimizing preprocessing pipelines or refining feature engineering methodologies. In this case, I propose saving fitted preprocessors within the benchmark itself to only spend compute on data transformation but not preprocessing.</p>
</section>
<section id="data-split-and-reproducibility" class="level2">
<h2 class="anchored" data-anchor-id="data-split-and-reproducibility">Data split and reproducibility</h2>
<p>Deep learning benchmarks, constrained by computational costs, typically employ a train-validation-test split. For instance, the <a href="https://www.tensorflow.org/datasets/catalog/imagenet2012">ImageNet dataset</a> on TensorFlow shows the <code>'train', 'validation', 'test'</code> splits. In contrast, tabular ML often uses cross-validation. Rather than redundantly storing multiple datasets for all folds, I suggest saving train-test index tuples, which can be easily converted to cross-validator iterator. This is also more direct and versatile than storing cross-validator parameters such as parameters of sklearn’s <code>KFold</code>.</p>
</section>
<section id="data-curation" class="level2">
<h2 class="anchored" data-anchor-id="data-curation">Data curation</h2>
<p>While having high-quality and unbiased benchmark datasets is important, the process of data curation in production environments requires a more thoughtful approach. Rather than simply dismissing outliers (and labeling them as simply <em>bad</em>), I suggest still including them while flagging them for separate analysis during benchmarking. This approach not only preserves the integrity of your dataset but also may discover new insights into your data.</p>
</section>
<section id="data-staleness" class="level2">
<h2 class="anchored" data-anchor-id="data-staleness">Data staleness</h2>
<p>In research settings, benchmark datasets are static and rarely updated. In production environments, however, this can be a problem because of data staleness, leading to the decreased representativeness of the benchmark. Drawing inspiration from A/B testing, I recommend periodically refreshing benchmark datasets or building sufficiently diverse datasets that cover the wide spectrum of product lifecycle.</p>
</section>
<section id="overfitting-to-the-benchmark-data" class="level2">
<h2 class="anchored" data-anchor-id="overfitting-to-the-benchmark-data">Overfitting to the benchmark data</h2>
<p><a href="https://arxiv.org/abs/2405.00332">A recent paper</a> has showed the susceptibility of LLMs to overfitting for public benchmark datasets because the benchmark datasets are repeatedly used when designing models. The iterative nature of ML development cycle creates a similar risk of information leakage through repeated test set exposure. To address this, I recommend a more strict test set isolation protocol or regular update to benchmark dataset (addressing data staleness as a bonus).</p>
</section>
<section id="other-considerations" class="level2">
<h2 class="anchored" data-anchor-id="other-considerations">Other considerations</h2>
<p>Beyond these, there are other important factors to consider. If we can define a measure of diversity of benchmark (e.g., subgroup diversity), it’s important that the benchmark is diverse and inclusive. Resource optimization, particularly in terms of data volume, must be balanced according to experiment design. Perhaps most critically, benchmark dataset development must go thorough privacy, security, and ethical evaluations as well to guarantee they meet regulation standards.</p>


</section>

 ]]></description>
  <category>ML</category>
  <category>ML Ops</category>
  <guid>https://hongsupshin.github.io/posts/2024-10-05/</guid>
  <pubDate>Sat, 05 Oct 2024 05:00:00 GMT</pubDate>
  <media:content url="https://datavizproject.com/wp-content/uploads/types/Bar-Chart-Vertical.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Modeling Tabular Data using Conditional GAN</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2024-05-30/</link>
  <description><![CDATA[ 





<p>At work, I train models using tabular data with extreme class imbalance, which is often quite challenging. To tackle this, I have been looking into data augmentation techniques for tabular data, which is a relatively understudied domain compared to unstructured data. <a href="https://docs.sdv.dev/sdv">Synthetic Data Vault</a> is a popular python library for tabular data synthesis, and the main techniques originate from the paper above. Since class imbalance is a common problem in real-world data and most of us work with tabular data, I think this will be an interesting paper to discuss. I presented the paper in Austin ML Journal Club in May 2024.</p>
<section id="paper-summary" class="level2">
<h2 class="anchored" data-anchor-id="paper-summary">Paper summary</h2>
<p>Several challenges lie in tabular data synthesis. They often have mixed data types with non-Gaussian or multi-modal distributions. Categorical variables in a tabular dataset often have severe class imbalance and provide insufficient samples. The authors argue that GAN’s flexibility can address these problems. The authors introduce several techniques for their approach, conditional-tabular GAN (CT-GAN).</p>
<section id="mode-specific-normalization" class="level3">
<h3 class="anchored" data-anchor-id="mode-specific-normalization">Mode-specific normalization</h3>
<p>The basic idea is mode-specific normalization is to convert a continuous-variable column into a set of columns where each column represents an estimated mode (like an indicator) and its value represents the weight of the corresponding mode. To estimate the number of the modes, they use variational Gaussian mixture (VGM) method.</p>
</section>
<section id="conditional-generator-and-training-by-sampling" class="level3">
<h3 class="anchored" data-anchor-id="conditional-generator-and-training-by-sampling">Conditional generator and training-by-sampling</h3>
<p>Traditional GANs have Gaussian assumption where a vector is sampled from a standard multivariate normal distribution. However, this doesn’t account for class imbalance, which is a problem in tabular data. So the goal here is to resample data so that all categories are sampled <em>evenly</em> during training, and to estimate the conditional distribution of rows given that particular value at that particular column.</p>
</section>
<section id="ct-gan-model" class="level3">
<h3 class="anchored" data-anchor-id="ct-gan-model">CT-GAN model</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2024-05-30/Fig 2.png" class="img-fluid figure-img"></p>
<figcaption>CT GAN model</figcaption>
</figure>
</div>
<p>In the CT-GAN model, the data are represented Conditional vector where each discrete column is a one-hot vector. The mask vector is used to only represent a category of a single discrete column (in one-hot way, the rest is all zero). The generator loss is cross-entropy between <img src="https://latex.codecogs.com/png.latex?m_%7Bi%5E*%7D"> (given by data) and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bd%7D_%7Bi%5E*%7D"> (produced by the generator) averaged over all rows in a batch. During the critic assessment stage, the model measures distance between learned conditional distribution and conditional distribution on real data <em>while exploring all possible values evenly</em>.</p>
</section>
<section id="model-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="model-evaluation">Model evaluation</h3>
<section id="baseline-models" class="level4">
<h4 class="anchored" data-anchor-id="baseline-models">Baseline models</h4>
<p>The paper uses multiple baseline models for performance comparison. They use two Bayesian network models (CLBN and PrivBN), other GAN models (MedGAN, VeeGAN, TableGAN), and a tabular VAE (TVAE) model that they built. The TVAE model has multivariate Gaussian assumption.</p>
</section>
<section id="evaluation-metrics" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-metrics">Evaluation metrics</h4>
<p>For model comparison, the authors use two metrics. Likelihood fitness metric which estimates how well the synthetic data can fit the data oracle well. They also use <em>ML efficacy</em>, which measures whether ML model performance from synthetic data is comparable to that from using real data.</p>
</section>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<p>For model comparison, they used several different sets of data. First, they used multi-modal Gaussian-simulated data. Here, other GANs suffer from model collapse and the CT-GAN showed better performance. When simulated data from Bayesian networks were used, the Bayesian baseline models (CLBN and PrivBN) performed the best. With real-world data, TVAE and CTGAN performed the best although TVAE was often better.</p>
</section>
</section>
</section>
<section id="journal-club-discussion" class="level2">
<h2 class="anchored" data-anchor-id="journal-club-discussion">Journal club discussion</h2>
<p>Even though we were pleased to see good baseline comparison with multiple different datasets, we thought the authors should have did more thorough work on literature review such as comparing their work with <code>synthpop</code>, a widely-used data synthesis package in R. They also could have looked into other statistical models because data synthesis an active research area in statistics.</p>
<p>We also questioned the validity of the VGM technique because it is the core of creating the embedding of continuous variables. The proposed model does not have checks that evaluate the fidelity of VGM estimation, and thus poor estimation of a multi-model distribution by it can further compromise model performance in general.</p>
<p>We also thought their claim on CT-GAN was a bit exaggerated. First, in most benchmark datasets, it was actually TVAE model that performed the best. In the performance comparison table, there was only one regression data, which we thought insufficient to evaluate the <em>ML efficacy</em> metric the authors proposed. Finally, the proposed models for the regression problem showed <em>negative</em> R scores, suggesting that the models performed quite poorly.</p>
<p>Overall, we thought the approach was novel but the evaluation could have been improved given that tabular datasets are quite diverse. Also, given that GAN is expensive train, we would have liked to learn more practical aspects of the implementation.</p>


</section>

 ]]></description>
  <category>paper</category>
  <category>GenAI</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2024-05-30/</guid>
  <pubDate>Thu, 30 May 2024 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2024-05-30/Fig 1.png" medium="image" type="image/png" height="128" width="144"/>
</item>
<item>
  <title>Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2024-03-28/</link>
  <description><![CDATA[ 





<p>Fresh out of academia and at my first job, I remember being surprised by the great power I was able to wield as the main data scientist in the team. A few lines of my code could easily have cascading impact on business decisions. And (as is often the case) since management didn’t care much about technical details, this power gave me a sense of grave responsibility, which was honestly often terrifying. To this day, this sense is something I try to remind myself of, especially because ML systems are getting more complex and we still have very little accountability for ML. So in a way, every ML practitioner is the first line of defense. And this responsibility is more critical if one is working in an high-impact domain such as healthcare.</p>
<p>These days it feels like ML is all about LLMs and AI assistants. But algorithmic bias is still widespread, and unfortunately it’s even more overshadowed by this massive hype. This seminal paper from 2019 identified racial bias in a healthcare algorithm and discussed the problem of label choice bias. I find this paper still relevant because this bias can easily sneak when building datasets and algorithms. Since the paper is a few years old, it will be interesting to discuss what’s happened since then. I presented the paper in Mar 2024 in Austin ML Journal Club.</p>
<section id="paper-summary" class="level2">
<h2 class="anchored" data-anchor-id="paper-summary">Paper summary</h2>
<p>This is a famous paper in Fair ML field that’s cited frequently. One reason is that this is one of the first algorithm audit papers that used a creative approach to dissect an algorithm already used in public. The first author Ziad Obermeyer is an active figure in algorithmic bias specifically in healthcare, and the corresponding author Sendhil Mullainathan is a seminal figure in behavioral economics.</p>
<p>The main message of the paper is simple and straightforward. In a widely used healthcare algorithm, the authors found that given a score generated by a trained algorithm, which predicts how sick a patient is, Black patients were much sicker than the White. By using creative approaches, the authors found that this algorithmic behavior originated from label choice bias. The algorithm used healthcare cost as target, which the ML practitioners considered as a proxy for a patient’s sickness. The authors cautioned that seemingly effective proxies for ground truth can cause algorithmic bias.</p>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background">Background</h3>
<p>The authors discussed the difficulty of empirical investigation on commercially used algorithms because they are considered proprietary, and thus researchers have to work “from the outside”. They can come up with creative methods but these can be still limiting because without knowing how the model works, it is difficult to understand how and why. For instance, we need training data, objective function, prediction method, and so on, to have a deeper understanding of an algorithm.</p>
<section id="high-risk-care-management-system" class="level4">
<h4 class="anchored" data-anchor-id="high-risk-care-management-system">High-risk care management system</h4>
<p>This study is about a high-risk care management system often used in hospitals to provide care for complex medical needs, which tend to be quite expensive. In essence, hospitals want to prioritize patients so that they can optimize which patients will benefit the most by optimizing resource allocation. For instance, early identification of a high risk patient can reduce expensive cost later, like an ER visit. The authors said healthy care systems rely on algorithms <em>extensively</em> for this type of care management. This type of algorithm actually has many analogies, and an algorithm like this is often called as a risk assessment tool, which is also used in finance, criminal justice, and so on.</p>
</section>
<section id="who-will-benefit-the-most" class="level4">
<h4 class="anchored" data-anchor-id="who-will-benefit-the-most">“Who will benefit the most?”</h4>
<p>The authors emphasized that this is a difficult <em>causal inference</em> problem. We need to estimate individual treatment effects after the resources are allocated. The underlying assumption is “those with the greatest needs will benefit the most,” and thus we are setting <em>future health care needs</em> as main target.</p>
</section>
</section>
<section id="the-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-algorithm">The algorithm</h3>
<p>For this study, the authors had a unique advantage to have access to training data, objective function, and prediction. This is mainly due to that the first author worked at the hospital which used the algorithm and he had access.</p>
<p>The data are from a large academic hospital, collected between 2013 and 2015. The authors focused on the white vs.&nbsp;black relationship to examine racial bias. There were about 6k black patients and 40k white patients. For the algorithm, <strong>total medical expenditure</strong> (<img src="https://latex.codecogs.com/png.latex?C">) is used as target variable. The input feature data are collected from insurance claim data, which includes demographics, insurance type, ICD-9 (international classification of diseases code), medications, medical encounters such as surgery or radiology, and so on. The authors shared the code and data <a href="https://gitlab.com/labsysmed/dissecting-bias/-/tree/master">here</a> and we can take a look at the data ourselves.</p>
<div id="660576c6" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-5">plt.style.use(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ggplot'</span>)</span>
<span id="cb1-6"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>config InlineBackend.figure_format<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retina'</span></span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span></code></pre></div>
</details>
</div>
<div id="0f908394" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv"</span>)</span>
<span id="cb2-2">data.head()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">risk_score_t</th>
<th data-quarto-table-cell-role="th">program_enrolled_t</th>
<th data-quarto-table-cell-role="th">cost_t</th>
<th data-quarto-table-cell-role="th">cost_avoidable_t</th>
<th data-quarto-table-cell-role="th">bps_mean_t</th>
<th data-quarto-table-cell-role="th">ghba1c_mean_t</th>
<th data-quarto-table-cell-role="th">hct_mean_t</th>
<th data-quarto-table-cell-role="th">cre_mean_t</th>
<th data-quarto-table-cell-role="th">ldl_mean_t</th>
<th data-quarto-table-cell-role="th">race</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">trig_min-high_tm1</th>
<th data-quarto-table-cell-role="th">trig_min-normal_tm1</th>
<th data-quarto-table-cell-role="th">trig_mean-low_tm1</th>
<th data-quarto-table-cell-role="th">trig_mean-high_tm1</th>
<th data-quarto-table-cell-role="th">trig_mean-normal_tm1</th>
<th data-quarto-table-cell-role="th">trig_max-low_tm1</th>
<th data-quarto-table-cell-role="th">trig_max-high_tm1</th>
<th data-quarto-table-cell-role="th">trig_max-normal_tm1</th>
<th data-quarto-table-cell-role="th">gagne_sum_tm1</th>
<th data-quarto-table-cell-role="th">gagne_sum_t</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1.987430</td>
<td>0</td>
<td>1200.0</td>
<td>0.0</td>
<td>NaN</td>
<td>5.4</td>
<td>NaN</td>
<td>1.110000</td>
<td>194.0</td>
<td>white</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>7.677934</td>
<td>0</td>
<td>2600.0</td>
<td>0.0</td>
<td>119.0</td>
<td>5.5</td>
<td>40.4</td>
<td>0.860000</td>
<td>93.0</td>
<td>white</td>
<td>...</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.407678</td>
<td>0</td>
<td>500.0</td>
<td>0.0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>white</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.798369</td>
<td>0</td>
<td>1300.0</td>
<td>0.0</td>
<td>117.0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>white</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>17.513165</td>
<td>0</td>
<td>1100.0</td>
<td>0.0</td>
<td>116.0</td>
<td>NaN</td>
<td>34.1</td>
<td>1.303333</td>
<td>53.0</td>
<td>white</td>
<td>...</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>5 rows × 160 columns</p>
</div>
</div>
</div>
<p>Note that the risk score (<img src="https://latex.codecogs.com/png.latex?R">, <code>risk_score_t</code>) and the predicted cost (<img src="https://latex.codecogs.com/png.latex?C">, <code>cost_t</code>) are not the same. <strong>We do NOT know how the risk score is calculated based on the predicted score.</strong></p>
<div id="2cd7f6e3-d93d-4af6-a896-fbc4abb1a59a" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb3-2">plt.plot(data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cost_t'</span>], data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_score_t'</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>)</span>
<span id="cb3-3">plt.loglog()</span>
<span id="cb3-4">plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'equal'</span>)</span>
<span id="cb3-5">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Predicted Cost'</span>)</span>
<span id="cb3-6">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Risk Score'</span>)</span>
<span id="cb3-7">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2024-03-28/index_files/figure-html/cell-4-output-1.png" width="548" height="528" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The algorithm score was generated for each patient during the enrollment period. After ranking this prediction scores from a group, the top 3% (97th percentile) is automatically flagged as required to be enrolled although this does not guarantee enrollment. The top 45% (55th percentile) is referred to their PCP for further examination.</p>
</section>
<section id="the-algorithm-audit" class="level3">
<h3 class="anchored" data-anchor-id="the-algorithm-audit">The algorithm audit</h3>
<p>As a main method, the authors measured the algorithm calibration to assess fairness across different racial groups. This means that the authors focused on parity of how risk score is calibrated across race. Fair result would satisfy <img src="https://latex.codecogs.com/png.latex?E%5BY%7CR,%20W%5D%20=%20E%5BY%7CR,%20B%5D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Y"> is our main interest, <img src="https://latex.codecogs.com/png.latex?W"> is the White, <img src="https://latex.codecogs.com/png.latex?B"> is the Black, and <img src="https://latex.codecogs.com/png.latex?R"> is the risk score.</p>
<p>To dissect the algorithm, the authors compared the distribution of <img src="https://latex.codecogs.com/png.latex?R_%7Bi,%20t%7D%7CH_%7Bi,%20t%7D"> and <img src="https://latex.codecogs.com/png.latex?R_%7Bi,%20t%7D%7CC_%7Bi,%20t%7D"> between White and Black patients where - <img src="https://latex.codecogs.com/png.latex?R_%7Bi,%20t%7D">: risk score given patient <img src="https://latex.codecogs.com/png.latex?i"> in year <img src="https://latex.codecogs.com/png.latex?t"> given the input feature <img src="https://latex.codecogs.com/png.latex?X_%7Bi,%20t-1%7D"> from the previous year - <img src="https://latex.codecogs.com/png.latex?H_%7Bi,%20t%7D">: realized health - <img src="https://latex.codecogs.com/png.latex?C_%7Bi,%20t%7D">: actual cost</p>
<p>To measure <img src="https://latex.codecogs.com/png.latex?H">, the authors used health record data, diagnoses, lab results, and vital sign data. As a main measure of realized health, the authors used comorbidity score, the total number of chronic illnesses over a year, which is a measure of medical complexity. The cost variable <img src="https://latex.codecogs.com/png.latex?C"> includes insurance claims on utilization, outpatient/ER visit, hospitalization, and general health care costs.</p>
</section>
<section id="using-a-counterfactual-scenario-to-describe-racial-bias-fig.-1b" class="level3">
<h3 class="anchored" data-anchor-id="using-a-counterfactual-scenario-to-describe-racial-bias-fig.-1b">Using a counterfactual scenario to describe racial bias (Fig. 1B)</h3>
<p>Fig. 1A shows mean comorbidity vs.&nbsp;risk score. At a given risk score, Blacks have significantly more illness than Whites. For instance, at the 97th percentile, mean comorbidity score was 4.8 for Black and 3.8 for White. This means that sicker black patients and healthier white patients can have the same score, causing substantial disparities in program screening. The authors designed a counterfactual scenario with no racial bias, and described the severity of the disparity.</p>
<p><strong>We found it difficult to understand how the authors measured the disparity between the counterfactual and actual</strong>. Fortunately, the code that the authors shared helped us. Here is how this assessment was done in the paper:</p>
<ol type="1">
<li>At a given percentile (e.g., 97th percentile) identify the following:
<ul>
<li>Group “White-above”: White patients whose risk score is above the threshold</li>
<li>Group “Black-above”: Black patients whose risk score is above the threshold</li>
</ul></li>
<li>If the comorbidity score of the healthiest patient in the White-above group is <em>lower</em> than that of the sickest patient in the Black-lower group, remove the healthiest white patients from White-above, and move the sickest Black patient from the Black-lower above the threshold.</li>
<li>Repeat this process until patients cannot be moved any more.</li>
</ol>
<p>The second step can be considered as a White patient who shouldn’t be in the enrollment group (above the threshold) giving their spot to the sicker black patient, who was supposed to be in the enrollment group. The authors’ original code was written in R but I translated it into python to further investigate this simulation.</p>
<div id="2cfdc3c9" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> setup(data, default_in_percentile<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">95</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">97</span>]):</span>
<span id="cb4-2">    cohort <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'race'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_score_t'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>]]</span>
<span id="cb4-3">    dt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cohort.copy()</span>
<span id="cb4-4">    dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_pctile'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.cut(dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_score_t'</span>], bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>np.unique(np.percentile(dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_score_t'</span>], np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">101</span>))), include_lowest<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb4-5">    </span>
<span id="cb4-6">    enroll_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(default_in_percentile), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb4-7">    enroll_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(enroll_stats, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black_before'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black_after'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ratio'</span>])</span>
<span id="cb4-8">    enroll_stats.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> default_in_percentile</span>
<span id="cb4-9">    </span>
<span id="cb4-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dt'</span>: dt, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'enroll_stats'</span>: enroll_stats}</span></code></pre></div>
</details>
</div>
<div id="81e4949a" class="cell" data-execution_count="49">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">default_in_percentile <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">97</span>]</span>
<span id="cb5-2">j <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb5-3"></span>
<span id="cb5-4">dt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> setup(data, default_in_percentile)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dt'</span>]</span>
<span id="cb5-5">enroll_stats <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> setup(data, default_in_percentile)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'enroll_stats'</span>]</span>
<span id="cb5-6"></span>
<span id="cb5-7">prior_enrolled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dt[dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_pctile'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> default_in_percentile[j]]</span>
<span id="cb5-8">prior_w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prior_enrolled[prior_enrolled[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'race'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span>]</span>
<span id="cb5-9">prior_b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> prior_enrolled[prior_enrolled[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'race'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>]</span>
<span id="cb5-10"></span>
<span id="cb5-11">upperb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dt[(dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_pctile'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> default_in_percentile[j]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'race'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>)]</span>
<span id="cb5-12">upperw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dt[(dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_pctile'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> default_in_percentile[j]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'race'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span>)]</span>
<span id="cb5-13">lowerb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dt[(dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_pctile'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> default_in_percentile[j]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> (dt[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'race'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>)]</span>
<span id="cb5-14"></span>
<span id="cb5-15">upperw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> upperw.sort_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>)</span>
<span id="cb5-16">lowerb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lowerb.sort_values([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'risk_score_t'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>], ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>])</span>
<span id="cb5-17"></span>
<span id="cb5-18">upperb_actual <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> upperb.copy()</span>
<span id="cb5-19">upperw_actual <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> upperw.copy()</span>
<span id="cb5-20">lowerb_actual <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lowerb.copy()</span></code></pre></div>
</details>
</div>
<div id="106954f1" class="cell" data-execution_count="50">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">sw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb6-2">sb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb6-3">switched_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb6-4">switched_w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb6-5">switched_b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb6-6"></span>
<span id="cb6-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> sb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> lowerb.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]:</span>
<span id="cb6-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> upperw.iloc[sw][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> lowerb.iloc[sb][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>]:</span>
<span id="cb6-9">        switched_w.append(upperw.iloc[sw])</span>
<span id="cb6-10">        switched_b.append(lowerb.iloc[sb])</span>
<span id="cb6-11"></span>
<span id="cb6-12">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># healthiest white patient is dropped and sickest black patient is added</span></span>
<span id="cb6-13">        upperb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([upperb, pd.DataFrame(lowerb.iloc[sb]).T], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb6-14">        upperw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> upperw.drop(upperw.index[sw])</span>
<span id="cb6-15">        upperw <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> upperw.sort_values(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>)</span>
<span id="cb6-16">        </span>
<span id="cb6-17">        sb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb6-18">        switched_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb6-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb6-20">        sb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb6-21">        switched_count <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> switched_count</span></code></pre></div>
</details>
</div>
<div id="f4c85304" class="cell" data-execution_count="51">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">fig, axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>), sharex<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, sharey<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-2">sns.kdeplot(upperw_actual[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'White-upper'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb7-3">sns.kdeplot(upperb_actual[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Black-upper'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb7-4">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(xlabel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Comorbidity score'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Actual'</span>)</span>
<span id="cb7-5">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].legend()</span>
<span id="cb7-6"></span>
<span id="cb7-7">sns.kdeplot(upperw[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'White-upper'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb7-8">sns.kdeplot(upperb[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gagne_sum_t'</span>], label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Black-upper'</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb7-9">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(xlabel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Comorbidity score'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Counterfactual'</span>)</span>
<span id="cb7-10">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].legend()</span>
<span id="cb7-11"></span>
<span id="cb7-12">fig.suptitle(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'97th percentile'</span>, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.95</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span>
<span id="cb7-13">fig.tight_layout()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2024-03-28/index_files/figure-html/cell-8-output-1.png" width="789" height="386" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The left figure shows the distribution of comorbidity score of white and black patients who are above the 97th percentile threshold, identified by the algorithm as those who need the high-risk care. The white patient distribution is tilted to the left and the black patient’s has more density towards right, indicating that white patients are generally healthier thant the black patients in the above-threshold group. The right figure is after we simulate the counterfactual scenario by swapping the healthiest white patient’s slot above the threshold with the sickest black patient below the threshold until the swapping cannot be done any more. The distribution now shows that the two distributions are similar.</p>
</section>
<section id="mechanisms-of-bias" class="level3">
<h3 class="anchored" data-anchor-id="mechanisms-of-bias">Mechanisms of bias</h3>
<p>Based on Fig. 3A, the algorithm calibrates cost (target variable) quite well, meaning this is not a problem of prediction quality. Fig. 3B shows that at a given level of health score, black patients generate lower costs than the White, making the algorithm prioritizing the White patients. Tab. S2 shows that how healthcare cost is spent between the black and white patients vary by categories. For instance, the black patients have higher costs related to ER and dialysis and fewer inpatient surgical and outpatient specialist costs, which also suggest that their healthcare cost occurs when the worst scenario happens (ER visit). The negative values mean black patients spend less money in that category.</p>
<p><img src="https://hongsupshin.github.io/posts/2024-03-28/Tab S2.png" class="img-fluid"></p>
<p>There are several socio-economic reasons. First, poor patients face substantial barriers to accessing health care, even the insured ones such as geography or transportation, demands from job and child care, and knowledge of reasons to seek care. There is also cost that is directly affected by race. <a href="https://en.wikipedia.org/wiki/Taste-based_discrimination">Taste-based discrimination</a>, employers’ prejudice or dislikes in an organizational culture rooted in prohibited grounds can have negative results in hiring minority workers, is one, and the doctor-patient relationship is another. It is known that black patients would take preventive care suggested by black provider more and they also have lower trust in health care system in general due to an incident like Tuskegee study. Doctors also may have different perceptions of black patients regarding such as intelligence, affiliation, or pain tolerance.</p>
</section>
<section id="experiments-on-label-choice-and-human-ai-interaction" class="level3">
<h3 class="anchored" data-anchor-id="experiments-on-label-choice-and-human-ai-interaction">Experiments on label choice and human-AI interaction</h3>
<p>The authors then conducted experiments to see alternative label choices other than the total health care cost can result in fairer results. They compared the original model (total cost) against the two alternatives: avoidable cost such as ER visit, and health (comorbidity score). Even though it was not perfect, they found that the algorithms trained on alternative labels produced fairer results.</p>
<p>Finally, the authors examined the interaction between human doctors and the algorithm. The algorithm’s risk score suggests a pool of patients candidates to doctors and they make the ultimate decision on the enrollment. The experiment showed that the doctors can “redress” (=correct) algorithm’s bias but not by much. This indicates that the racial bias produced by the algorithm can have drastic disparaging effect on healthcare management across different race groups that humans cannot correct completely.</p>
</section>
<section id="working-on-solutions-together" class="level3">
<h3 class="anchored" data-anchor-id="working-on-solutions-together">Working on solutions together</h3>
<p>Interestingly, the authors mentioned that they reached out to the manufacturer of the algorithm with their results. Fortunately, they were able to replicate the authors’ results on their own and by collaborating with the authors, they were able to reduce the bias by 84%. With this, the authors ended the paper on a hopeful note. They emphasized that algorithmic bias is fixable and this does not necessarily require changes in the algorithm itself.</p>
<p>The paper triggered multiple investigations and stimulated algorithm audit research in the following years. In the same year in 2019, <a href="https://www.wsj.com/articles/new-york-regulator-probes-unitedhealth-algorithm-for-racial-bias-11572087601">New York regulators started investing UnitedHealth algorithm for racial bias</a> and similarly in <a href="https://oag.ca.gov/news/press-releases/attorney-general-bonta-launches-inquiry-racial-and-ethnic-bias-healthcare">California in 2022</a>. In 2021, <a href="https://www.ftc.gov/business-guidance/blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai">FTC published a guidance document on truth, fairness, and equity in the use of AI in industry</a>. In 2024, there has been a <a href="https://www.finance.senate.gov/chairmans-news/wyden-statement-at-finance-committee-hearing-on-ai-in-health-care">Senate Finance Committee Hearing on AI in healthcare</a> as well.</p>
</section>
</section>
<section id="journal-club-discussion" class="level2">
<h2 class="anchored" data-anchor-id="journal-club-discussion">Journal club discussion</h2>
<p>We first had several questions about the care management system described in the paper. The cost categories listed in the paper seemed to include cost for both patients and hospitals and we were not sure whether the cost allocation mattered at all. Some of us were also curious about the actual outcome of the care management system especially for the patients who were enrolled (and potential racial bias of the realized outcome). We also talked about other demographic features that can cause medical bias such as gender and age, and especially their intersectionality.</p>
<p>We agreed that the technical description of the counterfactual scenario was not clear in the paper, and we spent some time understanding how it worked. Similarly, Tab. 2, where the authors created different models with alternative labels and measured the proportion of the high-risk patients identify each model, was a bit confusing to understand, but fortunately, because the authors shared their code for all analyses, we were able to follow eventually.</p>
<p>We then talked about how to satisfy certain criteria during model training. We questioned the feasibility of satisfying multiple criteria (e.g., fairness metric and accuracy) in healthcare where we often have to work with very small data. This problem will get worsened when considering intersectionality, for instance. And the small data problem naturally leads to safety and privacy issues, especially when data contain sensitive information like in healthcare.</p>
<p>We also discussed practical aspects such as how to roll out an algorithm carefully especially when it touches millions of people. Some companies release their models gradually and share training data publicly, but we were wondering whether there were more specific tips on how ML deployment can evolve while having human in the loop, especially those who are impacted by the algorithm. And finally, we agreed that even though it’s important to understand how algorithmic bias damages our society and people, we were hopeful that we would like to learn about positive and beneficial use case of ML.</p>


</section>

 ]]></description>
  <category>paper</category>
  <category>ML</category>
  <category>ethics</category>
  <category>fairness</category>
  <guid>https://hongsupshin.github.io/posts/2024-03-28/</guid>
  <pubDate>Thu, 28 Mar 2024 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2024-03-28/Fig 1b.png" medium="image" type="image/png" height="151" width="144"/>
</item>
<item>
  <title>Algorithmic Decision-Making and Fairness (Stanford Tech Ethics course, Week 1)</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2023-10-11-ethics-week2/</link>
  <description><![CDATA[ 





<section id="reading-assignments" class="level2">
<h2 class="anchored" data-anchor-id="reading-assignments">Reading assignments</h2>
<section id="weapons-of-math-destruction-introduction-and-chapter-1" class="level3">
<h3 class="anchored" data-anchor-id="weapons-of-math-destruction-introduction-and-chapter-1"><a href="https://archive.org/details/weapons-of-math-destruction_202209">Weapons of Math Destruction</a>, Introduction and Chapter 1</h3>
<p>This book by Cathy O’Neil is always a nice introduction to learning about algorithmic harms. I sometimes gift the book to friends and colleagues who just start their career in the field. The first two chapters of the book is especially enough to understand the general idea.</p>
<p>I was most interested in the D.C.’s Teacher Evaluation System, <a href="https://dcps.dc.gov/page/impact-dcps-evaluation-and-feedback-system-school-based-personnel">IMPACT</a> and its problems. The system uses student performance score to assess teacher performance. This extremely simple assumption itself seems to be highly problematic, in my opinion. My assumption is that the city considered that this would be one of the easiest solutions to act on (i.e., hiring and firing of teachers). After the system is implemented, highly praised teachers sometimes get fired and for them it was really difficult to understand why the decision was made. It also turned out that teachers sometimes cheat and correct students’ scores.</p>
<p>At the end of the first chapter, the author summarized a pattern of algorithimc harms in three ways. First, the models are usually opaque, which reduce accountability and make them inscrutable. Second, often these models can harm the people who are targeted because training data’s bias is simply reproduced. Third, these models are easily scalable and thus generate harms in a much larger scale.</p>
</section>
<section id="case-study-algorithmic-decision-making-and-accountability" class="level3">
<h3 class="anchored" data-anchor-id="case-study-algorithmic-decision-making-and-accountability"><a href="https://drive.google.com/file/d/12R_CKlk1T8GgvSfUXixWlYWPzyhMooAQ/view?usp=sharing">Case Study: Algorithmic Decision-Making and Accountability</a></h3>
<p>This cast study is about an algorithmic transparency bill introduced by a New York City Council, James Vacca back in 2017. The original version of the bill would require city agencies “that use algorithms or other automated processing methods that target services, impose penalties, or police persons to <strong>publish the source code</strong> used for such processing.” Given that this is still something very difficult to practice in 2023, I find this proposal quite radical and refreshing. His motivation made sense though: complete transparency on any public automated decision-making.</p>
<p>Unfortunately the bill has met many criticisms such as privacy concerns, exposure of proprietary information, and effectiveness in public understanding of the source code. The bill was modified several times and the final <em>watered-down</em> version was to create a task force that is comprised of experts in diverse fields that would examine algorithmic decision-making (ADS) tools that the city uses, and make recommendations. The catch though was that various city departments who use these tools are not required to participate in the collaboration with the task force, meaning the participation was voluntary. Also the fact that the task force would simply make recommendations means there is zero enforceability.</p>
<p>I did some digging because I got curious about what happened to the task force. Their website (<a href="https://www.nyc.gov/site/adstaskforce/index.page">New York City Automated Decision Systems Task Force</a>) shows the members of the task force, which actually have active members/researchers in the field of AI ethics. They published <a href="https://www.nyc.gov/assets/adstaskforce/downloads/pdf/ADS-Report-11192019.pdf">a report</a> back in 2019 and at a glance, they do make concrete recommendations on each type of algorithmic decision-making systems in NYC. In Nov 2019, the city seemed to decide to establish an Algorithms Management and Policy Officer (AMPO). But some news articles say this position doesn’t exist any more. Regardless, AMPO did release <a href="https://www.nyc.gov/assets/oti/downloads/pdf/reports/ampo-agency-compliance-cy-2020.pdf">a report</a> in 2020, which has “algorithmic tool directory” that shows how the city’s ADS tools work, which I find pretty useful. Here’s an example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-10-11-ethics-week2/ampo_2020_example.png" class="img-fluid figure-img"></p>
<figcaption>NYC Algorithmic Management and Policy Officer, Summary of Agency Compliance Reporting. Algorithmic Tool Directory example: Department of Correction</figcaption>
</figure>
</div>
</section>
<section id="a-guide-to-solving-social-problems-with-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="a-guide-to-solving-social-problems-with-machine-learning"><a href="https://hbr.org/2016/12/a-guide-to-solving-social-problems-with-machine-learning">A Guide to Solving Social Problems with Machine Learning</a></h3>
<p>This short 2016 guide was written by Jon Kleinberg, Jens Ludwig, and Sendhil Mullainathan. I’ve been to Kleinberg’s talks several times at FAccT and so I was aware of his previous work.</p>
<p>This guide stood out compared to other reading assignments this week because there was mention of the “enthusiasm” about solving social problems with machine learning (ML). <em>If</em> we can implement a ML system to solve these issues, we can provide greater transparency and potentially better and more desirable outcome especially because sometimes human decision-making process is opaque. This guide was also more technical than other reading materials. The authors mention the following tips:</p>
<section id="look-for-policy-problems-that-hinge-on-prediction" class="level4">
<h4 class="anchored" data-anchor-id="look-for-policy-problems-that-hinge-on-prediction">1. Look for policy problems that hinge on prediction</h4>
<p>The authors emphasize that many social-sector decisions do not require a prediction and the practitioners should be careful when formulating the problem. This is mostly related to assessing the predictive algorithm’s social impact.</p>
<blockquote class="blockquote">
<p>“just because something is predictable, that doesn’t mean we are comfortable having our decision depend on that prediction.”</p>
</blockquote>
</section>
<section id="make-sure-you-are-comfortable-with-the-outcome-youre-predicting" class="level4">
<h4 class="anchored" data-anchor-id="make-sure-you-are-comfortable-with-the-outcome-youre-predicting">2. Make sure you are comfortable with the outcome you’re predicting</h4>
<p>Since ML is rooted in statistics, there are cases where one has to deal with higher level of uncertainty in a system. This alone makes the system less reliable and more challenging to work with, but if the risk is also high, it is better to reconsider whether ML is well-suited here. The authors also mention that ML is often good at optimizing a defined metric only <em>at the expense of everything else</em>.</p>
</section>
<section id="check-for-bias" class="level4">
<h4 class="anchored" data-anchor-id="check-for-bias">3. Check for bias</h4>
<p>In this section, the author mentions racial bias in criminal justice system and how the bias can be easily replicated in ML when the training data simply consist of historic incidents. Another important thing they mentioned was that ML is not good for measuring hard-to-define combination of outcomes such as sentencing where society’s sense of retribution, mercy, and redemption should be factored in (and they are all hard to measure).</p>
</section>
<section id="verify-your-algorithm-in-an-experiment-on-data-it-hasnt-seen" class="level4">
<h4 class="anchored" data-anchor-id="verify-your-algorithm-in-an-experiment-on-data-it-hasnt-seen">4. Verify your algorithm in an experiment on data it hasn’t seen</h4>
<p>This section was about evaluating the model with the right validation data. They specifically talk about cases where there are inherent sampling bias in collecting labeled data. For instance, in a ML-based filter system, you end up collecting positively-labeled cases because the negative predictions never come through. They also warn about scalability:</p>
<blockquote class="blockquote">
<p>“It can be misguided, and sometimes outright harmful, to adopt and scale up new predictive tools when they’ve only been evaluated on cases from historical data with labels, rather than evaluated based on their effect on the key policy decision of interest.”</p>
</blockquote>
</section>
<section id="remember-there-is-still-a-lot-we-dont-know" class="level4">
<h4 class="anchored" data-anchor-id="remember-there-is-still-a-lot-we-dont-know">5. Remember there is still a lot we don’t know</h4>
<p>This part was more about how to build a human-in-the-loop system and how human decision-makers can coexist with ADS, which is an active area of human-centered AI (HAI). An example would be how judges use the model prediction from risk assessment tools in decision-making.</p>
</section>
</section>
<section id="algorithmic-accountability-a-primer" class="level3">
<h3 class="anchored" data-anchor-id="algorithmic-accountability-a-primer"><a href="https://datasociety.net/library/algorithmic-accountability-a-primer/">Algorithmic Accountability: A Primer</a></h3>
<p>This primer was written by <a href="https://datasociety.net/">Data &amp; Society</a>, an independent nonprofit research organization about tech ethics and governance. This was an excellent overview of ADS and algorithmic accountability.</p>
<p>It gives a summary of <a href="https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say">ProPublica’s COMPAS investigation</a>. It says the two issues of this incident are 1) lack of standard definition of algorithmic bias (related to the debate between Northpointe and Propublica’s conflicting opinions about what is fair) and 2) lack of mechanism for algorithmic accountability (wasn’t mentioned here but probably related to <a href="https://en.wikipedia.org/wiki/Loomis_v._Wisconsin">Loomis v. Wisconsin</a>).</p>
<p>Then it lists several key issues related to ADS. Regarding fairness and bias, it pointed out that algorithms can be quickly outdated unless they are consistently monitored and adjusted. About opacity and transparency, it mentioned that transparency can be exploited by bad actors gaming the system. Importantly, it mentioned the <em>repurposing</em> of algorithms and data meaning algorithms built for a problem can be repurposed and applied to a completely different problem (PredPol using an algorithm built for earthquake modeling). Finally it mentioned the lack of standards for auditing.</p>
<p>Regarding algorithmic accountability, which the primer defines as <strong>“the assignment of responsibility for how an algorithm is created ans its impact on society”</strong>, the lack of this makes me think the moral disengagement paper we read last week. In terms of the enforcement and regulation, it did mention a possibility of self-regulation by corporations but as I expected (and agreed), it said this approach is often ad hoc and out of control of citizens and governments, which is unreliable.</p>
</section>
</section>
<section id="the-lecture-and-talk-with-rumman-chowdhury" class="level2">
<h2 class="anchored" data-anchor-id="the-lecture-and-talk-with-rumman-chowdhury">The lecture and talk with Rumman Chowdhury</h2>
<p>This week’s lecture was led by Professor Mehran Sahami, who talked about algorithmic decision-making, different definitions of fairness, and their relationship to the COMPAS incident. Among many definitions of fairness, we covered anti-classification, classification parity, calibration, and lack of disparate impact. The first three are mathematically well-defined and the last one focuses on empirical evidence. Regarding the COMPAS case, he mentioned that Northpointe’s argument was that their model is fair because from the perspective of the <em>calibration</em> definition of fairness, their model had similar predictive probability between white and black based on their risk factors. But as our guest speaker for the week, Rumman Chowdhury pointed out, Nortpointe’s survey questions included irrelevant and bias-reproducing questions such as whether a person grew up in a poor neighborhood or knew people who went to jail, etc.</p>
<p>During Rumman Chowdhury’s session, she mentioned several interesting points. First, she summarized the problem of ADS as “models are good at generalization but bad at specificity.” Second, regarding her experience in Twitter, she mentioned that working on a product team by implementing research ideas was very helpful to produce power and momentum. Regarding generative AI, she mentioned that the error space by AI is now much wider than before, which creates bigger concerns, and where often the key question is “who gets to be the arbitor of the truth?” Regarding algorithmic auditing, on one hand she promoted red-teaming (especially as a community-driven approach), but also at the same time she said it’s important to have community consensus on auditing.</p>
</section>
<section id="overall-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="overall-thoughts">Overall thoughts</h2>
<p>The reading assignments were excellent and interesting. However, the course felt quite rushed. We had to cover 4 definitions of fairness and COMPAS case study within 15 minutes. I was already quite familiar with everything but I wonder how many attendees were able to process all of it in that short amount of time.</p>
<p>During the breakout session, instead of talking about fairness, ADS, and algorithmic accountability, we were asked to think about when we were denied to talk about tech ethics at work or work-adjacent settings, and why we thought that was the case. Then we did an exercise where we listed various values we care about and see whether any values are diametrical.</p>
<p>To begin with, the breakout session (group discussion) was short, but I was quite unhappy that the discussion was not much related to the topics or the readings. Yes, when building an ADS system, one needs to think about how to handle the trade-offs between different values but especially since this course was advertised as “for practitioners,” I was looking for something more concrete. The post-it exercise was in a way useful but I also thought it prevented us from having an actual conversational discussion.</p>
<p>Overall, this week has been a review of the stuff that I was mostly familiar with (thank you FAccT conferences!). I hope there are more active and practical engagements in the coming weeks.</p>


</section>

 ]]></description>
  <category>ethics</category>
  <category>fairness</category>
  <category>criminal justice</category>
  <guid>https://hongsupshin.github.io/posts/2023-10-11-ethics-week2/</guid>
  <pubDate>Wed, 11 Oct 2023 05:00:00 GMT</pubDate>
  <media:content url="https://assets-c3.propublica.org/legacy/images/_threeTwo800w/20160523-machine-bias-630x420_1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Moral complicity and moral disengagement (Stanford Tech Ethics course, Week 1)</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2023-10-04-ethics-week1/</link>
  <description><![CDATA[ 





<section id="the-course-ethics-technology-public-policy-for-practitioners" class="level2">
<h2 class="anchored" data-anchor-id="the-course-ethics-technology-public-policy-for-practitioners">The course: <a href="https://online.stanford.edu/courses/soe-xetech0001-ethics-technology-public-policy-practitioners">Ethics, Technology + Public Policy for Practitioners</a></h2>
<p>It’s been several years since I got interested in AI ethics. I have been to conferences and workshops about the topic but it wasn’t easy to find a course with a well-structured curriculum. It’s probably because AI ethics covers so many different disciplines: social/political science, ethics and philosophy, and computer science. And of course, the policy aspect requires real-world knowledge from public sector, which is hard to find in academia anyway.</p>
<p>A few months ago, I learned about <a href="https://online.stanford.edu/courses/soe-xetech0001-ethics-technology-public-policy-practitioners">Ethics, Technology + Public Policy for Practitioners</a> from <a href="https://www.linkedin.com/in/smithgk/XQ%3D%3D">Geneviève</a>, an Insight Data Science fellow whom I met at the Grace Hopper conference before the pandemic. I’ve been following <a href="https://hai.stanford.edu/">Stanford HAI</a> already and I was aware of their endeavor for human-centered AI and AI ethics, so in a way, this course made sense. At a glance, its outline seemed to be quite comprehensive because it covered various topics such as fairness, algorithmic bias, and generative AI. I also liked that it invited many guest speakers from academia, industry, and public sectors. Luckily, my application got accepted and I was able to join the course this fall.</p>
<p>Each week, we get reading assignments for the coming week’s discussion. For Week 1, we got to read the famous sci-fi and fantasy writer, Ursula K. Le Guin’s <a href="https://shsdavisapes.pbworks.com/f/Omelas.pdf">The Ones Who Walk Away from Omelas</a>. As an optional reading, <a href="https://drive.google.com/file/d/1VvicB1c1MMZpkq2CcYfkxSqqFuGGN_jO/view">Selective Moral Disengagement in the Exercise of Moral Agency</a> by <a href="https://en.wikipedia.org/wiki/Albert_Bandura">Albert Bandura</a> was assigned although we didn’t discuss the paper during the lecture.</p>
</section>
<section id="the-ones-who-walk-away-from-omelas" class="level2">
<h2 class="anchored" data-anchor-id="the-ones-who-walk-away-from-omelas">The Ones Who Walk Away from Omelas</h2>
<p>The story is about a place called Omelas, described as a utopian place. The author later reveals that there is a poor child locked up in a dungeon in Omelas, and this child’s poor condition is what makes Omelas a utopia. Interestingly, all people in Omelas are aware of this. But they just accept this fact and enjoy their life. At the end of the story, we find that sometimes some folks decide to walk away from Omelas to settle somewhere else.</p>
<p>In the course, the discussion was moderated by <a href="https://robreich.stanford.edu/">Rob Reich</a>, a philosopher and a political science professor at Stanford (he is also one of the main faculty of this course). He asked the cohort several questions about the story.</p>
<section id="heroes-or-cowards-what-would-you-do" class="level3">
<h3 class="anchored" data-anchor-id="heroes-or-cowards-what-would-you-do">Heroes or cowards? What would you do?</h3>
<p>We were first asked about those who left Omelas: are they heroes or cowards? The majority of the cohort said they were neither, which was also my answer. They were definitely not heroes because they didn’t do anything to improve the condition of the victim (the child) but simply walked away. But I also didn’t think that they were cowards because in a way, their decision of not wanting to be a part of Omelas, which included walking away from the comfortable life, should be weighed in.</p>
<p>As a follow up, we were now asked what <em>we</em> would do in this situation. This change of viewpoint already made me uncomfortable (later, we learned that this feeling can be called <em>moral caffeination</em>). I decided to poke a hole of the concept of utopia and thought a non-utopia would not mean a hellish place, meaning the cost of rescuing the child would be tolerable, which all have to think about. And if the people of Omelas share the cost together, it would be acceptable. Thus, I would first raise this question to other like-minded folks to investigate the situation and find a solution. (This definitely shows my researcher background!)</p>
<p>Some cohort members shared their thoughts. It was only a handful but I was surprised to know that some seemed to have a very strong conviction of righteousness by confidently saying that they would rescue the child for sure. Prof.&nbsp;Reich challenged their answers by asking them what gives them the moral superiority, how they think about this unilateral decision-making and the cost other people in Omelas would pay due to their actions. I don’t think I heard a satisfying answer but that’s probably because these are all very tough questions that we have to wrestle with.</p>
</section>
<section id="adulthood-and-complicity" class="level3">
<h3 class="anchored" data-anchor-id="adulthood-and-complicity">Adulthood and complicity</h3>
<p>Prof.&nbsp;Reich provided more explanations. He said, those who walk away may often have desire of keeping their hands very clean, which could be seen as a bit delusional because their walking away doesn’t make that the injustice just disappears. He said, the more important thing we need to think about is how we sit with <em>complicity</em> if we realize that we are complicit and in this together.</p>
<p>Regarding this, several cohort members mentioned the TV show, The Good Place. In one of the episodes, the show reveals that it’s been almost impossible to get into the good place these days because everybody is complicit (e.g., unknowingly buying a t-shirt made by a company who runs a sweatshop). Some of us also pointed out that it might not be possible to “leave” Omelas in our world.</p>
<p>We only briefly mentioned this but one thing I found interesting in the story was the young people who were upset about the child’s condition in the beginning, but later got used to the fact and accepted the situation. In a way, as they grow up, they become to accept the injustice as a background noise and move on with their lives. To be honest, I think we all have a similar experience like this. But I was particularly curious about how one becomes conditioned, dull, and feeling comfortable living with moral complicity.</p>
</section>
</section>
<section id="selective-moral-disengagement-in-the-exercise-of-moral-agency" class="level2">
<h2 class="anchored" data-anchor-id="selective-moral-disengagement-in-the-exercise-of-moral-agency">Selective Moral Disengagement in the Exercise of Moral Agency</h2>
<p>This ethics paper, written by Albert Bandura, provided some answers to this question. Overall, this paper provides a good summary of how and when moral disengagement happens in various social situations. We all have our moral standards that promote certain actions and prevent us from behaving immorally. When moral disengagement occurs, this regulatory checks can be skipped, which often leads to social harm. In a way, moral disengagement sets people free from feeling guilty and tortured about their immoral behavior.</p>
<p>The author goes through various mechanisms of moral disengagement and provides examples from human history. For instance, <em>moral justification</em> can be deployed by politicized religion such as holy terror. <em>Sanitizing language</em> is often seen by corporations and governments who want to minimize their moral and ethical responsibilities in sticky situations to make themselves look innocent. <em>Advantageous comparison</em> could be applied to the Omelas situation because we can always apply the utilitarianism argument to justify the child’s suffering.</p>
<section id="moral-disengagement-in-tech" class="level3">
<h3 class="anchored" data-anchor-id="moral-disengagement-in-tech">Moral disengagement in tech</h3>
<p>I found <em>displacement</em> and <em>diffusion</em> of <em>responsibility</em> more prominent in ethical problems in tech. The <em>displacement</em> means that the farther we are from witnessing harms directly, the easier moral disengagement can happen. This often happens in machine learning and data science where a human being can be reduced to a single data point in a dataset with a million rows. Popularity of a black-box algorithm like deep learning also accelerates this because we can’t even explain what is done to each individual data points. The <em>diffusion</em> of responsibility is an inevitable outcome of a complex modern society because division of labor means diffusion of moral responsibility. This is also a crucial point when we talk about algorithmic accountability because ML products are often complicated and multiple parties are involved, which makes it challenging to ask accountability.</p>
</section>
<section id="dual-nature-and-hope" class="level3">
<h3 class="anchored" data-anchor-id="dual-nature-and-hope">Dual nature and hope</h3>
<p>I found a hopeful and insightful message from the <em>Dual Nature of Moral Agency</em> section. Here, the author told a story of a soldier who directly witnessed civilian killing in a war. This experience later spiked courage in him and he ended up rescuing other remaining civilians. The author said the following about this story:</p>
<blockquote class="blockquote">
<p>Social psychology emphasises the power of the situation over the individual. In the case of proactive moral courage, the individual triumphs as a moral agent over compelling situational forces.</p>
</blockquote>
<p>The dual nature here means as a moral agent, we can inhibit immoral behaviors but also be proactive and courageous to behave humanely. I think probably most of the cohort members who signed up for this course were interested in the latter. And as the author said, if individual triumphs matter in moral courage, as I have often thought, educating individual engineers, who are often just treated as a cog in the wheel, would be a significant step towards building ethical tech, especially when building a machine learning product where a single line of code change can create a butterfly effect.</p>
</section>
</section>
<section id="final-thoughts-on-week-1" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts-on-week-1">Final thoughts on Week 1</h2>
<p>All in all, I very much enjoyed the reading material and the discussion during the course. Prof.&nbsp;Reich’s guiding words were also crucial to understand the bigger picture and idea of moral complicity. When we were talking about how we identify this uncomfortable feeling about the Omelas story, I confessed that it often felt like walking on the fine line between hope and despair. Moral complicity can have a profound emotional impact on one’s psyche and potentially put us in depression because the overwhelming complexity and the amount of complicity and injustice in the world makes us feel helpless and hopeless.</p>
<p>But I think that’s why we want to call this exercise as moral caffeination. Once we recognize this situation, we don’t want to just stay feeling uncomfortable, disoriented, and depressed. We want to alert ourselves and think about “now what?” as Prof.&nbsp;Reich said. Personally, I think learning about the complicity and thinking about it in depth with others is a first step towards solving ethics problems in tech.</p>


</section>

 ]]></description>
  <category>ethics</category>
  <guid>https://hongsupshin.github.io/posts/2023-10-04-ethics-week1/</guid>
  <pubDate>Wed, 04 Oct 2023 05:00:00 GMT</pubDate>
  <media:content url="https://upload.wikimedia.org/wikipedia/en/c/c3/TheOnesWhoWalkAwayFromOmelas.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Constitutional AI: Harmlessness from AI Feedback</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2023-08-31/</link>
  <description><![CDATA[ 





<p>I’ve been interested in AI safety and responsible AI for several years, and the rise of LLMs has certainly increased stakes. Currently there is an intense arms race among several major tech companies and Anthropic is one of them. They recently published a paper about their LLM and claims to adopt a more cautious approach than others by designing their LLMs to minimize potential harm. They call this <em>constitutional AI (CAI)</em> because their LLMs follow a constitution of principles. I wanted to learn more about how they teach their algorithm to follow these principles. I presented the paper in Austin ML Journal Club in Aug 2023.</p>
<section id="background-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="background-knowledge">Background knowledge</h2>
<p>To understand this paper properly, it’s better to be familiar with AI alignment problem and reinforcement learning from human feedback (RLHF). AI alignment is about aligning AI systems’ design and values with humanity values such as honesty. Norbert Wiener, an AI researcher back in 1960s, described the AI alignment problem as following:</p>
<blockquote class="blockquote">
<p>“If we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively (…) we had better be quite sure that the purpose put into the machine is the purpose which we really desire.”</p>
</blockquote>
<p>Regarding RLHF, I recommend <a href="https://huggingface.co/blog/rlhf">this great summary</a> from Hugging Face. At its core, RLHF is an attempt to distill human feedback into a model (often called <em>reward</em> or <em>preference</em> model) when training LLMs. This is because human feedback is often expensive to collect and difficult to generalize. An important thing to know is that to train this model, practitioners often use ranked preference modeling where human annotators are asked to rank generated text ouptuts from language models. The assumption here is that this approach may mimic human preference of certain responses over others. And because of this preference approach, RLHF papers use <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo scores</a>, a rating system originated from chess to show a player’s winning rates, to evaluate model performance.</p>
<p>In terms of the alignment values, Anthropic chose honesty, helpfulness, and harmlessness. The detailed definition of these concepts are described in one of their previous works:</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<a href="https://arxiv.org/abs/2112.00861">Askell et al.&nbsp;2021</a>, What are Helpfulness, Honesty, and Harmlessness?
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Helpful</strong></p>
<ul>
<li>The AI should make a clear attempt to perform the task or answer the question posed (as long as it isn’t harmful). It should do this as concisely and efficiently as possible.</li>
<li>When more information is required, the AI should ask relevant follow-up questions and obtain necessary details. It should respond with appropriate levels of sensitivity, insight, and discretion.</li>
<li>Ideally the AI will also re-direct ill-informed requests, e.g.&nbsp;if asked ‘how can I build a website in assembly language’ it might suggest a different approach.</li>
</ul>
<p><strong>Honest</strong></p>
<ul>
<li>At its most basic level, the AI should give accurate information. Moreover, it should be calibrated (e.g.&nbsp;it should be correct 80% of the time when it claims 80% confidence) and express appropriate levels of uncertainty. It should express its uncertainty without misleading human users.</li>
<li>Crucially, the AI should be honest about its own capabilities and levels of knowledge – it is not sufficient for it to simply imitate the responses expected from a seemingly humble and honest expert.</li>
<li>Ideally the AI would also be honest about itself and its own internal state, insofar as that information is available to it.</li>
<li>Honesty is more objective than helpfulness and harmlessness, so more aspects of honesty training may be possible without human input. This might include calibration training on factual claims and claims about the internal state of the model, and the use of search to augment accuracy.</li>
</ul>
<p><strong>Harmless</strong></p>
<ul>
<li>The AI should not be offensive or discriminatory, either directly or through subtext or bias.</li>
<li>When asked to aid in a dangerous act (e.g.&nbsp;building a bomb), the AI should politely refuse. Ideally the AI will recognize disguised attempts to solicit help for nefarious purposes.</li>
<li>To the best of its abilities, the AI should recognize when it may be providing very sensitive or consequential advice and act with appropriate modesty and care.</li>
<li>What behaviors are considered harmful and to what degree will vary across people and cultures. It will also be context-dependent, i.e.&nbsp;it will depend on the nature of the user query, who is using the AI assistant, and the time and place in which the assistant is being used.</li>
</ul>
</div>
</div>
</section>
<section id="motivations" class="level2">
<h2 class="anchored" data-anchor-id="motivations">Motivations</h2>
<p>The first motivation was scaling supervision. Given that LLMs require numerous examples, it’s better to automate the supervision process and use human annotators to get more curated and high quality answers. This is a similar idea behind the preference modeling in RLHF. The authors called theirs “reinforcement learning from AI Feedback” (RL<strong>AI</strong>F, not RL<strong>H</strong>F). A more interesting motivation was building a <em>non-evasive</em> and yet helpful AI assistant. Many currently available AI assistants often simply refuse to answer questions to harmful prompts (e.g., simply saying “I don’t know” or “I can’t answer that”). Their model was never evasive but tried to explain the reasoning behind their negative response to harmful questions. Finally, similar to the first point, they claimed that distilling human supervision into a model could help better understand general aspects of human feedback from many crowd-workers.</p>
</section>
<section id="the-constitutional-ai-approach" class="level2">
<h2 class="anchored" data-anchor-id="the-constitutional-ai-approach">The Constitutional AI Approach</h2>
<section id="supervised-stage" class="level3">
<h3 class="anchored" data-anchor-id="supervised-stage">Supervised stage</h3>
<p>Their constitutional AI (CAI) consisted of two stages: a supervised stage and a reinforcement learning stage. In the supervised stage, they used a pretrained LM (“Helpful RLHF model” from their previous work) as a starting point, and <a href="https://en.wikipedia.org/wiki/Red_team">red-teamed</a> the model by presenting harmful prompts (by human workers) and sampled the responses. Then, (this is the most interesting part in my opinion!) they used <em>natural language</em> to ask the model to critique and revise its own response based on certain principles. Here’s an example from the paper:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-08-31/appendixA.png" class="img-fluid figure-img"></p>
<figcaption>Appendix A. Sample critiques and revisions. The first two revisions are shown in this screenshot.</figcaption>
</figure>
</div>
<p>Appendix C contains a list of principles (constitution) they used to create the critique-revision requests. The paper doesn’t talk much about how they came up with the principles but according to <a href="https://www.anthropic.com/index/claudes-constitution">Anthropic’s website</a>, the principles were based on existing documents such as <a href="https://www.un.org/en/about-us/universal-declaration-of-human-rights">Universal Declaration of Human Rights</a> or Apple’s Terms of Service. As shown in the example above, a response can go through multiple critique-revision requests. The authors found that generally the more revisions mean less harmfulness although the first revision contributes most.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-08-31/fig5.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 5. Preference Model scores of responses and revisions from helpful RLHF models, evaluated on a set of red team prompts.</figcaption>
</figure>
</div>
</section>
<section id="reinforcement-learning-rl-stage" class="level3">
<h3 class="anchored" data-anchor-id="reinforcement-learning-rl-stage">Reinforcement learning (RL) stage</h3>
<p>The main idea behind this stage is identical to the RL stage in RLHF. The goal is to distill knowledge from a group of responses by training a reward model. The key difference is that <strong>these responses are now generated by a model not human</strong> (the supervised learning model from the previous stage). The authors called this reward model “feedback model” although it was a bit unclear which exact LMs they were referring to.</p>
<p>Another interesting aspect they added here was a “chain-of-thought” approach. This was inspired by <a href="https://arxiv.org/abs/2305.20050">Let’s Verify Step by Step</a>, a paper we covered in <a href="https://austinmljournalclub.github.io/posts/20230622/">a previous journal club meeting</a>. Here, after getting a response, the authors added the natural-language phrase <strong>“Let’s think step by step”</strong> to generate richer intermediate responses from the model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-08-31/appendixE.png" class="img-fluid figure-img"></p>
<figcaption>An example from Appendix E.2. Chain-of-Thought Prompts for RL-CAI.</figcaption>
</figure>
</div>
<p>In their final model (Reinforcement Learning-Constitutional AI with Chain of Thought, or <strong>RL-CAI w/ CoT</strong>), the authors found a major improvement in harmlessness Elo score without compromising the helpfulness Elo score much. Note that in the figure below, Elo score of 0 on the y axis (starting point of the RL model) represents the supervised learning model (SL-CAI), which means the SL-CAI model was used as initial base model for RL.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-08-31/fig8.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 8. Helpfulness (left) and harmlessness (right) Elo scores as a function of the total number of RL training sequences, as judged by crowd-workers via comparison tests.</figcaption>
</figure>
</div>
<p>One interesting aspect of the RL model the authors shared was its behavior when the model was over-fitted. They found that in this case, the response often included <em>boilerplate</em> language such as “you are valid, valued, and cared for.”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-08-31/4.3.png" class="img-fluid figure-img"></p>
<figcaption>An example of over-trained RL-CAI model response showing boilerplate language as part of their response (e.g.&nbsp;“you are valid, valued, and cared for”).</figcaption>
</figure>
</div>
</section>
</section>
<section id="broader-impacts" class="level2">
<h2 class="anchored" data-anchor-id="broader-impacts">“Broader Impacts”</h2>
<p>At the end of the paper, the authors emphasized that natural language feedback could change AI behavior and potentially increase robustness because red-teaming efforts could become more scalable (because feedback supervision can be generated by a model not humans). In section 6.2 Broader Impacts, they <em>briefly</em> mentioned the potential harm of the constitutional AI approach. Using natural language to change AI behavior means it will become much easier to train a malicious AI assistant especially given that this method reduces the need for human feedback.</p>
</section>
<section id="journal-club-discussion" class="level2">
<h2 class="anchored" data-anchor-id="journal-club-discussion">Journal club discussion</h2>
<p>First of all, compared to other on-average deep learning papers, we found this paper easier to read. We also appreciated that the authors provided numerous examples. We could see they also tried to do a better job at providing many details of the model although still not enough, especially when they were referring to their previous work.</p>
<section id="harmlessness-as-an-easy-alignment" class="level3">
<h3 class="anchored" data-anchor-id="harmlessness-as-an-easy-alignment">Harmlessness as an <em>easy</em> alignment</h3>
<p>Some of us suspected that the authors might have chosen harmlessness as their main target of alignment perhaps because it was relatively easier to tackle than other alignment such as helpfulness. The authors did mention the tension between harmfulness and helpfulness in the paper in that an AI assistant could become harmful if it was too eager to be helpful (e.g., providing a detailed answer to a prompt about how to commit a crime). We talked about more nuanced alignments (such as humor) and whether it would be possible to use natural language to change model behavior. Some of us pointed out that harmlessness could be relatively easy because diametrically opposed examples could be easily found in languages.</p>
</section>
<section id="does-chain-of-thought-count-as-an-explanation" class="level3">
<h3 class="anchored" data-anchor-id="does-chain-of-thought-count-as-an-explanation">Does chain-of-thought count as an explanation?</h3>
<p>Many of us were skeptical of treating responses from the chain-of-thought approach as explanations. Most examples shown in the paper seemed reasonable but given that what the model did with a CoT request was nothing more than just generating more detailed responses, we agreed that we should not treat them as step-by-step deductive reasoning. We were interested in looking at CoT examples that might sound gibberish and redundant. I personally also thought this was one of the examples of ML practitioners anthropomorphizing a behavior of an ML model.</p>
</section>
<section id="no-more-efforts-to-understand-the-model" class="level3">
<h3 class="anchored" data-anchor-id="no-more-efforts-to-understand-the-model">No more efforts to understand the model</h3>
<p>Most of us were surprised that the approach of using natural language to critique and revise its own behavior seemed to have worked. Before I read the paper, I was very curious to know what constraints they came up with and how they tried to model complex social concepts such as justice and harm. The fact that their approach seemed to be working was interesting but this also meant that we are in an era where we are no longer trying to change the model behavior at a lower level, but rather we treat the language models as if they are something we don’t completely understand. This paper was completely missing explanations of why this approach actually worked. From my perspective, as other numerous deep learning papers, this paper was saying “we tried this, we don’t exactly know why it works, but it seems to work.”</p>
</section>
<section id="why-diminish-the-work-and-labor-of-human-annotators" class="level3">
<h3 class="anchored" data-anchor-id="why-diminish-the-work-and-labor-of-human-annotators">Why diminish the work and labor of human annotators?</h3>
<p>In the abstract and at the end of the paper, the authors kept saying their model was trained “without any human labels identifying harmful outputs.” All of us agreed that this was an exaggeration. To train the supervised model, they needed human annotators, and once the supervised model was ready, then they were able to generate <em>AI</em> feedback. Given that Anthropic is a for-profit company that sells AI assistant software, highlighting that the maintenance cost of their system is <em>cheaper</em> than others because human feedback can be replaced by AI feedback, could be a good marketing strategy, but at the cost of marginalizing human labor.</p>
</section>
<section id="how-did-you-come-up-with-the-principles" class="level3">
<h3 class="anchored" data-anchor-id="how-did-you-come-up-with-the-principles">How did you come up with the principles?</h3>
<p>In Appendix C, the authors provided a comprehensive list of all principles they used to generate critique-revision responses. These were the core principles that guided the model behavior but the authors didn’t mention much about how they curated the list. Some principles were general and others were more specific to particular types of harms such as racism and misogyny. We suspected that there had been an iterative curation process to narrow the list down to these 16 principles specifically. If these were the main drivers of changes in model behavior, we think they should have provided much more details.</p>
</section>
<section id="the-double-edged-sword" class="level3">
<h3 class="anchored" data-anchor-id="the-double-edged-sword">The double-edged sword</h3>
<p>Finally, some of us were disappointed that the authors didn’t elaborate much on the potential harm of their approach. They spent a lot of time talking about harmlessness of their algorithms and yet they really fell short when talking about social impacts of their model, especially regarding lowering the barrier for experimenting with LMs and automating supervision by removing human further out of the loop. Particularly for the former, we agreed that it wouldn’t be surprising to see, in near future bad actors take advantage of this approach and come up with a highly toxic, malicious, and harmful AI assistant.</p>


</section>
</section>

 ]]></description>
  <category>paper</category>
  <category>GenAI</category>
  <category>LLM</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2023-08-31/</guid>
  <pubDate>Thu, 31 Aug 2023 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2023-08-31/principle.png" medium="image" type="image/png" height="156" width="144"/>
</item>
<item>
  <title>Visualization in Bayesian workflow</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2023-03-29/</link>
  <description><![CDATA[ 





<p>This paper summarizes types of data visualization that we can use in Bayesian modeling and inference. This is also a good overview of how to do Bayesian modeling properly, including validating results. The fact that the main author is one of the maintainers of the <a href="https://mc-stan.org/">stan</a> package, is another motivating factor. I presented the paper in Austin ML Journal Club in Mar 2023.</p>
<section id="paper-summary" class="level2">
<h2 class="anchored" data-anchor-id="paper-summary">Paper summary</h2>
<p>Given a problem, we incorporate our scientific knowledge into a causal (generative) model to simulate how the relevant variables are produced (input and output). Researchers need more than null hypothesis because it doesn’t talk about how your observation is generated. We can use a DAG as a scientific causal model and data generation process can be expressed in a generative model, which is often accompanied with Bayesian data analysis (BDA). BDA is particularly useful because we can simulate data from the model directly to design and debug during inference. To effectively estimate a posterior distribution, we need computational methods such as MCMC and others. One may say Bayesian might be an overkill but it’s extremeley useful for typical modeling problems such as measurement error, missing data, latent variables, and regularization. Again, it’s also generative!</p>
<p>The paper uses data visualization to express the followings: - Exploratory data analysis to come up with a proper model - Prior predictive distribution check to check model’s assumption - MCMC computational check to evaluate the sampling process - Posterior predictive check to validate inference process</p>
<p>This paper is based on R’s <code>bayesplot</code> but there are several python equivalents to this such as <code>pymc</code>, <code>arviz</code>, and <code>numpyro</code>. It uses a global air polllution dataset (pm2.5 particles) measured from satellite images. The goal of modeling is to predict the level of pm2.5 from the images. Hence, this is a regression problem. Fig. 1 shows the linear trend between the two variables of interest but also shows how sparse the data is depending on groups.</p>
<section id="exploratory-data-analysis-eda" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-data-analysis-eda">Exploratory data analysis (EDA)</h3>
<p>EDA is essential to understand and capture features and heterogeneity of data. The data pattern helps building a group-up modeling strategy to address the imbalance and sparsity of data. The authors emphasize that the top-down approach in typical ML communities these days is to throw everything into a non-parametric procedure, which can severely overfit. Fig. 2 shows that simple regression works pretty well, especially when the group identity is taken into account, which means we need a hierarchical approach.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-03-29/fig4.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 4: Visualizing the prior predictive distribution</figcaption>
</figure>
</div>
</section>
<section id="prior-predictive-check" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-check">Prior predictive check</h3>
<p>Instead of using a non-informative or uniform prior, weakly informative prior is always recommended, which takes into account modeler’s perspective. In the paper, we assume that the target varialbe follows a normal distribution defined by a mean and a <img src="https://latex.codecogs.com/png.latex?%5Csigma"> where the mean is a linear function of input variable (satellite data) and linear coefficients, which also have priors (0 mean and std (<img src="https://latex.codecogs.com/png.latex?%5Ctau">)).</p>
<p>Prior predictive checks are useful to visualize the impact of our assumption for prior definition. If we use a vague prior (very wide range, Fig. 4a), ranges from the sample don’t match the observation. Fig. 4b shows a much tighter prior where the simulated data points still overestimate but are in a much reasonable range. Obviously, tighter and sensible priors are better.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-03-29/fig5.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 5: Diagnostic plots for Hamiltonian Monte Carlo</figcaption>
</figure>
</div>
</section>
<section id="mcmc-diagnostics" class="level3">
<h3 class="anchored" data-anchor-id="mcmc-diagnostics">MCMC diagnostics</h3>
<p>Success of Hamiltonion Monte Carlo (HMC) depends on how smooth the posterior distribution is; if not smooth, HMC proposal diverges from the true trajectory, which may signal that the trajectories are stuck. Healthy MCMC samples, shown as a bivariate plot in Fig. 5a, shouldn’t have obvious patterns. The funnel shape there is due to <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B11%7D%20%5Csim%20N(0,%20%5C,%20%5Ctau_%7B1%7D%5E%7B2%7D)"> where small <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7B1%7D"> means <img src="https://latex.codecogs.com/png.latex?%5Cbeta_%7B11%7D"> distribution is narrow. The parallel co-ordinate plot (Fig. 5b) also shouldn’t have any particular structure.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-03-29/fig9.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 9: Graphical check of leave-one-out cross-validated probability integral transform (LOO-PIT)</figcaption>
</figure>
</div>
</section>
<section id="posterior-predictive-check" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-check">Posterior predictive check</h3>
<p>If a trained model has a good fit, generated data from the model should follow observations. Posterior predictive checking is mostly qualitative but it’s effective to compare empirical and simulated values (Fig. 6). Fig. 7 shows checking whether samples from models captures other statistics such as skewness (kurtosis) and Fig. 8 shows how we can evaluate whether samples from models capture summary statistics such as median Fig. 9 shows using visualization that checks whether leave-one-out cross-validation (LOO-CV) predictive cumulative density function is uniform or not, similar to the idea of a K-S test.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-03-29/fig10a.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 10a: Model comparisons using leave-one-out (LOO) cross-validation. The difference in pointwise ELPD values obtained from PSIS-LOO</figcaption>
</figure>
</div>
</section>
<section id="model-comparison" class="level3">
<h3 class="anchored" data-anchor-id="model-comparison">Model comparison</h3>
<p>When comparing models, Bayesian data analysis allows detailed examination of individual data points on a given model. We can use cross-validated LOO predictive distribution to do so; it shows the distribution of a data point from a model that’s built without that data point (i.e., LOO). We can use expected log-predictive densities (ELPD), which is essentially the mean of the log probability of each data point <em>i</em>, computed with posterior that omits the point <em>i</em> (the bigger the better). We use Pareto-smoothed importance sampling (PSIS) to compute this metric (we don’t have to fit the models N times). Once we have ELPD value for every data point of a model, we can repeat this for all the models we have and make comparison (Fig. 10a).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2023-03-29/fig10b.png" class="img-fluid figure-img"></p>
<figcaption>Fig. 10b: Model comparisons using leave-one-out (LOO) cross-validation. The <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> diagnostics from PSIS-LOO for Model 2</figcaption>
</figure>
</div>
<p>Similarly, we can compute <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bk%7D"> as well which represents degree of influence of a specific observation. High value means this data point is “unexpected”, meaning that it is likely to be an outlier or the model struggles to make valid prediction for this data point.</p>
</section>
</section>
<section id="journal-club-discussion" class="level2">
<h2 class="anchored" data-anchor-id="journal-club-discussion">Journal club discussion</h2>
<p>We had a lengthy discussion about the choice of prior and how tricky it can be. As the authors mentioned in conclusion, we were also slightly worried about double-dipping the data when running prior predictive checks and potential data leakage. It was also interesting to share our own experience on Bayesian inference ranging from dealing with prior assumptions, model comparison, to decision-making with uncertainty. But we all agreed that Bayesian data analysis is more empowering for us modellers compared to the typical top-down approach in ML where we often don’t have any generative models about data. We also agreed that Bayesian data anlsysis is absolutely more powerful when we suffer from a small data problem.</p>
<p>But we also some downsides of Bayesian data analysis too. It’s difficult to scalable and someone we ML practitioners are not the domain experts and without the domain expertise, it’s difficult to come up with a good DAG. Due to the nature of Bayesian analysis where we don’t often make a point-estimate summary, we appreciated that the paper spent a good amount of time discussing how to summarize a posterior distribution. We also discussed the importance of loss function when decision making with uncertainty.</p>
<p>In general, we liked the paper but we thought it fell slightly short because it wasn’t focusing on understanding scientific mechanism but rather on predictive modeling nature of Bayesian analysis. When it comes to model comparison particularly, we thought it’s important to evaluate the structure of the model too in addition to evaluating the goodness of fit. For instance, if the model performance varies across the regions, the way we compare the models would like to change as well, and potentially the DAGs too.</p>


</section>

 ]]></description>
  <category>paper</category>
  <category>Bayesian</category>
  <category>visualization</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2023-03-29/</guid>
  <pubDate>Wed, 29 Mar 2023 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2023-03-29/fig10a.png" medium="image" type="image/png" height="120" width="144"/>
</item>
<item>
  <title>Interoperability testing for hyperparameter tuning: MLflow, LightGBM, sklearn, and dask-ml</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/2023-03-17-mlflow-lightgbm-dask.html</link>
  <description><![CDATA[ 





<p>There are numerous open source ML packages in python ecosystem. Developers do their best to maximize interoperability in relation to other main ML packages but it’s not possible to check every possible combination. That’s why I think some of the responsibility of interoperability lies on users. MLflow’s autologging method is quite handy because with a single line of code (<code>mlflow.autologging</code>), we obtain useful metrics of model behavior such as confusion matrix, feature importance, or training loss over epochs. However, this is not always guaranteed when we apply model tuning on top by using scikit-learn and dask.</p>
<p>In this notebook, I first demonstrated what MLflow autologging method did particularly for LightGBM models. Then, I tried the same autologging in model tuning frameworks of scikit-learn and Dask-ML backend, and how the autologging method behaves. Check <code>environment.yml</code> to run the notebook.</p>
<div id="b9100bba-fd42-4807-bb14-6e85953611a5" class="cell" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> lightgbm <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> lgb</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> mlflow</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mlflow.client <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> MlflowClient</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datasets</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> train_test_split, RandomizedSearchCV, PredefinedSplit</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.pipeline <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Pipeline</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.metrics <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> log_loss, roc_auc_score</span>
<span id="cb1-11"></span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> dask_ml.model_selection <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> RandomizedSearchCV <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> dask_RandomizedSearchCV</span>
<span id="cb1-13"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> distributed <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Client</span>
<span id="cb1-14"></span>
<span id="cb1-15">seed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">97531</span></span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>load_ext autoreload</span>
<span id="cb1-18"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>autoreload <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>load_ext watermark</span>
<span id="cb1-21"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>watermark <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>g <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>r <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>iv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>a <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hongsup Shin"</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Author: Hongsup Shin

Last updated: 2023-03-19 23:00:45

Python implementation: CPython
Python version       : 3.9.16
IPython version      : 8.10.0

Git hash: 0eaf0c3c88c3e909b76c36af9b13fea1f04d7c08

Git repo: https://github.com/hongsupshin/hongsupshin.github.io.git

Git branch: 1-mlflow

lightgbm: 3.3.2
sklearn : 1.2.0
numpy   : 1.24.0
mlflow  : 2.1.1
</code></pre>
</div>
</div>
<center>
<img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/index.png" width="600">
</center>
<section id="set-up" class="level2">
<h2 class="anchored" data-anchor-id="set-up">Set up</h2>
<section id="mlflow" class="level3">
<h3 class="anchored" data-anchor-id="mlflow">MLflow</h3>
<p>MLflow comes with a tracking UI, which you can launch by running <code>mlflow ui</code>. By default, you can see the UI http://localhost:5000. Here, I assumed that you ran <code>mlflow ui</code> before the following cell where experiment location was defined.</p>
<div id="3741ce1c-7adb-4fdd-83f4-a7a024abd900" class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">mlflow.set_tracking_uri(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"http://127.0.0.1:5000"</span>)</span>
<span id="cb3-2">mlflow.set_experiment(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mlflow_tune_demo"</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>&lt;Experiment: artifact_location='mlflow-artifacts:/618881380489654419', creation_time=1679103932467, experiment_id='618881380489654419', last_update_time=1679103932467, lifecycle_stage='active', name='mlflow_tune_demo', tags={}&gt;</code></pre>
</div>
</div>
<p><code>mlflow.autolog()</code> should be called before running training but this enables all supported libraries that are imported. Thus, specific autologging is recommened:</p>
<div id="54c14ad0-99a1-45f6-b313-50b1c76af24b" class="cell" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">mlflow.lightgbm.autolog()</span></code></pre></div>
</div>
</section>
<section id="data-and-model" class="level3">
<h3 class="anchored" data-anchor-id="data-and-model">Data and model</h3>
<p>For this walkthrough, I used the breast cancer dataset from scikit-learn (<code>sklearn.datasets.load_breast_cancer()</code>), which is a binary classificaiton problem. For training, I split the dataset into train (50%), validation (25%) and test sets (25%). The validation set was used for model tuning.</p>
<div id="c3fda29c-d030-4411-a2e6-a10732b2f839" class="cell" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">breast_cancer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datasets.load_breast_cancer()</span>
<span id="cb6-2">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> breast_cancer.data</span>
<span id="cb6-3">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> breast_cancer.target</span>
<span id="cb6-4"></span>
<span id="cb6-5">X_train_val, X_test, y_train_val, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seed)</span>
<span id="cb6-6">X_train, X_val, y_train, y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X_train_val, y_train_val, test_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.33</span>, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seed)</span>
<span id="cb6-7"></span>
<span id="cb6-8">train_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgb.Dataset(X_train, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_train)</span>
<span id="cb6-9">valid_set <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgb.Dataset(X_val, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>y_val)</span></code></pre></div>
</div>
<p>Instead of <code>lightgbm.LGBMClassifier</code>, the scikit-learn API, we use the native LightGBM (<code>lightgbm.train</code>).</p>
</section>
</section>
<section id="lightgbm-autologging-for-a-single-training-run-no-tuning" class="level2">
<h2 class="anchored" data-anchor-id="lightgbm-autologging-for-a-single-training-run-no-tuning">LightGBM autologging for a single training run (no tuning)</h2>
<p>To test the limits of autologging and make things more interesting, I set up the following: - Apply an early-stopping callback - Track two types of metrics: log-loss (<code>"binary_logloss"</code>) and AUROC (<code>"auc"</code>) - Track two types of datasets: training and validation - Log test metrics in addition to the autologged metrics using <code>mlflow.log_metrics</code></p>
<p>and passed artbitrary hyperparameter values.</p>
<div id="01b3ca48-0642-4007-a3c1-23b3872347e3" class="cell" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">params <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"objective"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary"</span>,</span>
<span id="cb7-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"metric"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary_logloss"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"auc"</span>],</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"learning_rate"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"subsample"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>,</span>
<span id="cb7-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"seed"</span>: seed,</span>
<span id="cb7-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"num_iterations"</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb7-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"early_stopping_round"</span>: <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb7-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"first_metric_only"</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"force_col_wise"</span>:<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb7-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"verbosity"</span>: <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,    </span>
<span id="cb7-12">}</span></code></pre></div>
</div>
<div id="29875de3-ddac-4c67-9673-cb62634d446e" class="cell" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> mlflow.start_run(run_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"lgb_single"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> run:</span>
<span id="cb8-2">    </span>
<span id="cb8-3">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgb.train(</span>
<span id="cb8-4">        params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>params,</span>
<span id="cb8-5">        train_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_set,</span>
<span id="cb8-6">        callbacks<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[lgb.early_stopping(stopping_rounds<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>), lgb.log_evaluation()],</span>
<span id="cb8-7">        valid_sets<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[train_set, valid_set],</span>
<span id="cb8-8">        valid_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"val"</span>],</span>
<span id="cb8-9">    )</span>
<span id="cb8-10">    </span>
<span id="cb8-11">    y_pred_proba <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict(X_test)</span>
<span id="cb8-12">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> log_loss(y_test, y_pred_proba)</span>
<span id="cb8-13">    roc_auc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> roc_auc_score(y_test, y_pred_proba)</span>
<span id="cb8-14">    </span>
<span id="cb8-15">    mlflow.log_metrics(</span>
<span id="cb8-16">        {</span>
<span id="cb8-17">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test-logloss"</span>:loss,</span>
<span id="cb8-18">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test-auc"</span>: roc_auc,</span>
<span id="cb8-19">        }</span>
<span id="cb8-20">    )</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/anaconda3/envs/mlflow_tune/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] train's binary_logloss: 0.587637    train's auc: 0.986343   val's binary_logloss: 0.577365  val's auc: 0.942092
Training until validation scores don't improve for 5 rounds
[2] train's binary_logloss: 0.525352    train's auc: 0.989106   val's binary_logloss: 0.52163   val's auc: 0.963598
[3] train's binary_logloss: 0.473652    train's auc: 0.990591   val's binary_logloss: 0.47546   val's auc: 0.978383
[4] train's binary_logloss: 0.427402    train's auc: 0.992129   val's binary_logloss: 0.428912  val's auc: 0.983647
[5] train's binary_logloss: 0.388029    train's auc: 0.994553   val's binary_logloss: 0.392357  val's auc: 0.985551
[6] train's binary_logloss: 0.355106    train's auc: 0.995543   val's binary_logloss: 0.361549  val's auc: 0.986335
[7] train's binary_logloss: 0.323011    train's auc: 0.996247   val's binary_logloss: 0.330934  val's auc: 0.990031
[8] train's binary_logloss: 0.297144    train's auc: 0.996247   val's binary_logloss: 0.309573  val's auc: 0.990255
[9] train's binary_logloss: 0.272297    train's auc: 0.996508   val's binary_logloss: 0.286207  val's auc: 0.990927
[10]    train's binary_logloss: 0.250728    train's auc: 0.996455   val's binary_logloss: 0.265777  val's auc: 0.991823
Did not meet early stopping. Best iteration is:
[10]    train's binary_logloss: 0.250728    train's auc: 0.996455   val's binary_logloss: 0.265777  val's auc: 0.991823</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>2023/03/19 23:00:57 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: "/opt/anaconda3/envs/mlflow_tune/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils."</code></pre>
</div>
</div>
<p>When training was done, the UI showed the autologged metrics such as feature importance scores and plots:</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/lightgbm_autologging.png" class="img-fluid"></p>
<p>The UI also shoed other metrics I defined when setting up the training. This information is under “Metrics” section in the run. When I selected <code>train-binary_logloss</code>, it showed a log-loss vs.&nbsp;iteration curve. I could overlay <code>val-binary_logloss</code> on top of it, which would be useful to identify model overfitting.</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/train-val-binary_logloss.png" class="img-fluid"></p>
<p>I could fetch all logged metrics via <code>mlflow.client.MlflowClient</code>.</p>
<div id="9cf1bf8c-23fd-4ced-b627-d29a49f2ea33" class="cell" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">mlflow_client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> MlflowClient()</span>
<span id="cb12-2">run_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> run.info.run_id</span>
<span id="cb12-3">mlflow_run <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlflow_client.get_run(run_id)</span>
<span id="cb12-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(mlflow_run.data.metrics)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'train-auc': 0.9964553794829024, 'train-binary_logloss': 0.2507277280941103, 'val-binary_logloss': 0.2657767821072834, 'test-auc': 0.9735537190082645, 'stopped_iteration': 10.0, 'val-auc': 0.9918234767025089, 'best_iteration': 10.0, 'test-logloss': 0.30723647532041254}</code></pre>
</div>
</div>
<p>This confirms that with <code>mlflow.lightgbm.autolog</code>, the following metrics were logged in the UI: - Optimization loss over iterations - Metrics from train and validation datasets - Feature importance scores and plots - Additional metrics logged by <code>mlflow.log_metrics</code></p>
</section>
<section id="hyperparameter-tuning-and-mlflow-autologging" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-tuning-and-mlflow-autologging">Hyperparameter tuning and MLflow autologging</h2>
<p>After some testing, I learned that the autologging behavior changed depending on tuner and autologging types. I tested scikit-learn and LightGBM autologging, and scikit-learn and Dask-ML tuners. This resulted in the following four combinations to test:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Test #</th>
<th>LightGBM autologging</th>
<th>scikit-learn autologging</th>
<th>Tuner backend</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>No</td>
<td>Yes</td>
<td>scikit-learn</td>
</tr>
<tr class="even">
<td>2</td>
<td>No</td>
<td>Yes</td>
<td>dask-ml</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Yes</td>
<td>Yes</td>
<td>scikit-learn</td>
</tr>
<tr class="even">
<td>4</td>
<td>Yes</td>
<td>Yes</td>
<td>dask-ml</td>
</tr>
</tbody>
</table>
<section id="test-1.-sklearn-autolog-and-sklearn-tuner" class="level3">
<h3 class="anchored" data-anchor-id="test-1.-sklearn-autolog-and-sklearn-tuner">Test 1. sklearn autolog and sklearn tuner</h3>
<p>To reduce the interaction btw <code>mlflow.lightgbm.autolog</code> and <code>mlflow.sklearn.autolog</code>, I turned the former first.</p>
<div id="e3ab5e10-3192-4d76-beaa-4325b511fedf" class="cell" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">mlflow.lightgbm.autolog(disable<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<div id="f6daf717-37b6-4348-a2f8-e9005b9aabc6" class="cell" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">mlflow.sklearn.autolog(max_tuning_runs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># log all runs</span></span></code></pre></div>
</div>
<p>Here, I also used <code>PredefinedSplit</code> instead of k-fold to match the datasets for a hyperparameter search and evaluation parameters in LightGBM.</p>
<div id="033d7833-cbfb-40e3-b0d4-b8f5da4a2349" class="cell" data-tags="[]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">X_train_val, X_test, y_train_val, y_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_test_split(X, y, random_state<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seed)</span>
<span id="cb16-2"></span>
<span id="cb16-3">n_val_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb16-4">n_train_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train_val.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> n_val_size</span>
<span id="cb16-5">ps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> PredefinedSplit(test_fold<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_val_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>n_train_size)</span>
<span id="cb16-6"></span>
<span id="cb16-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> train_index, val_index <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> ps.split():</span>
<span id="cb16-8">    X_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train_val[train_index, :]</span>
<span id="cb16-9">    X_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_train_val[val_index, :]</span>
<span id="cb16-10">    y_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_train_val[train_index]</span>
<span id="cb16-11">    y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_train_val[val_index]</span></code></pre></div>
</div>
<p>Additionally, to be consistent with the autologging and tuner types, I used the scikit-learn API version of LightGBM (<code>LGBMClassifier</code>). For this tuning example, I chose <code>learning_rate</code> and <code>subsample</code> hyperparameters.</p>
<div id="82c861f2-d74a-4792-ba0a-e80460bcd864" class="cell" data-tags="[]" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">n_search <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb17-2"></span>
<span id="cb17-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> mlflow.start_run(run_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_1"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> run:</span>
<span id="cb17-4">    </span>
<span id="cb17-5">    clf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lgb.LGBMClassifier(</span>
<span id="cb17-6">        objective<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary"</span>,</span>
<span id="cb17-7">        metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary_logloss"</span>,</span>
<span id="cb17-8">        seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>seed,</span>
<span id="cb17-9">        class_weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"balanced"</span>,</span>
<span id="cb17-10">        n_estimators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>,</span>
<span id="cb17-11">    )</span>
<span id="cb17-12">    </span>
<span id="cb17-13">    pipe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Pipeline([(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"clf"</span>, clf)])</span>
<span id="cb17-14">    param_space <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb17-15">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"clf__learning_rate"</span>: np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb17-16">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"clf__subsample"</span>: np.linspace(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>),</span>
<span id="cb17-17">    }</span>
<span id="cb17-18">    </span>
<span id="cb17-19">    search_cv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomizedSearchCV(pipe, param_space, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ps, n_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_search)</span>
<span id="cb17-20">    search_cv.fit(</span>
<span id="cb17-21">        X_train_val,</span>
<span id="cb17-22">        y_train_val,</span>
<span id="cb17-23">        clf__eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_val, y_val)],</span>
<span id="cb17-24">        clf__eval_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'val'</span>],</span>
<span id="cb17-25">        clf__eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'binary_logloss'</span>],</span>
<span id="cb17-26">    )</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] val's binary_logloss: 0.624749
[2] val's binary_logloss: 0.568357
[3] val's binary_logloss: 0.520761
[4] val's binary_logloss: 0.480421
[5] val's binary_logloss: 0.442099
[6] val's binary_logloss: 0.410214
[7] val's binary_logloss: 0.382292
[8] val's binary_logloss: 0.357785
[9] val's binary_logloss: 0.335433
[10]    val's binary_logloss: 0.31392
[1] val's binary_logloss: 0.637054
[2] val's binary_logloss: 0.589451
[3] val's binary_logloss: 0.547508
[4] val's binary_logloss: 0.511241
[5] val's binary_logloss: 0.478765
[6] val's binary_logloss: 0.448812
[7] val's binary_logloss: 0.423736
[8] val's binary_logloss: 0.398193
[9] val's binary_logloss: 0.377066
[10]    val's binary_logloss: 0.356542
[1] val's binary_logloss: 0.653833
[2] val's binary_logloss: 0.618878
[3] val's binary_logloss: 0.586995
[4] val's binary_logloss: 0.558061
[5] val's binary_logloss: 0.531579
[6] val's binary_logloss: 0.507355
[7] val's binary_logloss: 0.485109
[8] val's binary_logloss: 0.463636
[9] val's binary_logloss: 0.444977
[10]    val's binary_logloss: 0.427695
[1] val's binary_logloss: 0.620853
[2] val's binary_logloss: 0.56225
[3] val's binary_logloss: 0.512275
[4] val's binary_logloss: 0.464753
[5] val's binary_logloss: 0.426894
[6] val's binary_logloss: 0.390463
[7] val's binary_logloss: 0.358157
[8] val's binary_logloss: 0.330555
[9] val's binary_logloss: 0.305165
[10]    val's binary_logloss: 0.282671</code></pre>
</div>
</div>
<p>The UI showed that 1 parent run and <code>n_search</code> (3) child runs were created, where the parent run had the autologged metrics such as confusion matrix, ROC curve, and PR curve:</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/test1.png" class="img-fluid"></p>
<p><code>cv_results</code> was also returned and the logged metrics from all child runs were similar to <code>cv_results</code>.</p>
</section>
<section id="test-2.-sklearn-autolog-and-dask-ml-tuner" class="level3">
<h3 class="anchored" data-anchor-id="test-2.-sklearn-autolog-and-dask-ml-tuner">Test 2. sklearn autolog and dask-ml tuner</h3>
<div id="3bbff4d0-39d3-4832-8711-1e82a11916d0" class="cell" data-tags="[]" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Client(processes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, threads_per_worker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, n_workers<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</div>
<div id="dc7954c5-4d19-4b2c-94fc-8d2b25f8dd4a" class="cell" data-tags="[]" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> mlflow.start_run(run_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"test_2"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> run:</span>
<span id="cb20-2">    </span>
<span id="cb20-3">    search_cv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dask_RandomizedSearchCV(pipe, param_space, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ps, n_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_search)</span>
<span id="cb20-4">    search_cv.fit(</span>
<span id="cb20-5">        X_train_val,</span>
<span id="cb20-6">        y_train_val,</span>
<span id="cb20-7">        clf__eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_val, y_val)],</span>
<span id="cb20-8">        clf__eval_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"val"</span>],</span>
<span id="cb20-9">        clf__eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary_logloss"</span>],</span>
<span id="cb20-10">    )</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] val's binary_logloss: 0.6207[1] val's binary_logloss: 0.6207
[1] val's binary_logloss: 0.64539

[2] val's binary_logloss: 0.603905
[2] val's binary_logloss: 0.561648
[2] val's binary_logloss: 0.561648
[3] val's binary_logloss: 0.566663
[3] val's binary_logloss: 0.51229
[3] val's binary_logloss: 0.51229
[4] val's binary_logloss: 0.533792
[4] val's binary_logloss: 0.470843
[4] val's binary_logloss: 0.470843
[5] val's binary_logloss: 0.504106
[5] val's binary_logloss: 0.43163
[5] val's binary_logloss: 0.43163
[6] val's binary_logloss: 0.476199
[6] val's binary_logloss: 0.399279
[6] val's binary_logloss: 0.399279
[7] val's binary_logloss: 0.452345
[7] val's binary_logloss: 0.371128
[7] val's binary_logloss: 0.371128
[8] val's binary_logloss: 0.430183
[8] val's binary_logloss: 0.345312
[8] val's binary_logloss: 0.345312
[9] val's binary_logloss: 0.409486
[9] val's binary_logloss: 0.323905
[9] val's binary_logloss: 0.323905
[10]    val's binary_logloss: 0.388993
[10]    val's binary_logloss: 0.303382
[10]    val's binary_logloss: 0.303382
[1] val's binary_logloss: 0.642694
[2] val's binary_logloss: 0.599408
[3] val's binary_logloss: 0.5599
[4] val's binary_logloss: 0.525228
[5] val's binary_logloss: 0.490788
[6] val's binary_logloss: 0.461964
[7] val's binary_logloss: 0.433798
[8] val's binary_logloss: 0.409936
[9] val's binary_logloss: 0.385757
[10]    val's binary_logloss: 0.365051</code></pre>
</div>
</div>
<p><code>mlflow.sklearn.autolog</code> still created confusion matrix, ROC curve, and PR curve but <strong>only a single run is returned,</strong> and all child runs are now missing. Besides, the UI also didn’t log <code>cv_results</code>.</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/test2.png" width="400"></p>
<section id="is-the-only-logged-run-the-best-run" class="level4">
<h4 class="anchored" data-anchor-id="is-the-only-logged-run-the-best-run">Is the only logged run the best run?</h4>
<p>Here was where the behavior of <code>mlflow.sklearn.autolog</code> changed. It was supposed to return a single parent run and multiple child runs but when <code>dask-ml</code> was used as tuner, it only logged a single run. I didn’t know whether thi was the best run or not, so I decided to compare the MLflow logged result with the actual search result.</p>
<div id="2977fc6e-2d8b-4e50-9201-84d886a080cd" class="cell" data-tags="[]" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">mlflow_run <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlflow_client.get_run(run.info.run_id)</span></code></pre></div>
</div>
<div id="a31b2c59-a15e-41d1-9c6f-d10948db8417" class="cell" data-tags="[]" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(search_cv.best_estimator_.get_params()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__learning_rate'</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb23-2">mlflow_run.data.params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__learning_rate'</span>]</span>
<span id="cb23-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(search_cv.best_estimator_.get_params()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__subsample'</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb23-4">mlflow_run.data.params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__subsample'</span>]</span></code></pre></div>
</div>
<p>Luckily, the assertions have passed, meaning that the single recorded run by MLflow was the best run. Except that users can’t see the child runs in the UI, this behavior seems acceptable.</p>
</section>
</section>
<section id="test-3.-lightgbmsklearn-autolog-and-sklearn-tuner" class="level3">
<h3 class="anchored" data-anchor-id="test-3.-lightgbmsklearn-autolog-and-sklearn-tuner">Test 3. lightgbm+sklearn autolog and sklearn tuner</h3>
<p>This test idea came to my mind becasue I imagined it would be very convenient if one could use autologging on top of a sklearn tuner. Thus, I decided to turn on lightgbm autologging in addition to the sklearn autologging.</p>
<div id="388183c5-ffaf-4d93-bf23-ce499ed243d0" class="cell" data-tags="[]" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">mlflow.lightgbm.autolog()</span></code></pre></div>
</div>
<div id="1a629ef7-4a01-406e-af8a-eb6ccd74353f" class="cell" data-tags="[]" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> mlflow.start_run(run_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test_3'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> run:</span>
<span id="cb25-2">    </span>
<span id="cb25-3">    search_cv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> RandomizedSearchCV(pipe, param_space, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ps, n_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_search)</span>
<span id="cb25-4">    search_cv.fit(</span>
<span id="cb25-5">        X_train_val,</span>
<span id="cb25-6">        y_train_val,</span>
<span id="cb25-7">        clf__eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_val, y_val)],</span>
<span id="cb25-8">        clf__eval_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"val"</span>],</span>
<span id="cb25-9">        clf__eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary_logloss"</span>],</span>
<span id="cb25-10">    )</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] val's binary_logloss: 0.64539
[2] val's binary_logloss: 0.603905
[3] val's binary_logloss: 0.566663
[4] val's binary_logloss: 0.533792
[5] val's binary_logloss: 0.504106
[6] val's binary_logloss: 0.476199
[7] val's binary_logloss: 0.452345
[8] val's binary_logloss: 0.430183
[9] val's binary_logloss: 0.409486
[10]    val's binary_logloss: 0.388993
[1] val's binary_logloss: 0.6207
[2] val's binary_logloss: 0.561648
[3] val's binary_logloss: 0.51229
[4] val's binary_logloss: 0.470843
[5] val's binary_logloss: 0.43163
[6] val's binary_logloss: 0.399279
[7] val's binary_logloss: 0.371128
[8] val's binary_logloss: 0.345312
[9] val's binary_logloss: 0.323905
[10]    val's binary_logloss: 0.303382
[1] val's binary_logloss: 0.632926
[2] val's binary_logloss: 0.582412
[3] val's binary_logloss: 0.538314
[4] val's binary_logloss: 0.500565
[5] val's binary_logloss: 0.467006
[6] val's binary_logloss: 0.436234
[7] val's binary_logloss: 0.410759
[8] val's binary_logloss: 0.384813
[9] val's binary_logloss: 0.361938
[10]    val's binary_logloss: 0.342656
[1] val's binary_logloss: 0.642694
[2] val's binary_logloss: 0.599408
[3] val's binary_logloss: 0.5599
[4] val's binary_logloss: 0.525228
[5] val's binary_logloss: 0.490788
[6] val's binary_logloss: 0.461964
[7] val's binary_logloss: 0.433798
[8] val's binary_logloss: 0.409936
[9] val's binary_logloss: 0.385757
[10]    val's binary_logloss: 0.365051</code></pre>
</div>
</div>
<p>This time, I found that sklearn autologging behaved normally but lightgbm autologging didn’t work at all. First, lightgbm autologging metrics such as feature importance scores were missing:</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/test3.png" width="400"></p>
<p>Second, <code>training-log_loss</code> wasn’t logged for every iteration but it was logged as a single numeric value, and thus was visualized as a bar graph:</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/test3_1.png" class="img-fluid"></p>
</section>
<section id="test-4.-lightgbmsklearn-autolog-and-dask-ml-tuner" class="level3">
<h3 class="anchored" data-anchor-id="test-4.-lightgbmsklearn-autolog-and-dask-ml-tuner">Test 4. lightgbm+sklearn autolog and dask-ml tuner</h3>
<p>Finally, I used the dask-ml tuner, lightgbm and sklearn autologging altogether.</p>
<div id="7d789316-6f32-425b-8082-c3cf605d3df9" class="cell" data-tags="[]" data-execution_count="18">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> mlflow.start_run(run_name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'test_4'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> run:</span>
<span id="cb27-2">    </span>
<span id="cb27-3">    search_cv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dask_RandomizedSearchCV(pipe, param_space, cv<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ps, n_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>n_search)</span>
<span id="cb27-4">    search_cv.fit(</span>
<span id="cb27-5">        X_train_val,</span>
<span id="cb27-6">        y_train_val,</span>
<span id="cb27-7">        clf__eval_set<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[(X_val, y_val)],</span>
<span id="cb27-8">        clf__eval_names<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"val"</span>],</span>
<span id="cb27-9">        clf__eval_metric<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"binary_logloss"</span>],</span>
<span id="cb27-10">    )</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] val's binary_logloss: 0.632926[1]   val's binary_logloss: 0.653833

[1] val's binary_logloss: 0.632926
[2] val's binary_logloss: 0.582412
[2] val's binary_logloss: 0.618878
[2] val's binary_logloss: 0.582412
[3] val's binary_logloss: 0.586995
[3] val's binary_logloss: 0.538314
[3] val's binary_logloss: 0.538314
[4] val's binary_logloss: 0.558061
[4] val's binary_logloss: 0.500565
[4] val's binary_logloss: 0.500565
[5] val's binary_logloss: 0.531579
[5] val's binary_logloss: 0.467006
[5] val's binary_logloss: 0.467006
[6] val's binary_logloss: 0.507355
[6] val's binary_logloss: 0.436234
[6] val's binary_logloss: 0.436234
[7] val's binary_logloss: 0.485109
[7] val's binary_logloss: 0.410759
[7] val's binary_logloss: 0.410759
[8] val's binary_logloss: 0.463636
[8] val's binary_logloss: 0.384813
[8] val's binary_logloss: 0.384813
[9] val's binary_logloss: 0.444977
[9] val's binary_logloss: 0.361938
[9] val's binary_logloss: 0.361938
[10]    val's binary_logloss: 0.427695
[10]    val's binary_logloss: 0.342656
[10]    val's binary_logloss: 0.342656
[1] val's binary_logloss: 0.651621
[2] val's binary_logloss: 0.615177
[3] val's binary_logloss: 0.581242
[4] val's binary_logloss: 0.550942
[5] val's binary_logloss: 0.522853
[6] val's binary_logloss: 0.494685
[7] val's binary_logloss: 0.470868
[8] val's binary_logloss: 0.447008
[9] val's binary_logloss: 0.42506
[10]    val's binary_logloss: 0.406164</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>2023/03/19 23:02:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 4f0ea51beb0748139aa4364c5d332279. Failed operations: [MlflowException("API request to http://127.0.0.1:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\'127.0.0.1\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\'too many 500 error responses\'))")]')]</code></pre>
</div>
</div>
<p>This time, similar to Test 2, a single run was returned but it seemed that lightgbm autologging actually worked because the UI generated images from both sklearn and lightgbm autologging methods:</p>
<p><img src="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/test4.png" width="400"></p>
<p>However, only single run was returned, again like in Test 2. Unfortunately, this time, this single run didn’t pass the assertion test.</p>
<div id="6fbc0c8d-a207-4fbc-a084-e7e86a1a8b19" class="cell" data-tags="[]" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">mlflow_run <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mlflow_client.get_run(run.info.run_id)</span></code></pre></div>
</div>
<div id="527128e5-6c5e-4e62-8e0f-7616560f7701" class="cell" data-tags="[]" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(search_cv.best_estimator_.get_params()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__learning_rate'</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb31-2">mlflow_run.data.params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>]</span>
<span id="cb31-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">assert</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(search_cv.best_estimator_.get_params()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__subsample'</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb31-4">mlflow_run.data.params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>]</span></code></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">AssertionError</span>                            Traceback (most recent call last)
Cell <span class="ansi-green-fg">In[21], line 1</span>
<span class="ansi-green-fg">----&gt; 1</span> <span style="font-weight:bold;color:rgb(0,135,0)">assert</span> <span style="color:rgb(0,135,0)">str</span>(search_cv<span style="color:rgb(98,98,98)">.</span>best_estimator_<span style="color:rgb(98,98,98)">.</span>get_params()[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">clf__learning_rate</span><span style="color:rgb(175,0,0)">'</span>]) <span style="color:rgb(98,98,98)">==</span> \
<span class="ansi-green-fg ansi-bold">      2</span> mlflow_run<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>params[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">learning_rate</span><span style="color:rgb(175,0,0)">'</span>]
<span class="ansi-green-fg ansi-bold">      3</span> <span style="font-weight:bold;color:rgb(0,135,0)">assert</span> <span style="color:rgb(0,135,0)">str</span>(search_cv<span style="color:rgb(98,98,98)">.</span>best_estimator_<span style="color:rgb(98,98,98)">.</span>get_params()[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">clf__subsample</span><span style="color:rgb(175,0,0)">'</span>]) <span style="color:rgb(98,98,98)">==</span> \
<span class="ansi-green-fg ansi-bold">      4</span> mlflow_run<span style="color:rgb(98,98,98)">.</span>data<span style="color:rgb(98,98,98)">.</span>params[<span style="color:rgb(175,0,0)">'</span><span style="color:rgb(175,0,0)">subsample</span><span style="color:rgb(175,0,0)">'</span>]

<span class="ansi-red-fg">AssertionError</span>: </pre>
</div>
</div>
</div>
<div id="d93e952b-4be8-46d7-9afd-7176dbc5ace4" class="cell" data-tags="[]" data-execution_count="22">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(search_cv.best_estimator_.get_params()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__learning_rate'</span>]),</span>
<span id="cb32-2">      mlflow_run.data.params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'learning_rate'</span>])</span>
<span id="cb32-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>(search_cv.best_estimator_.get_params()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'clf__subsample'</span>]),</span>
<span id="cb32-4">      mlflow_run.data.params[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'subsample'</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.05 0.07777777777777778
0.5 0.7000000000000001</code></pre>
</div>
</div>
<p>This means that when dask-ml, sklearn autolog, and lightgbm autologgin are used at once, we cannot trust the MLflow tracking UI becasue the single set of represented hyperparameters in the UI are not the best estimator’s hyperparameters. This means this combination gives unreliable results, which we should avoid at all costs.</p>
<div id="b4ff5db1-d59e-4445-adfc-be8a9f9c5594" class="cell" data-tags="[]" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">client.close()</span></code></pre></div>
</div>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>In this notebook, I demonstrated how different combinations of autologging and tuners could produce different results. Some of these changed behaviors were simple omissions but I found a more troubling combination as well where the results were just simply wrong. This suggests that when it comes to testing interoperability, we should not only check whether they work together but also whether the returned results are accurate.</p>


</section>

 ]]></description>
  <category>ML</category>
  <category>ML Ops</category>
  <category>Interoperability</category>
  <guid>https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/2023-03-17-mlflow-lightgbm-dask.html</guid>
  <pubDate>Fri, 17 Mar 2023 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2023-03-17-mlflow-lightgbm-dask/images/index.png" medium="image" type="image/png" height="109" width="144"/>
</item>
<item>
  <title>Comparing type inference methods for mixed data arrays</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2023-03-02-compare-type-inference/2023-03-02-compare-type-inference.html</link>
  <description><![CDATA[ 





<p>Some people work with data where the meaning of features (columns) is very clear because only common sense is required. For instance, even without a schema, in a housing price dataset, a column called “number of rooms” would be the number of rooms in a housing unit, and it’s very likely that the values of this column will be integers.</p>
<p>In hardware (microprocessor) verification, it’s often impossible to understand the meaning of the columns. If you are an ML practitioner without hardware engineering background, you can nag verification engineers to explain it but it’s very likely that you wouldn’t completely understand, and there are hundreds and thousands of columns that need explanation. Even if you do have the background, depending on the product type, it’s likely that you can’t have full understanding of all columns.</p>
<p>Besides, sometimes you need to work with so called “mixed data type” arrays. An example would be an array of boolean and float such as <code>[True, 0.0]</code>. If you use pandas to read this type of data, you should know that it infers the data type of an array like this as <code>object</code> quite often. This inference is done by the <code>pandas.DataFrame.infer_objects</code> method. However, a lot of different types of mixed arrays can be inferred as <code>object</code> dtype. This “blanket” approach might be useful for practical data handling but it is not suitable for more accurate and granular type inference. If the goal is to <em>understand the actual content</em> of the arrays.</p>
<p>You may not have known that pandas has another type inference method in their api: <code>pandas.api.types.infer_dtype</code>, which provides granular type inference and allows to ignore null values (<code>skipna=True</code>). This method returns a name of inferred type as a string such as <code>"boolean"</code> or <code>"floating"</code>. For the comprehensive list of the type names, see the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.api.types.infer_dtype.html">pandas documentation</a>.</p>
<p>This notebook compares the two type inference methods of pandas (<code>pandas.DataFrame.infer_objects</code> and <code>pandas.api.types.infer_dtype</code>) when they are faced with various cases of mixed data type arrays. For the comparison, I used exhaustive combination of <code>None, array(list), str, bool, float, int</code> data to generate various mixed arrays, and then applied the two inference methods to compare the results.</p>
<div id="3791c520-1a4e-447a-b5c8-3c6c801ec4d8" class="cell" data-tags="[]" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span></code></pre></div>
</div>
<section id="testing-data-generating-arrays-of-mixed-data-types" class="level2">
<h2 class="anchored" data-anchor-id="testing-data-generating-arrays-of-mixed-data-types">Testing data: generating arrays of mixed data types</h2>
<p>Here I generated a dataframe with various mixed types: <code>"nan"(</code>np.nan<code>), "none", "array" (</code>list<code>), "str", "bool", "float", "int"</code>. Using their exhaustive combinations (<img src="https://latex.codecogs.com/png.latex?N_%7Btype%7D=2">), I created a 2-element array for each combination. For fair comparison, I assigned <code>object</code> dtypes to all columns.</p>
<div id="b2a4941e-169a-4482-bcfa-cfd42c6813cc" class="cell" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">example <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(</span>
<span id="cb2-2">    {</span>
<span id="cb2-3">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan'</span>: [np.nan, np.nan],</span>
<span id="cb2-4">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan_none'</span>: [np.nan, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>],</span>
<span id="cb2-5">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan_array'</span>: [np.nan, []],</span>
<span id="cb2-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan_str'</span>: [np.nan, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>],</span>
<span id="cb2-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan_bool'</span>: [np.nan, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>],        </span>
<span id="cb2-8">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan_float'</span>: [np.nan, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],        </span>
<span id="cb2-9">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nan_int'</span>: [np.nan, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],</span>
<span id="cb2-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>],        </span>
<span id="cb2-11">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none_array'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, []],</span>
<span id="cb2-12">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none_str'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>],        </span>
<span id="cb2-13">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none_bool'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>],</span>
<span id="cb2-14">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none_float'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>],</span>
<span id="cb2-15">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'none_int'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],</span>
<span id="cb2-16">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'array'</span>: [[], []],</span>
<span id="cb2-17">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'array_str'</span>: [[], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>],</span>
<span id="cb2-18">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'array_bool'</span>: [[], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>],</span>
<span id="cb2-19">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'array_float'</span>: [[], <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],</span>
<span id="cb2-20">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'array_int'</span>: [[], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],</span>
<span id="cb2-21">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'str'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span>],</span>
<span id="cb2-22">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'str_bool'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>],</span>
<span id="cb2-23">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'str_float'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>],</span>
<span id="cb2-24">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'str_int'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],</span>
<span id="cb2-25">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bool'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>],</span>
<span id="cb2-26">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bool_float'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>],</span>
<span id="cb2-27">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bool_int'</span>: [<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],</span>
<span id="cb2-28">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'float'</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>],</span>
<span id="cb2-29">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'float_int'</span>: [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb2-30">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'int'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb2-31">    },</span>
<span id="cb2-32">    dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span></span>
<span id="cb2-33">)</span>
<span id="cb2-34"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(example.dtypes.value_counts())</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>object    28
dtype: int64</code></pre>
</div>
</div>
<div id="992bfe14-39d5-41b9-b4b6-81311e9fdbac" class="cell" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">example.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">nan</th>
<th data-quarto-table-cell-role="th">nan_none</th>
<th data-quarto-table-cell-role="th">nan_array</th>
<th data-quarto-table-cell-role="th">nan_str</th>
<th data-quarto-table-cell-role="th">nan_bool</th>
<th data-quarto-table-cell-role="th">nan_float</th>
<th data-quarto-table-cell-role="th">nan_int</th>
<th data-quarto-table-cell-role="th">none</th>
<th data-quarto-table-cell-role="th">none_array</th>
<th data-quarto-table-cell-role="th">none_str</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">str</th>
<th data-quarto-table-cell-role="th">str_bool</th>
<th data-quarto-table-cell-role="th">str_float</th>
<th data-quarto-table-cell-role="th">str_int</th>
<th data-quarto-table-cell-role="th">bool</th>
<th data-quarto-table-cell-role="th">bool_float</th>
<th data-quarto-table-cell-role="th">bool_int</th>
<th data-quarto-table-cell-role="th">float</th>
<th data-quarto-table-cell-role="th">float_int</th>
<th data-quarto-table-cell-role="th">int</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>NaN</td>
<td>None</td>
<td>None</td>
<td>None</td>
<td>...</td>
<td>a</td>
<td>a</td>
<td>a</td>
<td>a</td>
<td>True</td>
<td>True</td>
<td>True</td>
<td>1.0</td>
<td>1.0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>NaN</td>
<td>None</td>
<td>[]</td>
<td>a</td>
<td>True</td>
<td>1.0</td>
<td>1</td>
<td>None</td>
<td>[]</td>
<td>a</td>
<td>...</td>
<td>b</td>
<td>True</td>
<td>1.0</td>
<td>1</td>
<td>False</td>
<td>0.0</td>
<td>1</td>
<td>0.0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>

<p>2 rows × 28 columns</p>
</div>
</div>
</div>
</section>
<section id="type-inference-with-pandas.dataframe.infer_objects" class="level2">
<h2 class="anchored" data-anchor-id="type-inference-with-pandas.dataframe.infer_objects">Type inference with <code>pandas.DataFrame.infer_objects</code></h2>
<div id="889a05aa-2582-4b9f-bd04-40367eb0e344" class="cell" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">example_results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> example.T</span>
<span id="cb5-2">example_results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_objects'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> example.infer_objects().dtypes</span>
<span id="cb5-3">example_results</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">pd_infer_objects</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan</td>
<td>NaN</td>
<td>NaN</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_none</td>
<td>NaN</td>
<td>None</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_array</td>
<td>NaN</td>
<td>[]</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_str</td>
<td>NaN</td>
<td>a</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_bool</td>
<td>NaN</td>
<td>True</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_float</td>
<td>NaN</td>
<td>1.0</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_int</td>
<td>NaN</td>
<td>1</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none</td>
<td>None</td>
<td>None</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_array</td>
<td>None</td>
<td>[]</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none_str</td>
<td>None</td>
<td>a</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_bool</td>
<td>None</td>
<td>True</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none_float</td>
<td>None</td>
<td>0.0</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_int</td>
<td>None</td>
<td>1</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array</td>
<td>[]</td>
<td>[]</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_str</td>
<td>[]</td>
<td>a</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array_bool</td>
<td>[]</td>
<td>True</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_float</td>
<td>[]</td>
<td>1.0</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array_int</td>
<td>[]</td>
<td>1</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">str</td>
<td>a</td>
<td>b</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">str_bool</td>
<td>a</td>
<td>True</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">str_float</td>
<td>a</td>
<td>1.0</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">str_int</td>
<td>a</td>
<td>1</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">bool</td>
<td>True</td>
<td>False</td>
<td>bool</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">bool_float</td>
<td>True</td>
<td>0.0</td>
<td>object</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">bool_int</td>
<td>True</td>
<td>1</td>
<td>object</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">float</td>
<td>1.0</td>
<td>0.0</td>
<td>float64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">float_int</td>
<td>1.0</td>
<td>0</td>
<td>float64</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">int</td>
<td>1</td>
<td>0</td>
<td>int64</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>At a glance, this method infers most of these mixed arrays as <code>object</code>, which naturally doesn’t deliver much information about what exact mixture of types the arrays have. Plus, some <code>object</code> arrays can receive <code>int</code> or <code>float</code> casting (e.g., <code>[True, 1]</code>), but some can’t (e.g., <code>['a', 1]</code>).</p>
</section>
<section id="type-inference-with-pandas.api.types.infer_dtype" class="level2">
<h2 class="anchored" data-anchor-id="type-inference-with-pandas.api.types.infer_dtype">Type inference with <code>pandas.api.types.infer_dtype</code></h2>
<p>This method allows two variants: with skipping na values and without. Let’s get inference results from the both.</p>
<div id="2fffe96a-ee40-4a50-8972-b46906f75b2c" class="cell" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">example_results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_dtype'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> example.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x:pd.api.types.infer_dtype(x, skipna<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>))</span>
<span id="cb6-2">example_results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_dtype_skipna'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> example.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">apply</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x:pd.api.types.infer_dtype(x, skipna<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</div>
<div id="718bcc57" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">example_results</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">pd_infer_objects</th>
<th data-quarto-table-cell-role="th">pd_infer_dtype</th>
<th data-quarto-table-cell-role="th">pd_infer_dtype_skipna</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan</td>
<td>NaN</td>
<td>NaN</td>
<td>float64</td>
<td>floating</td>
<td>empty</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_none</td>
<td>NaN</td>
<td>None</td>
<td>float64</td>
<td>mixed</td>
<td>empty</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_array</td>
<td>NaN</td>
<td>[]</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_str</td>
<td>NaN</td>
<td>a</td>
<td>object</td>
<td>mixed</td>
<td>string</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_bool</td>
<td>NaN</td>
<td>True</td>
<td>object</td>
<td>mixed</td>
<td>boolean</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_float</td>
<td>NaN</td>
<td>1.0</td>
<td>float64</td>
<td>floating</td>
<td>floating</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_int</td>
<td>NaN</td>
<td>1</td>
<td>float64</td>
<td>integer-na</td>
<td>integer</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none</td>
<td>None</td>
<td>None</td>
<td>object</td>
<td>mixed</td>
<td>empty</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_array</td>
<td>None</td>
<td>[]</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none_str</td>
<td>None</td>
<td>a</td>
<td>object</td>
<td>mixed</td>
<td>string</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_bool</td>
<td>None</td>
<td>True</td>
<td>object</td>
<td>mixed</td>
<td>boolean</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none_float</td>
<td>None</td>
<td>0.0</td>
<td>float64</td>
<td>mixed</td>
<td>mixed-integer-float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_int</td>
<td>None</td>
<td>1</td>
<td>float64</td>
<td>mixed-integer</td>
<td>integer</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array</td>
<td>[]</td>
<td>[]</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_str</td>
<td>[]</td>
<td>a</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array_bool</td>
<td>[]</td>
<td>True</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_float</td>
<td>[]</td>
<td>1.0</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array_int</td>
<td>[]</td>
<td>1</td>
<td>object</td>
<td>mixed-integer</td>
<td>mixed-integer</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">str</td>
<td>a</td>
<td>b</td>
<td>object</td>
<td>string</td>
<td>string</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">str_bool</td>
<td>a</td>
<td>True</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">str_float</td>
<td>a</td>
<td>1.0</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">str_int</td>
<td>a</td>
<td>1</td>
<td>object</td>
<td>mixed-integer</td>
<td>mixed-integer</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">bool</td>
<td>True</td>
<td>False</td>
<td>bool</td>
<td>boolean</td>
<td>boolean</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">bool_float</td>
<td>True</td>
<td>0.0</td>
<td>object</td>
<td>mixed</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">bool_int</td>
<td>True</td>
<td>1</td>
<td>object</td>
<td>mixed-integer</td>
<td>mixed-integer</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">float</td>
<td>1.0</td>
<td>0.0</td>
<td>float64</td>
<td>floating</td>
<td>floating</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">float_int</td>
<td>1.0</td>
<td>0</td>
<td>float64</td>
<td>mixed-integer-float</td>
<td>mixed-integer-float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">int</td>
<td>1</td>
<td>0</td>
<td>int64</td>
<td>integer</td>
<td>integer</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="comparison-with-vs.-without-na-values-in-pandas.api.types.infer_dtype" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-vs.-without-na-values-in-pandas.api.types.infer_dtype">Comparison: with vs.&nbsp;without na values in <code>pandas.api.types.infer_dtype</code></h2>
<p>When we don’t skip na values (<code>skipna=False</code>), we often get <code>"mixed"</code> results from <code>pandas.api.types.infer_dtype</code> for arrays that are inferred as <code>object</code> by <code>pandas.DataFrame.infer_objects</code>. This means, the inference results are not granular, like we just saw from <code>pandas.DataFrame.infer_objects</code>. For instance, in the table above, <code>"nan_array", "nan_str", "nan_bool"</code> are all identified as <code>"mixed"</code> when we don’t ignore nan.</p>
<div id="aab69619" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">example_results.loc[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nan_array"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nan_str"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nan_bool"</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pd_infer_dtype"</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>nan_array    mixed
nan_str      mixed
nan_bool     mixed
Name: pd_infer_dtype, dtype: object</code></pre>
</div>
</div>
<p>However, when we ignore na values, we get more granular results, which identify the correct data types (without missing).</p>
<div id="819b8ecf" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">example_results.loc[[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nan_array"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nan_str"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"nan_bool"</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pd_infer_dtype_skipna"</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>nan_array      mixed
nan_str       string
nan_bool     boolean
Name: pd_infer_dtype_skipna, dtype: object</code></pre>
</div>
</div>
</section>
<section id="comparison-pandas.dataframe.infer_objects-vs.-pandas.api.types.infer_dtypeskipnatrue" class="level2">
<h2 class="anchored" data-anchor-id="comparison-pandas.dataframe.infer_objects-vs.-pandas.api.types.infer_dtypeskipnatrue">Comparison: <code>pandas.DataFrame.infer_objects</code> vs.&nbsp;<code>pandas.api.types.infer_dtype(skipna=True)</code></h2>
<p>Because <code>pandas.DataFrame.infer_objects</code> has a blanket approach to mixed data arrays, using this method to various mixed arrays, we get a lot of <code>object</code> columns. Let’s take a deeper look at the columns inferred as <code>object</code> by <code>pandas.DataFrame.infer_objects</code>, and examine the inference result from <code>pandas.api.types.infer_dtype(skipna=True)</code>.</p>
<div id="f8e3242e-7d90-4e7e-a7a3-1012464128ca" class="cell" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">example_results[example_results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_objects'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">object</span>].drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_dtype'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).sort_values(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_dtype_skipna'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">pd_infer_objects</th>
<th data-quarto-table-cell-role="th">pd_infer_dtype_skipna</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">nan_bool</td>
<td>NaN</td>
<td>True</td>
<td>object</td>
<td>boolean</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none_bool</td>
<td>None</td>
<td>True</td>
<td>object</td>
<td>boolean</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none</td>
<td>None</td>
<td>None</td>
<td>object</td>
<td>empty</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_array</td>
<td>NaN</td>
<td>[]</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">str_float</td>
<td>a</td>
<td>1.0</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">str_bool</td>
<td>a</td>
<td>True</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_float</td>
<td>[]</td>
<td>1.0</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array_bool</td>
<td>[]</td>
<td>True</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_str</td>
<td>[]</td>
<td>a</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">array</td>
<td>[]</td>
<td>[]</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">none_array</td>
<td>None</td>
<td>[]</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">bool_float</td>
<td>True</td>
<td>0.0</td>
<td>object</td>
<td>mixed</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">array_int</td>
<td>[]</td>
<td>1</td>
<td>object</td>
<td>mixed-integer</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">str_int</td>
<td>a</td>
<td>1</td>
<td>object</td>
<td>mixed-integer</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">bool_int</td>
<td>True</td>
<td>1</td>
<td>object</td>
<td>mixed-integer</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">none_str</td>
<td>None</td>
<td>a</td>
<td>object</td>
<td>string</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">str</td>
<td>a</td>
<td>b</td>
<td>object</td>
<td>string</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">nan_str</td>
<td>NaN</td>
<td>a</td>
<td>object</td>
<td>string</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This shows that a variety of mixed arrays is inferred as <code>object</code> by <code>pandas.DataFrame.infer_objects</code> but <code>pandas.api.types.infer_dtype(skipna=True)</code> can often identify true types. It’s true that the latter returns a lot of different arrays as <code>"mixed"</code> but most of them have non-numerical values such as string or array.</p>
<p>One interesting observation is that <code>[True, 0.0]</code> is inferred as <code>"mixed"</code> but <code>[True, 1]</code> as <code>"mixed-integer"</code>, which implies that <code>pandas.api.types.infer_dtype</code> method is designed to highlight the presence of integers in inferred type information.</p>
<p>Finally, we can compare the returned values of two methods:</p>
<div id="bd601df0-8797-4da3-8d81-9006bb4e60c1" class="cell" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> val <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(example_results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_objects'</span>]):</span>
<span id="cb13-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(val, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(val))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>int64 &lt;class 'numpy.dtype[int64]'&gt;
float64 &lt;class 'numpy.dtype[float64]'&gt;
bool &lt;class 'numpy.dtype[bool_]'&gt;
object &lt;class 'numpy.dtype[object_]'&gt;</code></pre>
</div>
</div>
<div id="4ea2f495-c820-4733-8611-fdb8ead00094" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> val <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(example_results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'pd_infer_dtype_skipna'</span>]):</span>
<span id="cb15-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(val, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(val))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>mixed &lt;class 'str'&gt;
floating &lt;class 'str'&gt;
boolean &lt;class 'str'&gt;
mixed-integer &lt;class 'str'&gt;
string &lt;class 'str'&gt;
integer &lt;class 'str'&gt;
mixed-integer-float &lt;class 'str'&gt;
empty &lt;class 'str'&gt;</code></pre>
</div>
</div>
<p>This shows that <code>pandas.DataFrame.infer_objects</code> returns a readily usable python types as inference results but <code>pandas.api.types.infer_dtype</code> returns string values, that need to be further processed or mapped if we want to cast more granular data types to these mixed arrays.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Hardware verification datasets often do not have schema and the feature meanings cannot be understood without extremely specialized domain knowledge. The fact that the datasets often have mixed data type arrays makes it difficult for ML practitioners to understand the content of the datasets. Therefore, type inference becomes an important step in the data digestion stage.</p>
<p>We can use pandas for type inference. But it has two methods: <code>pandas.DataFrame.infer_objects</code>, <code>pandas.api.types.infer_dtype</code>. The former (<code>pandas.DataFrame.infer_objects</code>) is designed to return practical python data types that can be easily cast on arrays. Thus its type inference tends to adopt a blanket approach where inferred type should work without any further steps to handle the data immediately.</p>
<p>On the other hand, <code>pandas.api.types.infer_dtype</code> does a more granular type inference job where it can also ignore na values. However, it returns string values as a result, not python types. Therefore, we need a further process to use this information for type casting such as <code>"boolean" -&gt; bool</code>.</p>


</section>

 ]]></description>
  <category>ML</category>
  <category>data preprocessing</category>
  <guid>https://hongsupshin.github.io/posts/2023-03-02-compare-type-inference/2023-03-02-compare-type-inference.html</guid>
  <pubDate>Thu, 02 Mar 2023 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Zero-Shot Text-to-Image Generation</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2022-12-15/</link>
  <description><![CDATA[ 





<p>Generative AI has a lot of hype in ML community these days. OpenAI’s DALL·E, GPT-3, and ChatGPT are good examples. And there’s also stable diffusion. Since they all have public API, not just ML practitioners but general public can use the models to generate texts or images, which creates even bigger hype around generative AI.</p>
<p>But whenever there is hype around something, I think we should be more curious about what’s going on behind the scene. Understanding how it works helps us see through the hype and that is why I chose this paper. We can understand how DALL·E’s text-to-image generative model works, what the authors did to make this happen, and how they validated the result.</p>
<p>To understand this paper thoroughly, you need to know other deep learning model frameworks such as transformer, variational autoencoder, and OpenAI’s CLIP (Contrastive Language-Image Pre-training) model. I found these <a href="https://ml.berkeley.edu/blog/posts/vq-vae/">two</a> <a href="https://ml.berkeley.edu/blog/posts/dalle2/">articles</a> extremely useful, written by Charlie Snell at UC Berkeley. In this post, I will talk about a high-level summary and the interesting discussion we had as a group. If you are interested in more detailed summary of the paper itself, I recommend those two posts.</p>
<section id="big-picture" class="level2">
<h2 class="anchored" data-anchor-id="big-picture">Big picture</h2>
<p>The authors created a deep learning model which <strong>generate images from a text input</strong>. For instance, if you type “hands”, the model will generate images of hands. As the title says, this is done in <em>zero-shot</em> way, meaning that it can generate images that it hasn’t seen before. To be clear, the authors of this paper are not the first ones who created a model like this. There have been precedents but the authors say that the generated images from those still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements. So the authors made improvements by adopting these two approaches: using a large set of training data and <strong>building a bigger model</strong>.</p>
<p>Before we look into the results, let’s first talk about the model architecture. Their model consists of two parts: <strong>variational autoencoder (VAE)</strong> and <strong>transformer</strong>.</p>
</section>
<section id="variational-autoencoder-vae" class="level2">
<h2 class="anchored" data-anchor-id="variational-autoencoder-vae">Variational autoencoder (VAE)</h2>
<p>The VAE contributes to the generative nature of the model because VAEs have latent representation in the middle that is a probability distribution. Once trained, we can use this distribution to draw samples from it, providing a <em>generative</em> framework. To train the VAE, the authors assumed uniform prior over the latent space. The model can <strong>learn the actual prior from the transformer later</strong> to generate images that match to text input. To train the VAE, the authors used images with text captions from various sources such as Wikipedia images.</p>
<p>What is interesting about the VAE they used is that it assumes <strong>discrete latent distribution</strong> instead of continuous. This variant of VAE is called <strong>vector-quantized VAE (VQ-VAE)</strong>. The motivation is that images and texts are discrete than continuous. But this assumption comes with a major complication: a discrete space is non-differentiable (i.e., can’t back-propagate). That’s why VQ-VAE has a <strong>codebook</strong>, which is essentially a look-up table where a discrete representation is associated with a codebook vector. To be accurate, this paper used a variant of VQ-VAE called <strong>dVAE</strong> where they made this look-up as a weighted average to further smooth out the space.</p>
<p>This VAE also acts as a dimensionality reduction technique because the discrete latent space the authors used has a resolution of 32x32 instead of 256x256, the resolution of the original training images. This brings compression benefit so that the transformer doesn’t have to memorize extremely long sequence but a sequence of length 1024 (=32*32).</p>
</section>
<section id="transformer" class="level2">
<h2 class="anchored" data-anchor-id="transformer">Transformer</h2>
<p>Once the VAE is learned, <strong>we can abandon the uniform prior assumption and use transformer to learn the actual prior</strong>. Transformers help image generation by pixel-wise prediction in an autoregressive way. For instance, given the sequence of previous pixels, the transformer can predict what the next pixel would look like.</p>
<p>Once the transformer is trained, when we give a text prompt to the model, the transformer makes predictions for the image latents (32x32 space) in an autoregressive way. Once we have all predictions, we use the dVAE codebook to lookup the vectors and generate the image. Since we can sample the sequence in a new way, we can generate multiple images. The authors used a <strong>top k approach</strong> to return the <em>best</em> images by ranking the generated images from a candidate pool based on the <strong>scores from OpenAI’s CLIP model</strong>, which represents how well the images match the caption.</p>
<p>The transformer has <strong>12 billion parameters</strong> and a good chunk of the paper is dedicated to all the tricks the authors came up with to fit the model in GPU.</p>
</section>
<section id="journal-club-discussion" class="level2">
<h2 class="anchored" data-anchor-id="journal-club-discussion">Journal club discussion</h2>
<section id="are-the-results-representative-enough" class="level3">
<h3 class="anchored" data-anchor-id="are-the-results-representative-enough"><font color="blue">Are the results representative enough?</font></h3>
<p>Most of us were somewhat <em>disappointed</em> by the authors’ model validation. Figures 3 and 4 in the paper gave some idea of how realistic the generated images are but we were <strong>not sure whether these were cherry-picked or not</strong> because the spectrum of images the model can generate is so wide. Figure 7 showed results from human evaluators. Most of them said the authors’ model was more realistic than the competitors’. Aside from the ethical issues surrounding hiring mturk workers, we thought <strong>the number of mturk workers was small</strong> (5 people) and the number of images they evaluated was small as well.</p>
</section>
<section id="why-not-investigate-model-failures" class="level3">
<h3 class="anchored" data-anchor-id="why-not-investigate-model-failures"><font color="blue">Why not investigate model failures?</font></h3>
<p>What was more interesting to us was Fig. 8, the CUB dataset which have images of birds. The example images here looked worse than others and the authors speculated that this was due to the detail-oriented text information of images, which might have been lost during the compression in dVAE. This was a plausible explanation but <strong>we wanted to see more in-depth investigation on model failures</strong>. There are numerous examples of terrifyingly looking images of hands generated by DALL·E because apparently it keeps failing at generating images of humans hands with five fingers.</p>
<p>We also discussed the <strong>lack of investigation on model failure from an ethical and responsible AI perspective.</strong> If OpenAI was going to publish a public API for a model like this, which would have varying degrees of socio-technical impact (look at all the issues ChatGPT has been creating these days), it would have been more responsible for them to test the model’s capacity more thoroughly and rigorously before rolling it out.</p>
<p>We found <a href="https://github.com/openai/DALL-E/blob/master/model_card.md">a model card</a> from their repository and it was <strong>disappointingly short and did not address any possible ethical and social ramifications</strong> that would be caused by the model.</p>
</section>
<section id="validity-of-the-scoring-metrics" class="level3">
<h3 class="anchored" data-anchor-id="validity-of-the-scoring-metrics"><font color="blue">Validity of the scoring metrics</font></h3>
<p>The authors used FID and IS scores (generated by the CLIP model) to assess how well the images reflect the text input. The scores were used to rank a pool of candidate images and the model returned top k results. We questioned the validity of the decision behind using these scores because they are model-dependent, which means <strong>they are training-data-dependent</strong>. Plus, there was no mention of (at least) a qualitative comparison between the training datasets of this paper and the CLIP paper. This made us question the reliability of the CLIP model scores. It might have been interesting to see a batch of images that were ranked high (or low) so that we could judge the validity of the scores and understand the model behavior better.</p>
</section>
<section id="qualitative-contribution" class="level3">
<h3 class="anchored" data-anchor-id="qualitative-contribution"><font color="blue">Qualitative contribution</font></h3>
<p>As in other deep learning papers, it was <strong>difficult for us to understand which decisions they made led to their results and advancement</strong>. For instance, they highlighted the larger training dataset and the larger model size. What was the measurable impact of each, and which one was more important? Similar to this, it would have been nice if they had some guidance on model tuning and hyperparameter selection to inform other researchers on model architecture design.</p>
</section>
<section id="reproducibility-and-novelty" class="level3">
<h3 class="anchored" data-anchor-id="reproducibility-and-novelty"><font color="blue">Reproducibility and novelty</font></h3>
<p>To be blunt, <strong>the main highlight of this paper seemed to be the scale</strong>. They were able to use bigger datasets with a bigger model. But let’s be honest, how many academic institutions or companies are able to afford to train a model with 12 billion parameters? Especially without proper model inspection, <strong>how can we understand the model properly when we can’t reproduce it easily?</strong> Although there were certain elements of novelty especially on their tricks of utilizing GPU resources to train the model, if the scale is the main factor of success, <strong>can we really call this as a novel invention?</strong></p>
</section>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final thoughts</h2>
<p>Thanks to the paper, we learned that VQ-VAE and transformer together can generate images from text inputs. However, we questioned the results and model validation especially due to the lack of investigation on model failure. We also thought about ethical aspect of this model being available in public. Just because it belongs to computer vision, which tends to <em>amuse</em> general audience, it does not mean that it is exempt from any social responsibility. And in deep learning with image and speech data, it is often the case that model validation is often looser than tabular data used in industries with higher stakes such as health care, finance, or risk assessment. That said, we would like to learn more about other techniques mentioned in the paper to have a deeper understanding of how they work.</p>


</section>

 ]]></description>
  <category>paper</category>
  <category>computer vision</category>
  <category>GenAI</category>
  <guid>https://hongsupshin.github.io/posts/2022-12-15/</guid>
  <pubDate>Thu, 15 Dec 2022 06:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2022-12-15/Fig 8.png" medium="image" type="image/png" height="124" width="144"/>
</item>
<item>
  <title>“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2022-10-27/</link>
  <description><![CDATA[ 





<p>Many ML articles exist but few talk about how the sausage gets made. They are often based on toy or clean benchmark datasets, which are quite different from what we get in real world. That’s why I enjoy reading survey papers. They interview people in the field like us and try to address common pain points to find a broader picture.</p>
<p>For the past several years, I have been noticing a trend in ML community. Many practitioners tend to ignore data quality but instead put all their efforts into models and algorithms. I find it very troubling because many problems in real-world ML are caused by data-related issues. <a href="https://research.google/pubs/pub49953/">“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI</a> talks about this pattern based on the interviews from dozens of ML practitioners all over the globe. I presented the paper in Austin ML Journal Club in Oct 2022.</p>
<section id="data-cascades" class="level2">
<h2 class="anchored" data-anchor-id="data-cascades">Data cascades</h2>
<p>The paper focuses on “data cascades,” a series of compounding negative events due to data-related issues. The authors say the problems are wildly prevalent (92% reported experience at least one type of data cascades). <font color="blue">During the discussion, we thought about sampling bias because those who are ignorant of these problems may not be able to recognize them.</font> Indeed, they are often opaque because there are no clear indicators and often discovered later. Data cascades often lead to technical debt and harm to beneficiary communities. They can sour relationships between stakeholders and in extremely cases, force ML practitioners discard entire datasets. Figure 1 shows the schematic of the data cascades. <font color="blue">We thought the figure wasn’t particularly informative because too many arrows exist between the points. We hoped that the authors explained why there aren’t arrows between certain points.</font></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2022-10-27/Data cascades in high-stakes AI full.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1. Data cascades in high-stakes AI</figcaption>
</figure>
</div>
</section>
<section id="high-stake-domains" class="level2">
<h2 class="anchored" data-anchor-id="high-stake-domains">High-stake domains</h2>
<p>Data cascades are more critical in high stake domains such as landslide detection, suicide prevention, and cancer detection. There are several reasons:</p>
<ul>
<li>More ML applications are deployed in these domains where more direct humanitarian impact exists.</li>
<li>This impact can be disproportionate towards vulnerable communities.</li>
<li>It is often very challenging to acquire high quality data in these domains.</li>
<li>The problems frequently require more multidisciplinary approach.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2022-10-27/Summary of participant demographics.png" class="img-fluid figure-img"></p>
<figcaption>Table 1. Summary of participant demographics; Domain</figcaption>
</figure>
</div>
<p>The authors find that the problems are due to human factors. Unfortunately solutions have been focusing on other issues such as database, legal, or license. To gather firsthand experience in the field, the authors interviewed 50+ ML practitioners all over the world, ranging from the US to India and African countries, and from founders to developers. Table 1 summarizes various high-stake domains. <font color="blue">It was fascinating for us to learn that ML is used in areas as landslide detection, poaching prevention, or regenerative farming. It would have been more interesting to see how data cascades create specific negative consequences in some domains.</font></p>
</section>
<section id="data-cascade-triggers" class="level2">
<h2 class="anchored" data-anchor-id="data-cascade-triggers">Data cascade triggers</h2>
<p>The authors introduce three triggers that cause data cascades.</p>
<section id="physical-world-brittleness" class="level3">
<h3 class="anchored" data-anchor-id="physical-world-brittleness">Physical world brittleness</h3>
<p>Physical world changes over time and thus often ML systems can’t produce robust results. Data drifts due to hardware (measurements) and environmental changes are commonly mentioned in ML Ops literature. In high-stake domains, they become more pronounced because training data are very limited and policy or regulation changes can impact the ML systems in various ways.</p>
</section>
<section id="inadequate-application-domain-expertise" class="level3">
<h3 class="anchored" data-anchor-id="inadequate-application-domain-expertise">Inadequate application-domain expertise</h3>
<p>Most ML practitioners are not equipped with domain knowledge. <font color="blue">All of us admitted that our academic background does not match to the domain that we work in.</font> Even though close collaboration between domain experts and ML practitioners is always emphasized, in practice the authors find that domain experts are often detached from the larger impact of the applications. The authors explain two specific types of problems:</p>
<p><strong>Subjectivity in ground truth:</strong> Areas such as insurance claim approval or medical imaging for cancer detection involve highly specialized and often subjective decision-making. To build a reliable and robust ML system, it is necessary to standardize the decision-making criteria and find consensus. However, ML practitioners are asked to rush through the development process, and thus do not have time to address it.</p>
<p><strong>Poor domain expertise in finding representative data:</strong> ML practitioners often start building ML applications without involving domain experts much because the practitioners simply believe that data are reliable. However, because they lack domain knowledge, practitioners make incomplete assumptions, which results in disparity between data collection and deployment. This often leads to poor and unreliable model performance.</p>
</section>
<section id="conflicting-reward-system" class="level3">
<h3 class="anchored" data-anchor-id="conflicting-reward-system">Conflicting reward system</h3>
<p>Data collection and any data-related work are often considered non-technical and undervalued. The situation gets worse for frontline workers because they are asked to collect and curate field data on top of their existing responsibilities but they are not well compensated.</p>
</section>
<section id="poor-cross-organizational-documentation" class="level3">
<h3 class="anchored" data-anchor-id="poor-cross-organizational-documentation">Poor cross-organizational documentation</h3>
<p>Metadata about data collection, quality, and curation are also often missing. Some good practices the authors suggest involve keeping good documentation on reproducible assets such as data collection plan, data strategy handbooks, design documents, file conventions, field notes, and so on.</p>
</section>
</section>
<section id="broader-context" class="level2">
<h2 class="anchored" data-anchor-id="broader-context">Broader context</h2>
<p>Data cascades discussion can be extended to bigger problems in ML community.</p>
<section id="incentives-and-currency-in-ai" class="level3">
<h3 class="anchored" data-anchor-id="incentives-and-currency-in-ai">Incentives and currency in AI</h3>
<p>Because of the low incentives, data-related work are not rewarded or even tracked. This makes us difficult to get buy-in from stakeholders. The situation is similar in academia as well. Most practitioners and researchers focus on developing algorithms but they rarely mention or work on data. <font color="blue">The title <em>“Everyone wants to do the model work, not the data work”</em> was a verbatim from an interviewee. Unfortunately, we were all able to relate to this quote.</font></p>
</section>
<section id="data-education" class="level3">
<h3 class="anchored" data-anchor-id="data-education">Data education</h3>
<p>Most ML or data science curricula lack any mention of data quality or ethics. They use toy datasets or very clean benchmark datasets. <font color="blue">As experience ML practitioners, we wholeheartedly agreed with this finding. We lamented that these courses do not prepare students with practical knowledge because one never works with clean datasets in real world. Some of us have experienced this pattern firsthand because we have been interviewing candidates for an ML practitioner position and found that the candidates who only worked with clean datasets (or primarily worked on algorithms) often lack basic practical ML knowledge.</font></p>
</section>
<section id="data-bootstrapping" class="level3">
<h3 class="anchored" data-anchor-id="data-bootstrapping">Data bootstrapping</h3>
<p>Data bootstrapping describes ML practitioners’ use of other data sources such as established data services or existing datasets to create their own dataset. <font color="blue">For most of us, it was surprising to learn that many ML practitioners in high-stake domains in countries from Global South had to collect data from scratch. We agreed that challenges in data collection and lack of access to quality data would create inequality between countries.</font></p>
</section>
</section>
<section id="how-to-address-data-cascades" class="level2">
<h2 class="anchored" data-anchor-id="how-to-address-data-cascades">How to address data cascades</h2>
<p>The authors introduce several ways of addressing the problem of data cascades. They introduce the concept of <em>“data excellence”</em>, an effort to <strong>“focus on the practices, politics, and values of humans of the data pipeline to improve the quality and sanctity of data, through the use of processes, standards, infrastructure and incentives”</strong>.</p>
<section id="from-goodness-of-fit-to-goodness-of-data" class="level3">
<h3 class="anchored" data-anchor-id="from-goodness-of-fit-to-goodness-of-data">From goodness-of-fit to goodness-of-data</h3>
<p>The first is to use the right metric to evaluate data quality. Many ML practitioners use model performance metrics such as accuracy and RMSE to evaluate data quality. <font color="blue">Some of us had a similar experience. We had to argue that model metrics shouldn’t be used to make a decision on data-pipeline and data-quality features. We hope that the authors can introduce specific examples of goodness-of-data metrics in the future.</font></p>
</section>
<section id="incentives-for-data-excellence" class="level3">
<h3 class="anchored" data-anchor-id="incentives-for-data-excellence">Incentives for data excellence</h3>
<p>There are several ways to address the low incentives of data work. First, journals and conferences should require dataset documentation, provenance, and ethics as mandatory disclosure. Second, organizations should reward data-related work similar to how good software engineering is rewarded. Finally, partnership between stakeholders can nurture data excellence by sharing the reward on data-related work such as data collection, anomaly identification, and model verification.</p>
</section>
<section id="education-and-visibility" class="level3">
<h3 class="anchored" data-anchor-id="education-and-visibility">Education and visibility</h3>
<p>The authors argue for real-world data literacy in AI education, which includes training on data collection, infrastructure building, data documentation, data sense-making, and data ethics and responsible AI in general. Increasing the data visibility in ML lifecycle is important as well; implementing good monitoring system is one of the most important ML Ops practices anyway.</p>
</section>
</section>
<section id="journal-club-discussion" class="level2">
<h2 class="anchored" data-anchor-id="journal-club-discussion">Journal club discussion</h2>
<p>Some of us found this paper vindicating because we have been advocating for data quality at work and had to fight for the attention it deserves. The paper helped us share our own practices at work that address data-related issues especially data collection, curation, and post-deployment data problems. Even though we generally agreed with authors’ suggestions, some of us wanted something more specific, like a case study. Overall, we found the paper interesting and insightful. We thought it would be beneficial to read the paper with our colleagues at work to start a discussion for data excellence.</p>


</section>

 ]]></description>
  <category>paper</category>
  <category>governance</category>
  <category>responsible AI</category>
  <guid>https://hongsupshin.github.io/posts/2022-10-27/</guid>
  <pubDate>Thu, 27 Oct 2022 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2022-10-27/Data cascades in high-stakes AI.png" medium="image" type="image/png" height="70" width="144"/>
</item>
<item>
  <title>Tech volunteering tips for nonprofits</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2021-05-25-ds-volunteering-part2/</link>
  <description><![CDATA[ 





<p>Throughout my career, I have met many tech workers interested in using their technical skills in more meaningful ways. When I started my industry career, I had similar thoughts and luckily I have been able to volunteer at various non-profits for several occasions. Some may say volunteering will require much less effort than paid labor because the work is free but because of that exact reason, more caution is required. Based on my experience, I would like to share some tips especially regarding using ML and data science skills for tech volunteering.</p>
<p>I gave a talk at <a href="https://www.meetup.com/austinpython/">Austin Python Meetup</a> in Apr, 2021 About this. If you’re interested, check out the video below.</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/gfqKaplRTsk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<section id="general-advice" class="level2">
<h2 class="anchored" data-anchor-id="general-advice">General advice</h2>
<p>First, before delving into specific tech-skill tips, let’s talk about more general aspects of tech volunteering.</p>
<section id="local-than-national-or-international" class="level3">
<h3 class="anchored" data-anchor-id="local-than-national-or-international">Local than national or international</h3>
<p>The first thing you’ll have to do is to choose a nonprofit. If you’ve been interested in data science for social good, you are already familiar with an organization such as <a href="https://www.datakind.org/">DataKind</a>. I’ve worked with DataKind a couple of times for small projects and normally you go through a detailed application process to become a volunteer. Since it’s a popular international organization, the application process is competitive and it’s possible that you will get rejected, which also happened to me several time.</p>
<p>Famous organizations are all good but I’d like to suggest you look around and go for <strong>local nonprofits</strong>. They often lack resources to afford tech workers which means that you can easily work with them and your work will make a bigger impact. I’ve joined <a href="https://texasjusticeinitiative.org/">Texas Justice Initiative (TJI)</a> through a local nonprofit <a href="https://www.open-austin.org/">Open Austin</a>, a Code for America Brigade which advocates for open government and civic technology. I went to their meetup a couple of times and joined their Slack channel, where I met our TJI director, Eva.</p>
</section>
<section id="long-term-than-short-term" class="level3">
<h3 class="anchored" data-anchor-id="long-term-than-short-term">Long-term than short-term</h3>
<p>Depending on the situation, you may work on one-off type projects or more long-term ones. After working with TJI for over a year, I am learning that I get to work on <strong>more diverse and in-depth projects</strong> since I’ve been here more than a year. For instance, now that I have better understanding of backend, I am paying more attention to automating many backend processes. Plus, with newly accumulated data, I also get to update data journalism reports and launch new investigation.</p>
</section>
<section id="do-your-homework" class="level3">
<h3 class="anchored" data-anchor-id="do-your-homework">Do your homework</h3>
<p>If you’re a first-timer and your nonprofit-of-interest doesn’t have any existing volunteers, you might feel intimidated. Then find an organization that already has tech volunteers. When I was reading about TJI before I joined, I found that there are a dozen tech volunteers already, which gave me a sense of reassurance. Most nonprofits disclose <strong>funding sources and board members</strong> information on their websites. This will give you a better idea on whether your values and what they represent are aligned.</p>
</section>
<section id="reasonable-expectations" class="level3">
<h3 class="anchored" data-anchor-id="reasonable-expectations">Reasonable expectations</h3>
<p>Nonprofits can be like any other workplaces; there will be some you enjoy working with and some you don’t, and there will be times when things are going really well or falling apart. Thus, having realistic and healthy expectations is important.</p>
</section>
</section>
<section id="ml-and-statistical-inference" class="level2">
<h2 class="anchored" data-anchor-id="ml-and-statistical-inference">1. ML and statistical inference</h2>
<p>Now let’s talk about specific tech-skill opportunities. Let’s start with ML and statistical inference first.</p>
<section id="predictive-modeling" class="level3">
<h3 class="anchored" data-anchor-id="predictive-modeling">Predictive modeling</h3>
<p>For my police shooting report, <strong>I purposefully did not use any ML models</strong> because I am aware of how ML tools can generate unintended harm easily when used prematurely (or at all). Whenever I talk to ML practitioners, they often love to devour any datasets and create predictive models. This is extremely dangerous.</p>
<p>Let’s assume that for whatever reason, we decided to build a predictive model by using the police shooting data. One can potentially use the <code>severity</code> column (whether a person was killed or injured during an incident) as a target and use the rest of the data as predictor variables. Given that most of our data has demographic and location information, <strong>this can easily become a bias-reproducing machine.</strong></p>
<p>Besides, if these models are deployed in public, it’s possible that <strong>the models can be used by bad actors to exploit community members who are already suffering</strong> from these tragic incidents. For instance, insurance companies might be able to use a model like this and predict how likely people of certain demographics are more likely to be killed by police. Then they can increase health insurance premium for these people.</p>
<p>Before even starting a project, I strongly recommend <strong>considering and evaluating the risk and ethical concerns first</strong>. If the risk is too high, then you shouldn’t build a model. Note that this thoughtful examination <strong>requires a diverse set of opinions</strong>. Thus, please open the discussion to other members in the organization (and even further) and try to thoroughly examine the risk.</p>
</section>
<section id="on-significance-testing" class="level3">
<h3 class="anchored" data-anchor-id="on-significance-testing">On significance testing</h3>
<p>Some data scientists like computing <em>p</em>-values from their analyses but first and foremost, <strong><em>p</em>-values have many issues</strong> (see <a href="https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf">the official statement from American Statistical Association</a> cautioning the use of <em>p</em>-values). Plus, given that they are usually not the domain experts, it is likely that the hypotheses are not well thought out. Moreover, once you make value statements based on thresholding by using <em>p</em>-values, it is very likely that <strong>you will contribute to propagation of misinformation</strong> because these statements (e.g., “A is significantly different from baseline”) can be easily cherry-picked and propagated.</p>
</section>
<section id="then-what-can-i-do" class="level3">
<h3 class="anchored" data-anchor-id="then-what-can-i-do">Then what can I do?</h3>
<p>I am not telling you to drop any statistical inference or ML completely. Rather, I am asking you to <strong>recognize the responsibility that comes with using ML techniques</strong>. In fact, there are many research papers that use ML in clever ways. For instance, in Gonen and Goldberg’s 2019 paper, <a href="https://arxiv.org/abs/1903.03862">Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a>, the authors used predictive modeling to show that (gender) de-biased word embeddings still contain information about gender, which proves that the de-biasing technique doesn’t work. In short, we need to <strong>be more thoughtful and creative</strong> when it comes to ML.</p>
</section>
</section>
<section id="data-governance" class="level2">
<h2 class="anchored" data-anchor-id="data-governance">2. Data governance</h2>
<p>If ML requires more cautious approach, an easier project you can tackle as a data person is to help a nonprofit establish a good data governance practice. Data governance is a practice of <strong>documenting what the data contains and how it should be used</strong>. It’s a great opportunity to learn about data and help the nonprofit utilize the datasets responsibly.</p>
<p>There is an excellent paper, <strong><a href="https://www.fatml.org/media/documents/datasheets_for_datasets.pdf">Datasheets for Datasets (Gebru et al., 2018)</a></strong> that can guide you through this process. The paper has an exhaustive list of questions that you can answer to understand a dataset and its potential problems and impact. At TJI, I’ve gone through this process and published <a href="https://github.com/texas-justice-initiative/data-processing/blob/hs-datasheet/datasheet/datasheet_DIC_composition.md">a Datasheet on TJI’s custodial death dataset</a> focusing on the dataset’s composition. This was a great exercise for all of us because we were able to discuss some potential caveats such as sampling bias of our dataset. Another template you can follow is <a href="https://gdpr.eu/data-protection-impact-assessment-template/">Data Protection Impact Assessment</a> from European Union’s General Data Protection Regulation (GDPR). This also asks you to answer questions to understand the impact and ethical concerns that come with the data.</p>
<p>Finally, once you’ve created data governance documentation, make sure you provide it to users when they consume the dataset so that they are also aware of the information. This creates a chain of responsibility.</p>
</section>
<section id="data-journalism" class="level2">
<h2 class="anchored" data-anchor-id="data-journalism">3. Data journalism</h2>
<section id="interactive-dashboards-need-a-context" class="level3">
<h3 class="anchored" data-anchor-id="interactive-dashboards-need-a-context">Interactive dashboards need a context</h3>
<p>Interactive dashboards are useful in that audience can play with the data directly. However, it’s likely that <strong>you can’t have comprehensive knowledge on what users might see</strong> when they start slicing the data in every possible way. Plus, you still need <strong>context</strong> to provide a narrative. Personally, I <strong>prefer data journalism reports with static images</strong> because I can control what audience can see and I can minimize the chance of them misunderstanding the data.</p>
</section>
<section id="find-a-reference-point" class="level3">
<h3 class="anchored" data-anchor-id="find-a-reference-point">Find a reference point</h3>
<p>Providing a context is extremely important. If I don’t know where something stands compared with others, how would I know whether it is better or worse? Luckily, this comparison can be easily done by <strong>comparing your data with other reference datasets</strong>. In Part 1, I’ve used the US census data and public health data for this. You can see this approach all the time in data journalism reports from major news organizations such as <a href="https://www.nytimes.com/interactive/2020/04/23/upshot/five-ways-to-monitor-coronavirus-outbreak-us.html">New York Times</a>. Comparing your data with a reference data helps you ask more interesting questions to understand the difference.</p>
</section>
<section id="get-a-domain-expert-review" class="level3">
<h3 class="anchored" data-anchor-id="get-a-domain-expert-review">Get a domain expert review</h3>
<p>It’s likely that tech workers are not the domain experts. In this case, make sure your work is seen by them before you publish. You can start by sharing your work with other members in the nonprofit. You can also <strong>reach out to academics</strong> who are most likely very interested in this type of work. For my report, Eva helped me reach out to Howard Henderson of Texas Southern University’s Barbara Jordan-Mickey Leland School of Public Affairs and Center for Justice Research, and Kevin Buckler of University of Houston-Downtown’s Department of Criminal Justice for a review.</p>
</section>
<section id="leave-out-any-questionable-information" class="level3">
<h3 class="anchored" data-anchor-id="leave-out-any-questionable-information">Leave out any questionable information</h3>
<p>You might want to use everything that’s in a dataset to write a report. This is not necessarily the best idea, unfortunately. In our police shooting dataset, there is a column called <code>deadly_weapon</code> which shows whether the person shot by police possessed a <em>deadly</em> weapon. It turns out that <strong>this information is highly contentious</strong> because whether an object is perceived as <em>deadly</em> varies widely. Based on studies and news reports, a BB gun, a butter knife, or a candlestick can be perceived as a deadly weapon. After deliberation, we’ve decided not to use this information because we thought this categorization misrepresents true information. We plan to revisit this after collecting information about actual objects used in the incidents.</p>
</section>
<section id="research-opportunities" class="level3">
<h3 class="anchored" data-anchor-id="research-opportunities">Research opportunities</h3>
<p><strong>Datasets gathered by local nonprofits are so specific and unique</strong> that they provide interesting research opportunities, which becomes great material for a data journalism report. Currently at TJI, several volunteers and I have been working on identifying systematic pattern in interaction between officers and civilians in police shooting incidents. <a href="https://www.linkedin.com/in/jiletta-kubena-phd-21849812a/">Jiletta Kubena</a>, a criminologist and a TJI volunteer, has been studying survey results on various populations, such as people in custody, of those who’ve been impacted by COVID-19.</p>
</section>
<section id="tech-setup" class="level3">
<h3 class="anchored" data-anchor-id="tech-setup">Tech setup</h3>
<p>You will be creating same types of plots over and over with small changes depending on how your investigation goes. Thus, it’s better to <strong>spend time in the beginning to create modules for plotting</strong>. At the same time, ask if your nonprofit already has a specific visual style guide that you can follow. This helps visual consistency.</p>
</section>
</section>
<section id="sustainable-workflow" class="level2">
<h2 class="anchored" data-anchor-id="sustainable-workflow">4. Sustainable workflow</h2>
<p>What kind of tech tools and workflow gets to be implemented in a nonprofit heavily depends on their volunteers. However, the <strong>volunteer-dependent workflow is quite fragile</strong> because volunteers in general have weaker commitment because they are not always readily available. Thus, it’s quite important to help a nonprofit build a sustainable workflow.</p>
<p><strong>Implementing CI/CD flow</strong> is a must and you can even reuse your notebooks to create analytics reports with updated data by using a package such as <a href="https://papermill.readthedocs.io/en/latest/">Papermill</a>. If you have a heterogenous dataset, you can utilize scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"><code>ColumnTransformer</code></a> to build a consistent and robust data preprocessing pipeline. Finally, you can also set up a data validation check by using a tool like <a href="https://pandera.readthedocs.io/en/stable/">Pandera</a>. This will help you monitor and flag any unexpected data drift.</p>
<p>A sustainable workflow is also more than just the tech side. Recently, at TJI, we’ve implemented a <strong>formal review process</strong> to improve the quality of our work. We used to review manuscripts on Google Docs but now we review them on GitHub as a <strong>pull request</strong>. <a href="https://www.linkedin.com/in/nickholden/">Nick Holden</a>, a software engineer and a TJI volunteer, connected our content management system (CMS) to this process so that people who are not familiar with GitHub can easily create a manuscript. Writing a plain text document on the CMS automatically creates a pull request and other members at TJI can review and exchange feedback transparently.</p>
</section>
<section id="community-building" class="level2">
<h2 class="anchored" data-anchor-id="community-building">5. Community building</h2>
<p>As a volunteer, you can always work by yourself on a one-off project. Even if that’s the case, having a sense of community makes your work more enjoyable and prevents fast volunteer turnover. In this sense, community building is quite important.</p>
<p>Community building can be socializing but it’s also about building robust infrastructure because it reduces unnecessary stress. Given that <strong>volunteer-dependent workflow is fragile, and robustness, accountability, and transparency matter even more.</strong> As a tech worker, you can help nonprofits adopt good practices from tech workflow such as automation, code review, software engineering practices, and so on. But when bringing a new tech tool, please try to <strong>use open source software</strong> as much as possible. I’ve seen a case where a tech worker brought their company’s product and left, which made the nonprofit’s entire backend hugely dependent on a niche commercial product. Sometimes nonprofits are approached by private partners. With your corporate experience, you can help the nonprofit protect their asset too.</p>
<p>Implementing <strong>a good practice for collaboration</strong> is equally important. You can start by encouraging other volunteers to showcase and highlight their work. I’ve suggested a dedicated <strong>blog where all volunteers can write about their work in their own words</strong>. This accelerates knowledge transfer within the organization and helps other nonprofits too.</p>
<p>Finally, I want to emphasize that you should <strong>watch out for volunteer burnout</strong>. It happened to me multiple times unfortunately. I got super excited and passionate about a subject matter and I ended up with exhausting all my energy in a short period of time. So please pace yourself and take breaks if needed. But when you do so, instead of ghosting please tell others so that they have reasonable expectations while you’re gone.</p>
</section>
<section id="my-personal-experience" class="level2">
<h2 class="anchored" data-anchor-id="my-personal-experience">My personal experience</h2>
<p>Some may think volunteering would be using your existing skills without learning. That was never the case for me. It’s been amazing to have <strong>opportunities to work with different types of data</strong> that I don’t get to see at work. I’ve also got to work on various projects from exercising data assessment practice and writing data journalism reports, to creating custom data visualization modules and transferring ML Ops practices to the existing workflow. I’m looking forward to exploring research opportunities with causal modeling and explainable AI with our dataset in the future.</p>
<p>But more importantly, I’ve been lucky to work with our director Eva and all the volunteers who have been kind, cordial, and open to new ideas. It was also fantastic to <strong>work with people from different disciplines</strong> and to learn from these domain experts. This helped me expand my network and meet other nonprofit organizers, academics, and tech workers. Finally, I’ve <strong>learned a lot about local social issues surrounding my neighborhood and my city</strong>, which have significant impact on our community members. I am looking forward to many other interesting opportunities in TJI.</p>
<p>I hope these tips help you start exploring volunteering opportunities. Finally, you are more helpful than you think! So don’t worry about whether you have the right skill sets or not. They will appreciate your knowledge and there will be many things you can contribute to. So if you find any interesting organizations, don’t be afraid to reach out. You will help other people and learn a lot in the process. Good luck!</p>


</section>

 ]]></description>
  <category>volunteering</category>
  <category>journalism</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2021-05-25-ds-volunteering-part2/</guid>
  <pubDate>Tue, 25 May 2021 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2021-05-25-ds-volunteering-part2/datakind.png" medium="image" type="image/png" height="31" width="144"/>
</item>
<item>
  <title>Police shooting in Texas 2016-2019</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html</link>
  <description><![CDATA[ 





<blockquote class="blockquote">
<p>I have met several tech workers who were interested in using their technical skills in more meaningful projects particularly for social good. As a tech worker myself, I’ve been volunteering at several organizations about two years now. Among those, I have been volunteering at Texas Justice Initiative (TJI). In this post, I’d like to share some of my volunteering at TJI, particularly on police shooting in Texas.</p>
</blockquote>
<div id="9f1e85ee-5200-4737-b2bc-b615a59db45c" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2">pd.set_option(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'precision'</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sys</span>
<span id="cb1-7"></span>
<span id="cb1-8">plt.style.use(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ggplot'</span>)</span>
<span id="cb1-9"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>config InlineBackend.figure_format <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'retina'</span></span>
<span id="cb1-10"></span>
<span id="cb1-11"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> IPython.core.pylabtools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> figsize</span>
<span id="cb1-12"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> IPython.display <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> display</span>
<span id="cb1-13">figsize(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb1-14"></span>
<span id="cb1-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># custom modules</span></span>
<span id="cb1-16">sys.path.append(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'../src/'</span>)</span>
<span id="cb1-17"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plot</span>
<span id="cb1-18"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> preprocess</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>load_ext watermark</span>
<span id="cb1-21"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>load_ext autoreload</span>
<span id="cb1-22"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>autoreload <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-23"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-24"></span>
<span id="cb1-25"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>watermark <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>d <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>u <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>v <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>g <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>r <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>iv <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>a <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hongsup Shin"</span></span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Author: Hongsup Shin

Last updated: 2021-05-24 18:31:44

Python implementation: CPython
Python version       : 3.7.10
IPython version      : 7.20.0

Git hash: d7b4e435d90633a7d179a01e136b02487d2c5f0b

Git repo: https://github.com/hongsups/blog.git

Git branch: TJI_volunteering

numpy     : 1.20.2
pandas    : 1.2.3
seaborn   : 0.11.1
matplotlib: 3.4.1
sys       : 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 15:59:12) 
[Clang 11.0.1 ]
</code></pre>
</div>
</div>
<div id="666624f9-5b53-43b3-b01d-7e0d4b0107f3" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"></span>
<span id="cb3-2"></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># styling setup</span></span>
<span id="cb3-4">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font.family'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sans-serif'</span></span>
<span id="cb3-5">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font.size'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span></span>
<span id="cb3-6"></span>
<span id="cb3-7">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'axes.labelsize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span></span>
<span id="cb3-8">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'axes.titlesize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span></span>
<span id="cb3-9"></span>
<span id="cb3-10">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'figure.dpi'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">72</span></span>
<span id="cb3-11">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'figure.titlesize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span></span>
<span id="cb3-12"></span>
<span id="cb3-13">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'legend.frameon'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb3-14">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'legend.edgecolor'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span></span>
<span id="cb3-15"></span>
<span id="cb3-16">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'xtick.labelsize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb3-17">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ytick.labelsize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb3-18"></span>
<span id="cb3-19">cols_race <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#CE2827'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#3167AE'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#4C5151'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'#B8BAB9'</span>]</span></code></pre></div>
</details>
</div>
<p>As a tech worker, I’ve been volunteering at several organizations about two years now. Among those, I have been volunteering at <a href="https://texasjusticeinitiative.org/">Texas Justice Initiative (TJI)</a> consistently over a year where I published <a href="https://texasjusticeinitiative.org/static/TJI_OISReport_2020.pdf">a data journalism report on police shooting in Texas</a> with our TJI director, <a href="https://www.linkedin.com/in/eva-ruth-moravec-m-a-1aa9332/">Eva Ruth Moravec</a>. <a href="https://texasjusticeinitiative.org/">Texas Justice Iniative (TJI)</a> is a criminal justice nonprofit in Austin, TX. It was founded in 2016 by Eva Ruth Moravec, a journalist, and Amanda Woog, a researcher. The main goal of TJI is to <strong>create data portal for criminal justice in Texas</strong>. TJI mostly relies on tech volunteers. We currently have <a href="https://texasjusticeinitiative.org/about">11 active volunteers</a>, including myself.</p>
<p>In this post, I’d like to share some of my volunteering at TJI, particularly on police shooting in Texas. In the follow-up post, I will share some tips on tech volunteering at nonprofits, especially focusing on how to use machine learning and data science knowledge.</p>
<section id="police-shooting-in-the-us-is-a-globally-recognized-public-health-issue" class="level2">
<h2 class="anchored" data-anchor-id="police-shooting-in-the-us-is-a-globally-recognized-public-health-issue">Police shooting in the US is a globally recognized public health issue</h2>
<p>It’s 2021 now and almost everyone in the US is aware that there is an epidemic of police brutality all over the country. Academic researchers now agree that <a href="https://pubmed.ncbi.nlm.nih.gov/31383756/">police contact is a significant contributing factor to health inequality and particuarly to early mortality for people of color</a>. The police brutality in the US is also internationally recognizied such as in <a href="https://www.justsecurity.org/wp-content/uploads/2014/11/UN-Committee-Against-Torture-Concluding-Observations-United-States.pdf">the 2014 report by the UN Committee Against Torture</a>: &gt; The Committee is concerned about numerous reports of police brutality and excessive use of force by law enforcement officials, in particular against persons belonging to certain racial and ethnic groups, immigrants and LGBTI individuals, racial profiling by police and immigration offices and growing militarization of policing activities.</p>
<p>If the issue of police brutality is this severe, one would expect that there might be a national database of police shooting incidents. <strong>Unfortunately, the US goverment does not maintain a national database of police shooting incidents.</strong> Rather, most efforts to collect police shooting incidents were done by journalists. D. Brian Burghart’s <a href="https://fatalencounters.org/">Fatal Encounter</a> and Washington Post’s <a href="https://www.washingtonpost.com/graphics/investigations/police-shootings-database/">Fatal Force</a> are famous examples. Some local goverments have pulic data portals for police shooting but they are mostly at a city level and it’s rare to find a state-level database.</p>
</section>
<section id="in-texas-law-enforcement-agencies-must-report-officer-involved-shooting-ois-incidents" class="level2">
<h2 class="anchored" data-anchor-id="in-texas-law-enforcement-agencies-must-report-officer-involved-shooting-ois-incidents">In Texas, law enforcement agencies must report <em>officer-involved shooting</em> (OIS) incidents</h2>
<p>The state of Texas collects officer-involved shooting (OIS) data from all law enforcement agencies in the state. The term <strong>officer-involved shooting</strong> refers to two different types of incidents: 1. <strong>Shooting <em>by</em> officers</strong>. In this case, people harmed during a shooting incident are <strong>civilians</strong>. 2. <strong>Shooting <em>of</em> officers</strong>. Here, those harmed during an incident are <strong>police officers.</strong></p>
<p>This was possible thanks to the legistation in 2015 (HB 1036) which requires all officer-involved shootings to be reported to the <strong>Office of Attorney General (OAG) of Texas</strong>. In 2017, another bill was passed, which now requires OAG to investigate missing reports and to fine law enforcement agencies if the reports were delayed. This data is is available in public in <a href="https://oagtx.force.com/oisreports/apex/OISReportsPage">the OAG website</a>. Every report in the data is saved as a PDF file, which makes it challenging to digest.</p>
</section>
<section id="data-collection-takes-a-huge-amount-of-effort" class="level2">
<h2 class="anchored" data-anchor-id="data-collection-takes-a-huge-amount-of-effort">Data collection takes a huge amount of effort</h2>
<p>Many ML practicioners and data scientists are usually detached from data collection process, which makes it easy for us to overlook the importance of the effort and cost of data collection. Thus, in this section, I’d like to describe the data collection process for the OIS data briefly to show you that <strong>data doesn’t just appear for free</strong>.</p>
<p>First and foremost, for a government dataset like this, legislations and polices should be established as a consorted effort to collect data formally. Now every law enforcement agency has to file a one-page report (see the example <a href="https://www.austintexas.gov/sites/default/files/files/Police/Olson_Dr._-_Peace_Officer_Involved_Injuries.pdf">here</a>) whenever they identify an officer-involved shooting incident in their jurisdiction. This OIS report contains many pieces of valuable information. To list a few: - Date and location of the incident (location as detailed as street address) - Demographic information (gender, race, and age) of the person who was shot by police and the police officer who shot the person - Severity of the incident (binary; injury or death) and whether the person shot possessed a deadly weapon - Whether multiple officers were involved, whether the officer was on duty, and so on.</p>
<p>To acquire this data, every month, TJI submits an open records request via OAG’s online portal. Luckily, we get a tabular data (not the PDF files) in a csv file format and our director Eva manually inspects and fixes errors. In this process, we contact the agencies and ask for clarification for errors. Finally, the data is added to the existing data and the data on our website is updated. You can download the <a href="https://texasjusticeinitiative.org/datasets/civilians-shot">OIS data on our website</a>.</p>
</section>
<section id="the-oags-annual-report-lacks-insight.-can-we-do-better" class="level2">
<h2 class="anchored" data-anchor-id="the-oags-annual-report-lacks-insight.-can-we-do-better">The OAG’s annual report lacks insight. Can we do better?</h2>
<p>Since the legislations were passed, the OAG publishes an annual report to provide a summary of all OIS incidents in Texas in a given year. The two-page <a href="https://www.texasattorneygeneral.gov/sites/default/files/files/divisions/criminal-justice/2020-Annual-Report.pdf">2020 report</a> has only three paragraphs where they discuss the data:</p>
<blockquote class="blockquote">
<p>From January 1, 2020 until December 31, 2020, there were one hundred ninety-four (194) separate incidents statewide involving peace officer shootings with a firearm that caused injury or death. Those incidents resulted in one hundred ten (110) deaths and eighty-four (84) injuries to individuals. Additionally, twenty-nine (29) peace officers were injured and six (6) were killed.</p>
<p>Of the individuals (non-peace officers) who were either injured or killed in these incidents, sixty-seven (67) were Hispanic, sixty-three (63) were Caucasian, sixty-one (61) were African-American, one (1) was Asian or Pacific Islander, one (1) was not available and one (1) was of another nationality or race. One hundred seventy-six (176) of these incidents involved individuals who were reported to be carrying a deadly weapon; eighteen (18) did not.</p>
<p>The reason for the officers’ involvement are broken down as follows: one hundredfour (104) Emergency Calls or Requests for Assistance; twenty-five (25) involving Execution of a Warrant; twenty (20) Hostage, Barricade and Other Emergency Situations; thirty-eight (38) Traffic Stops; and forty-one (41) Other Uncategorized Situations.</p>
</blockquote>
<p>This summary from the OAG report has several issues: 1. The data is <strong>aggregated at the state level</strong>. Texas has over 250 counties and that granular information is nowhere to be found. 2. There are <strong>no intersectional analyses</strong>. There are counts based on race and the reasons for officer involvement but that’s all. 3. There are <strong>no multi-year analyses</strong>. They have been collecting the data since 2015 but year-to-year comparison doesn’t exist. 4. There is <strong>no disclosure on data preprocessing</strong>. When analyzing a dataset, one has to make numerous decisions how they handle the data such as data exclusion, imputation, discretization, aggregation, and so on. For public data, preprocessing information is a key to transparancey and accountability. 5. There is <strong>missing information</strong>. The OIS report has information such as age demographics, report dates, on-duty-ness of officers, etc. However, none of that information is addressed in the report.</p>
<p>To address these points, I’ve decided to analyze the Texas police shooting data in the following way: 1. <strong>Comparison with reference datasets</strong>. If we want to make a value assessment on the OIS dataset, we need a target that we can compare with. To do this, I used US Census data and mortality data from Texas Department of Health Services. 2. <strong>County-level analyses</strong>. Texas nas numerous counties and some are quite different from others. This aspect should be reflected. 3. <strong>Various intersectional analyses</strong>. We can find many different ways to create interesting intersections of the data. Given the serious racial bias in police brutality, I made sure we look at intersections that contain race demographics. 4. <strong>Multi-year analyses</strong>. This way, we can idenfity whether there is any temporal trend in the analyses. 5. <strong>Reproducible and publically available data preprocessing information</strong>. I published all the code, visualizations, raw &amp; intermediate datasets, and Jupyter notebooks on <a href="https://github.com/texas-justice-initiative/officer_involved_shooting">our GitHub repository</a>. 6. <strong>Analyses on age demogrphics</strong>. This was never addressed in the OAG’s annual reports, and I made sure we utilize this information.</p>
<p>In this post, I am going to focus on several major findings from my <a href="https://texasjusticeinitiative.org/static/TJI_OISReport_2020.pdf">original report</a> (but updated with 2020 information) focusing on the race demogrphics.</p>
</section>
<section id="overview-of-the-ois-data" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-the-ois-data">Overview of the OIS data</h2>
<div id="98fef74c-180a-4187-ac8c-bce1f6122a1f" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"></span>
<span id="cb4-2">df_civilian <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_pickle(</span>
<span id="cb4-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://github.com/texas-justice-initiative/officer_involved_shooting/blob/master/Data/Preprocessed/civilian_preprocessed_20162020.pkl?raw=true'</span></span>
<span id="cb4-4">)</span>
<span id="cb4-5">columns_to_show <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'date_incident'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'incident_county'</span>, </span>
<span id="cb4-6">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_gender'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_age'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_race'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_died'</span>,</span>
<span id="cb4-7">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Traffic Stop'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Emergency/Request for Assistance'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Execution of a Warrant'</span>,</span>
<span id="cb4-8">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hostage/Barricade/Other Emergency'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Other'</span></span>
<span id="cb4-9">                  ]</span>
<span id="cb4-10">display(df_civilian[columns_to_show].tail().reset_index(drop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">date_incident</th>
<th data-quarto-table-cell-role="th">incident_county</th>
<th data-quarto-table-cell-role="th">civilian_gender</th>
<th data-quarto-table-cell-role="th">civilian_age</th>
<th data-quarto-table-cell-role="th">civilian_race</th>
<th data-quarto-table-cell-role="th">civilian_died</th>
<th data-quarto-table-cell-role="th">Traffic Stop</th>
<th data-quarto-table-cell-role="th">Emergency/Request for Assistance</th>
<th data-quarto-table-cell-role="th">Execution of a Warrant</th>
<th data-quarto-table-cell-role="th">Hostage/Barricade/Other Emergency</th>
<th data-quarto-table-cell-role="th">Other</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>2020-12-14</td>
<td>HARRIS</td>
<td>MALE</td>
<td>34.0</td>
<td>WHITE</td>
<td>DEATH</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>2020-12-14</td>
<td>HARRIS</td>
<td>MALE</td>
<td>30.0</td>
<td>BLACK</td>
<td>INJURY</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>2020-12-15</td>
<td>BRAZORIA</td>
<td>MALE</td>
<td>80.0</td>
<td>WHITE</td>
<td>DEATH</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2020-12-25</td>
<td>HARRIS</td>
<td>MALE</td>
<td>31.0</td>
<td>BLACK</td>
<td>DEATH</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2020-12-31</td>
<td>JEFFERSON</td>
<td>MALE</td>
<td>39.0</td>
<td>WHITE</td>
<td>DEATH</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This is a snapshot of a <em>preprocessed</em> OIS dataset (only a small portion of all columns shown). In general, the data has information about the time and location of the incident, demographics, and incident causes (the last 5 columns that are one-hot encoded). The <a href="https://texasjusticeinitiative.org/static/TJI_OISReport_2020.pdf">original report</a> has a number of intersectional visualizations but here, I would like to share one example to give you an idea.</p>
<p>This heatmap below shows the number of civilians shot by police by race and age groups at the state level. The numbers in the parentheses represent the total in the corresponding category and the gray cells represent 0s.</p>
<div id="16579e37-a802-4d94-b44b-36e58592c367" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"></span>
<span id="cb5-2">plot.plot_heatmap_age_race_year(</span>
<span id="cb5-3">    df_civilian, </span>
<span id="cb5-4">    figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.5</span>), </span>
<span id="cb5-5">    cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Blues'</span>,</span>
<span id="cb5-6">    title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Number of Civilians Shot in OIS Incidents (All Counties) by Age Groups'</span></span>
<span id="cb5-7">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-5-output-1.png" width="997" height="266" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This simple intersection of race and age already reveals something important that was hiddne in the OAG’s reports; <strong>almost every year, young children were shot by police.</strong></p>
</section>
<section id="racial-disparity-in-police-shooting-in-texas" class="level2">
<h2 class="anchored" data-anchor-id="racial-disparity-in-police-shooting-in-texas">Racial disparity in police shooting in Texas</h2>
<div id="1bff59d0-d17b-4b19-8e3e-18c12add16a3" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"></span>
<span id="cb6-2">df_census <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_pickle(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://github.com/texas-justice-initiative/officer_involved_shooting/blob/master/Data/Interim/census_county_race_2010.pkl?raw=true'</span>)</span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># highlighting the top 5 counties in terms of the no. civilians shot</span></span>
<span id="cb6-5">top5_locs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'incident_county'</span>].value_counts()[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>].index.values</span>
<span id="cb6-6"></span>
<span id="cb6-7">temp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian.groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'incident_county'</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'date_incident'</span>].count().sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span>
<span id="cb6-8">county_names_by_incidents <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> temp.index</span>
<span id="cb6-9">pop_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_census.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).loc[county_names_by_incidents.values]</span></code></pre></div>
</details>
</div>
<div id="846978a1-ac7c-4ca6-a1eb-d505efd5d0a1" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"></span>
<span id="cb7-2">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>))</span>
<span id="cb7-3">ax.plot(pop_size, temp, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'k'</span>, markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>)</span>
<span id="cb7-4">ax.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(xscale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'log'</span>, xlabel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Population Size'</span>, ylabel<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Civilians Shot'</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'General population vs. OIS data by County'</span>, xlim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e7</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], ylim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>])</span>
<span id="cb7-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> county_name <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> top5_locs:</span>
<span id="cb7-6">    ax.annotate(county_name.capitalize(), (pop_size.loc[county_name]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3e5</span>, temp.loc[county_name]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'crimson'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-7-output-1.png" width="335" height="281" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When I broke down the OIS data at the county level, I found that the number of civilians shot by police is positively correlated with the population size. Interestingly, most counties have a very small number of incidents and more than half of the data come from the top 5 most populous counties (annotated in the figure); <strong>Harris, Bexas, Dallas, Tarrant, and Travis.</strong></p>
<p>The race composition of all the civilians shot by police in our data shows that the proportion of each race group, white, Black and Hispanic is comparable.</p>
<div id="eaf875e6-a2f8-4cf9-85a3-95a30eeb829b" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"></span>
<span id="cb8-2">plot.plot_pie(df_civilian, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_race'</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cols_race, remove_labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, </span>
<span id="cb8-3">              title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Race Composition in the OIS Data</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(All Counties)'</span>,</span>
<span id="cb8-4">              figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>), </span>
<span id="cb8-5">              fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>,              </span>
<span id="cb8-6">              bbox_to_anchor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-8-output-1.png" width="455" height="284" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>However, if we focus on the top 5 counties, it is now clear that <strong>Black people are shot more often (39.3%) than white (25.1%) and Hispanic (32.2%).</strong></p>
<div id="79c7f0cd-2186-4bc7-8d7d-6d88b6fc23a6" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"></span>
<span id="cb9-2">df_civilian_top5 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian.loc[df_civilian[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'incident_county'</span>].isin(top5_locs), :]</span>
<span id="cb9-3">plot.plot_pie(df_civilian_top5, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_race'</span>, colors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cols_race, remove_labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, </span>
<span id="cb9-4">              title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Race Composition in the OIS Data</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(Top 5 Counties)'</span>,</span>
<span id="cb9-5">              figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb9-6">              fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>,</span>
<span id="cb9-7">              bbox_to_anchor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>))</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-9-output-1.png" width="455" height="284" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="black-people-are-overrepresented-in-police-shooting-incidents" class="level3">
<h3 class="anchored" data-anchor-id="black-people-are-overrepresented-in-police-shooting-incidents">Black people are overrepresented in police shooting incidents</h3>
<p>At this point, one might think that this is simply because the top 5 counties have larger Black populations than other counties. It’s true that in Texas, these top 5 counties with metropolitan areas have more Black people. If this hypothesis is true, this disproportionate distribution across race groups <em>should be reflected in the census data</em>.</p>
<div id="e0d5b3b4-ff5c-4c97-8e2b-8b54c6bd2e66" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"></span>
<span id="cb10-2">plot.plot_scatter_compare_race_incident_vs_population_pulled(</span>
<span id="cb10-3">    df_civilian, </span>
<span id="cb10-4">    df_census,</span>
<span id="cb10-5">    n_county<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb10-6">    annotate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb10-7">    xylim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>,</span>
<span id="cb10-8">    figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb10-9">    title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Racial Bias in Police Shooting</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(Top 5 Counties, 2016-2020)'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-10-output-1.png" width="277" height="277" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Unfortunately, I found that that was not the case. The plot above shows race representation of general population (US census, x axis) and of the police shooting incidents (OIS data, y axis). The dotted diagonal line shows equal representation between the census and the police shooting data. <strong>Any data points above the line indicate overrepresentation in the OIS data compared with the general population data</strong> (census).</p>
<p>The four colored dots represent the race groups in our data; Black (B), Hispanic (H), white (w), and others (O). The annotated numbers next to the dots show the rate of overrepresentation in the police shooting data of a race group compared to the census data. This analysis shows that <strong>Black people are 2.4 times overrepresented in police shooting compared with general population</strong> in these top 5 counties. The figure below shows that this pattern is in fact consistent across all 5 counties although <strong>the rate of overrepresentation for Black people is much higher particuarly for Harris and Dallas counties</strong>.</p>
<div id="fe89db69-c59d-4b55-9b5e-5fba6f0d10ed" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"></span>
<span id="cb11-2">plot.plot_scatter_compare_race_incident_vs_population(</span>
<span id="cb11-3">    df_civilian, </span>
<span id="cb11-4">    df_census, </span>
<span id="cb11-5">    fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>,</span>
<span id="cb11-6">    title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Racial Bias in Police Shooting (2016-2020)'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-11-output-1.png" width="925" height="206" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="black-and-hispanic-people-are-overrepresented-in-fatal-police-shooting-incidents" class="level3">
<h3 class="anchored" data-anchor-id="black-and-hispanic-people-are-overrepresented-in-fatal-police-shooting-incidents">Black and Hispanic people are overrepresented in <em>fatal</em> police shooting incidents</h3>
<p>Since the OIS data has a column that shows severity of incidents in terms of whether victims were killed or just injured, I decided to use the morality data of general population published by Texas Department of State Health Services (DSHS) to examine the <strong>racial bias in fatal police shooting incidents</strong>.</p>
<div id="eb2da1ee-20b7-41f0-8f51-37ce2b1e6a7e" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"></span>
<span id="cb12-2">df_civilian_died <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian[df_civilian[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'died'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb12-3">df_death_county <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://raw.githubusercontent.com/texas-justice-initiative/officer_involved_shooting/master/Data/Raw/Census/mortality_rate_by_county.csv'</span>, index_col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'County'</span>)</span>
<span id="cb12-4">df_death_county <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_death_county.drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TOTAL'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
</details>
</div>
<div id="813338d4-ea28-4d68-8f4f-9b186605a1a7" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"></span>
<span id="cb13-2">plot.plot_scatter_compare_race_incident_vs_population_pulled(</span>
<span id="cb13-3">    df_civilian, </span>
<span id="cb13-4">    df_death_county,</span>
<span id="cb13-5">    n_county<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,</span>
<span id="cb13-6">    annotate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb13-7">    xylim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>,</span>
<span id="cb13-8">    figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>),</span>
<span id="cb13-9">    title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Racial Bias in Fatal Police Shooting</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(Top 5 Counties, 2016-2020)'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-13-output-1.png" width="277" height="277" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Similar to the comparison with the census data, I found the <strong>overrepresentation of Black people in the fatal police shooting incidents (2.1 times overrepresented)</strong>. In addition, <strong>Hispanic people were overrepresented as well (1.6 times)</strong>. This pattern of overrepresentation of Black and Hispanic people was consistent across all 5 counties:</p>
<div id="ca2ad1e8-edbb-413b-a7fb-f2493e0c4656" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"></span>
<span id="cb14-2">plot.plot_scatter_compare_race_incident_vs_population(</span>
<span id="cb14-3">    df_civilian_died, </span>
<span id="cb14-4">    df_death_county, </span>
<span id="cb14-5">    fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>,</span>
<span id="cb14-6">    title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Racial Bias in Fatal Police Shooting (2016-2020)'</span>)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-14-output-1.png" width="925" height="206" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="young-black-men-are-overrepresented-in-fatal-police-shooting-incidents" class="level3">
<h3 class="anchored" data-anchor-id="young-black-men-are-overrepresented-in-fatal-police-shooting-incidents">Young Black men are overrepresented in fatal police shooting incidents</h3>
<p>I found a similar result when I looked at the intersection of race and age demographics. In the figure below, I’ve compared the race composition of mortality data (left, “County Deaths”) and the OIS data (right, “Deaths by OIS”) in various age groups (5 subplots). I used the same color palette as above (red for white, blue for Black, and dark gray for Hispanic). If there is no disproportionate race representation in the fatal police shooting data per age group, the two stacked bars (general population’s mortality and fata police shooting) in a subplot should look similar.</p>
<p>However, it is clear that Black people are overrepresented especially in younger age groups (ages 15-24 particularly). <strong>In the ages 15-24, Black people were about 3 times overrepresented in fatal police shooting incidents.</strong> This pattern quickly disappears as we move towards older age groups (plots on the right side).</p>
<div id="62d1d3ff-a3ad-42e9-b58b-cb432145f024" class="cell" data-tags="[]" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"></span>
<span id="cb15-2">race_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'WHITE'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BLACK'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'HISPANIC'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'OTHER'</span>]</span>
<span id="cb15-3"></span>
<span id="cb15-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># mortality data by age and gender (male only)</span></span>
<span id="cb15-5">df_death_age_male <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(</span>
<span id="cb15-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'https://raw.githubusercontent.com/texas-justice-initiative/officer_involved_shooting/master/Data/Raw/Census/mortality_rate_by_age_male.csv'</span>, </span>
<span id="cb15-7">    index_col<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Age'</span>).iloc[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:, :]</span>
<span id="cb15-8">age_range_names <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_death_age_male.index.values</span>
<span id="cb15-9"></span>
<span id="cb15-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># intersection of race and gender</span></span>
<span id="cb15-11">df_civilian_died_male_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian_died.loc[df_civilian_died[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_gender'</span>]<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'MALE'</span>].groupby([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_age_binned'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_race'</span>])[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'date_incident'</span>].count().unstack().fillna(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)[race_list]</span>
<span id="cb15-12"></span>
<span id="cb15-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># selecting the age groups of interest</span></span>
<span id="cb15-14">inds_age_binned <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>)</span>
<span id="cb15-15">df_civilian_died_male_age <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian_died_male_age.loc[inds_age_binned, :].drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'OTHER'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-16">df_death_age_male <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_death_age_male.iloc[inds_age_binned, :].drop(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'TOTAL'</span>, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-17"></span>
<span id="cb15-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># convert counts to proportions</span></span>
<span id="cb15-19">df_civilian_died_male_age_pct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preprocess.pct(df_civilian_died_male_age, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-20">df_death_age_male_pct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preprocess.pct(df_death_age_male, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb15-21"></span>
<span id="cb15-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># changing the index for plotting</span></span>
<span id="cb15-23">df_civilian_died_male_age_pct.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Age </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> age_range_names[inds_age_binned]]</span>
<span id="cb15-24">df_death_age_male_pct.index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Age </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(s) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> age_range_names[inds_age_binned]]</span></code></pre></div>
</details>
</div>
<div id="5bb13ff7-d35d-4166-8ad0-75c4b8124df5" class="cell" data-tags="[]" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"></span>
<span id="cb16-2">plot.plot_stackedbar_compare_ratio(</span>
<span id="cb16-3">    df_civilian_died_male_age_pct,</span>
<span id="cb16-4">    df_death_age_male_pct,</span>
<span id="cb16-5">    df_civilian_died_male_age.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).astype(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>), </span>
<span id="cb16-6">    severity<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Deaths'</span>, </span>
<span id="cb16-7">    legend<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb16-8">    figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb16-9">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-16-output-1.png" width="710" height="421" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="incident-cause-for-black-people-is-often-other" class="level3">
<h3 class="anchored" data-anchor-id="incident-cause-for-black-people-is-often-other">Incident cause for Black people is often “Other”</h3>
<p>I found another example of racial disparity in how law enforcement agencies labeled the cause of incidents. The OIS report provides a list of five categories as a potential cause of an incident; “Traffic Stop”, “Emergency/Request for Assistance”, “Execution of a Warrant”, “Hostage/Barricade/Other Emergency”, and “Other”. Obviously, the last category, <strong>“Other” has the lowest accountability and transparency</strong> because it can be anything.</p>
<div id="5afd50b8-c13f-47cb-854f-feffb1f614d7" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"></span>
<span id="cb17-2">incident_causes_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Traffic Stop"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Emergency/Request for Assistance"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Execution of a Warrant"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hostage/Barricade/Other Emergency"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Other"</span>]</span>
<span id="cb17-3">incident_causes_list_sorted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian[incident_causes_list].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).sort_values(ascending<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>).index</span>
<span id="cb17-4">df_civilian_top5 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian.loc[df_civilian[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'incident_county'</span>].isin(top5_locs), :]</span>
<span id="cb17-5">df_civilian_incident_race_top5 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_civilian_top5.groupby(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'civilian_race'</span>)[incident_causes_list_sorted].<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().loc[race_list, incident_causes_list_sorted]</span>
<span id="cb17-6">df_civilian_incident_race_top5_pct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> preprocess.pct(df_civilian_incident_race_top5, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div>
</details>
</div>
<div id="24662627-075a-4854-86b0-f6158ac164bf" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"></span>
<span id="cb18-2">fig, ax <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span>
<span id="cb18-3">df_civilian_incident_race_top5_pct.T[::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].plot(kind<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'barh'</span>, stacked<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ax, legend<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cols_race)</span>
<span id="cb18-4">plot.annotate(ax, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'h'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'percent'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb18-5">ax.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(xlim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>])</span>
<span id="cb18-6">ax.set_yticklabels([s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">' (</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(n) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> s, n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(df_civilian_incident_race_top5.columns, </span>
<span id="cb18-7">                                                          df_civilian_incident_race_top5.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).values)][::<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb18-8">fig.legend(race_list, ncol<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.81</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.09</span>), fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb18-9"></span>
<span id="cb18-10">fig.suptitle(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Race Demographics by Incident Cause (Top 5 Counties)'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.6</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.03</span>)</span>
<span id="cb18-11">fig.tight_layout()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1_files/figure-html/cell-18-output-1.png" width="783" height="239" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>When incident cause data is broken down by race groups, the “Other” category shows unusually high proportion of Black people. This means that <strong>when OIS reports were filed, more than half of the times, the incident cause was identified as “Other” when the victim was a Black person.</strong> Again, considering this category’s low accountability and transparency, this racial disparity was another concerning example.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>In this post, I’ve introduced several examples of my data science volunteering work at Texas Justice Initiative and a larger context surrounding it. Using reference data and conducting intersectional analyses, I was able to identify important racial disparity in Texas police shooting data that was never addressed in the state government’s official annual reports. In the follow-up post, I will share some useful tips for tech volunteering (especially in terms of using data science skills).</p>


</section>

 ]]></description>
  <category>criminal justice</category>
  <category>visualization</category>
  <category>EDA</category>
  <category>volunteering</category>
  <category>journalism</category>
  <guid>https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/2021-05-24-Data-Science-Volunteering-Part1.html</guid>
  <pubDate>Mon, 24 May 2021 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2021-05-24-ds-volunteering-part1/index.png" medium="image" type="image/png" height="101" width="144"/>
</item>
<item>
  <title>FAccT 2021. Journalism, data leverage, education, and language models</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2021-03-10-facct-2021-main3/</link>
  <description><![CDATA[ 





<section id="keynote-algorithms-accountability-and-journalism" class="level2">
<h2 class="anchored" data-anchor-id="keynote-algorithms-accountability-and-journalism">Keynote: Algorithms, Accountability, and Journalism</h2>
<p>The final keynote was given by Julia Anguin, an investigative journalist and a co-founder of <a href="https://themarkup.org/">The Markup</a>, “a nonprofit newsroom that investigates how powerful institutions are using technology to change our society.” I was already familiar with her body of her because she’s famous for <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">her ProPublica article</a> on racial bias in the risk assessment software, COMPAS. Her talk consisted of two parts. First, she talked about how her organization Markup is run and what methods they use to publish investigative journalism reports. Later, she gave several examples of their work from last year, which was mostly on algorithm auditing.</p>
<p>Markup consists of <strong>engineers and journalists</strong>. It’s interesting to hear her saying that engineers are essentially investigative journalists themselves. They collaborate with journalists in the team but they bring tech expertise to tech reporting. Once Markup has written a report, they have an extensive vetting process (called the “bulletproof” stage) where they actively seek out critiques from various external experts. What’s interesting is that each work product comes with a pair of publications; a news article targeting general public and <strong>an extensively detailed methodological write-up</strong>. They also have established a practice of publishing datasets and codes so that other people can replicate their analysis and apply those to their own projects.</p>
<p>When she gave examples of their algorithm auditing projects, it was amusing to learn about various novel approaches they have made to probe algorithms. In one project, they simply analyzed search results, but in other projects, they created numerous web accounts to try to reverse-engineer opaque decisions algorithms make. Markup also seemed to put resources into creating tools such as <a href="https://themarkup.org/blacklight">Blacklight</a> that runs privacy tests on virtual browsers for websites to inspect their privacy violations, or <a href="https://themarkup.org/series/citizen-browser">Citizen Browser</a> where volunteer citizens use Markup’s app to scrape data from Facebook so that the team can examine various decisions the Facebook app makes.</p>
<p>During the Q&amp;A, one memorable remark she made was <strong>all data is political.</strong> Whether it’s leaked, accessed through protocols, or publicly available, data collection starts with a certain intention. She said there’s no national database on police violence in the US, which reveals the political will and intention of the US government, which was revealing. To fight this issue, she said it’s really <strong>important to know the limitations of data and be very transparent about it.</strong></p>
<p>She mentioned that Markup often tries to co-publish pieces with other news companies for distribution reasons. She said sometimes the process is challenging because these organizations do not have technical experts to review their lengthy methodological write-up. This resonated with me most because this is what I’ve been experiencing while volunteering at a local non-profit. Nevertheless, when one of the audience members asked her about how individual tech workers help, she said the most straightforward way to help journalists it via financially supporting them, especially the local news organizations.</p>
</section>
<section id="data-leverage" class="level2">
<h2 class="anchored" data-anchor-id="data-leverage">Data Leverage</h2>
<p>In the tech world, there’s a tendency to trivialize data labor. Tech companies also conduct problematic practices to collect data. For the public, who are in a very vulnerable position, what can they do? <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445885"><em>Data Leverage: A Framework for Empowering the Public in its Relationship with Technology Companies</em></a> explores several ways to leverage the fact that we the public are data providers. The defined the data leverage as <strong>power to influence a company held by those who implicitly or explicitly contribute data on which companies rely</strong>. The large goal of identifying data leverage is to give more agency to people. They explore three options; data strike, data poisoning, and conscious data contribution. Each options has its pros and cons and some have more complex legal landscape to navigate. Personally I found <strong>data poisoning</strong> most interesting, which is about inputting fake and random data (somewhat similar to adversarial attacks).</p>
</section>
<section id="measurement" class="level2">
<h2 class="anchored" data-anchor-id="measurement">Measurement</h2>
<p>Any attempts to address FAccT topics in mathematical models begin with constructing a measurement; we are trying to measure unobservable and abstract concepts. <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445901"><em>Measurement and Fairness</em></a> proposes <strong>measurement modeling framework</strong> in fairness research in computer science. The authors target fairness specifically because fairness itself is an <strong>essentially contested construct</strong>; it’s heavily context-dependent and some constructs are conflicting. Besides, in general, determining which measurements and metrics to use requires extreme caution. To mitigate these problems, the authors came up with two criteria; <strong>construct reliability</strong> and <strong>construct validity</strong>. The former checks whether similar inputs can return similar measurements. The latter checks whether measurements are meaningful and useful such as whether they capture relevant events, or whether the impact from using the measurements has been addressed.</p>
</section>
<section id="education" class="level2">
<h2 class="anchored" data-anchor-id="education">Education</h2>
<p>Since last year, the conference has been including more papers about ethics education. This year, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445914"><em>“You Can’t Sit With Us”: Exclusionary Pedagogy in AI Ethics Education</em></a> stood out. This survey paper collected more than 250 AI ethics courses in computer science curriculums from more than 100 universities around the world and analyzed the pattern. Sadly, they found a predominant pattern of exclusion in many courses. The authors found this exclusion had many shapes; the discipline not valuing other ways of knowing, lack of collaboration with other disciplines, and lack of interest in learning other’s work. The fact that computer science itself can’t solve AI ethics problems, this seemed very worrisome.</p>
</section>
<section id="language-models" class="level2">
<h2 class="anchored" data-anchor-id="language-models">Language Models</h2>
<p>What happens when an authoritarian government and AI meet? <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445916"><em>Censorship of Online Encyclopedias: Implications for NLP Models</em></a> explores how censorship in training data influence downstream processes in NLP applications using Chinese language models. One of the dataset they looked into was <a href="https://en.wikipedia.org/wiki/Baidu_Baike">Baidu Baike</a>, a censored language dataset that is often used in Chinese language models. They first checked the word embeddings and examined the position of words such as <em>democracy, surveillance, social control, CCP (Chinese Communist Party)</em> with respect to positive and negative words. They found that <em>democracy</em> often appeared with negative words and the rest in the example were the opposite. They also found similar pattern in a sentiment classification application that uses web news headlines. These results were concerning because as the authors addressed, these applications can be used to monitor public opinion and curate social media posts, essentially as a highly effective propaganda machine in a massive scale.</p>
<p>For those who’ve been following the news of Timit Gebru, an AI ethics researcher who was fired by Google, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922"><em>On the Dangers of Stochastic Parrots:Can Language Models Be Too Big?</em></a> was the paper that was at the core. The paper explores potential risks of the current trend in language modeling where researchers and practitioner pay more attention to larger models. As shown in the paper, these models have <strong>billions, sometimes trillions of parameters</strong>. The first risk the authors bring up is environmental and financial cost. Studies have shown that <strong>training a single BERT base model (without tuning) requires as much energy as a trans-American flight</strong>. This gets more problematic if you become aware that <strong>most language models serve English-speaking communities</strong> and the most impacted communities from climate change are not those. There are more sinister risks too such as training data still lacking diversity (i.e., <strong>“big” doesn’t mean “diverse.”</strong>) and lacking oversight, which creates harmful effects. Plus, since many efforts and resources in ML community go into building large language models, other research topics are naturally overlooked. The authors suggest simple mitigation strategies - <strong>step back and think</strong>; evaluate various approaches, ask yourself if we really need large models, and conduct <strong>pre-mortem</strong> analysis.</p>


</section>

 ]]></description>
  <category>conference</category>
  <category>journalism</category>
  <category>measurement</category>
  <category>education</category>
  <category>LLM</category>
  <category>responsible AI</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2021-03-10-facct-2021-main3/</guid>
  <pubDate>Wed, 10 Mar 2021 06:00:00 GMT</pubDate>
</item>
<item>
  <title>FAccT 2021. Automated decision-making, causal accountability, and robustness</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2021-03-09-facct-2021-main2/</link>
  <description><![CDATA[ 





<section id="keynote-in-praise-of-flawed-mathematical-models" class="level2">
<h2 class="anchored" data-anchor-id="keynote-in-praise-of-flawed-mathematical-models">Keynote: In Praise of (Flawed) Mathematical Models</h2>
<p>One of the reasons I try to attend the FAccT conference is because I always get inspired by novel approaches from this community to quantify and measure social concepts such as fairness. However, this approach has its own demise. It’s possible that these attempts get lost in the weeds while discussing abstract and theoretical aspects of the models which can easily alienate researchers from the real world. Today’s keynote by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a> from Hebrew University addressed this issue.</p>
<p>During the talk, she discussed the pros and cons of using mathematical models in general, but also in the context of studying FAccT-related topics. Many positive aspects of mathematical models she mentioned were exactly the same reasons why I have been drawn to this area; mathematical models are <strong>explicit about their assumptions</strong> such as what’s included and what’s not, usually come with provable guarantees, and provide solid foundations for future research. Mathematical models can also easily <strong>become a fig leaf</strong> by letting bad actors hide behind the models’ specificity and complexity. People also have a misperceived notion on mathematical models that the models are objective (and thus have authority).</p>
<p>She talked a lot about this duplicity during the Q&amp;A. It was nice to hear from Katrina that theoreticians have tendency to be drawn to intellectually stimulating problems and sometimes lose touch with the main purpose that our models serve. She also talked about how to bridge the gap between people who build mathematical models and people who do not have the background but are impacted by the models. She first talked about the <strong>importance of translational type of work</strong> which explains theoretical concepts in real-world meanings. This way, it is going to be much easier to <strong>critique models’ basic assumptions</strong> where many problems begin.</p>
<p>She ended the talk in a positive note by saying that even with the flaws of mathematical models, our approach should not be banning mathematical models but rather <strong>encouraging modelers to acknowledge the shared responsibility</strong> within the community they serve, to take advantage of the rigorousness built in mathematical approaches to carefully examine the problems, and to constantly critique the models.</p>
</section>
<section id="automated-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="automated-decision-making">Automated Decision-Making</h2>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445921">Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems</a> presents an easy-to-follow and straightforward guide for automated decision-making (ADM) system review. Being able to review something means that we can hold companies easily accountable for the consequences from ADM systems, which already have serious issues of lack of accountability. The authors borrowed the idea from <strong>administrative law’s approach to reviewing human decision-making</strong>, and presents a practical and holistic framework. The suggested idea consists of various parts such as commissioning, model building, decision-making, investigation, group deliberation, and assessment. I liked their idea originated from law because this will help legislators regulate ADMs.</p>
<p>Speaking of regulation, <a href="https://dl.acm.org/doi/epdf/10.1145/3442188.3445938">An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists</a> was an excellent paper in that it provided an example of <strong>algorithmic equity toolkit</strong> designed for diverse stakeholders. I assume it was possible because the authors come from diverse background; computer science, social and political science, and activism. The toolkit aims to increase public participation for AI-policy action and consists of various formats including flowcharts, maps, and questionnaires. With this diverse methods, it tries to deliver comprehensive of information of AI technology such as the type, functionality, and its potential impact. This looks like a handy way to facilitate communication between many different stakeholders such as communities, government officials, and even technologists.</p>
</section>
<section id="accountability-and-recourse" class="level2">
<h2 class="anchored" data-anchor-id="accountability-and-recourse">Accountability and Recourse</h2>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445905">Designing Accountable Systems</a> suggests that accountability requires a causal model because if something happens and we need to hold someone accountable for the consequences. The authors bring an interesting distinction between forward- and backward-looking causalities. When we build causal models, we use the forward-looking approach such as “A causes B”. However, when we need to use causality for accountability, we lean on the <strong>backward-looking</strong> approach such as “the fact that A happened last year causes B to die yesterday”. Thus, the authors use Structural Causal Models to evaluate a system’s design for accountability.</p>
<p>In a similar context, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445899">Algorithmic Recourse: from Counterfactual Explanations to Interventions</a> also emphasizes the importance of causal models. Many previous studies on counterfactuals as a means of algorithmic recourse overlooked how to achieve the counterfactual scenarios. <strong>Without addressing the feasibility, algorithms may provide nonsensical counterfactual explanations</strong> such as “in order to get a loan, change your ethnicity”. This study shows that causal models can mitigate this issue because causal explanations inherently avoids nonsensical explanations from the beginning.</p>
<p>Finally, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445943">Epistemic Values in Feature Importance Methods: Lessons From Feminist Epistemology</a> looks at explainable AI (particularly feature importance) through the lense of feminism studies, which was highly enlightening. The authors say explanations and epistemology (study of human knowledge) are inseparable; <strong>useful explanations have epistemic values</strong>. And providing AI explanations in a feminist way is means we <strong>focus on understanding of situated knowledge</strong> and avoid totalization. To elaborate, explanations should <strong>respect different perspectives, particularly from marginalized individuals</strong>, and encourage active conversation about the knowledge. If I look back on how I used feature importance as a tool for explainable AI, it is true that it’s usually a one-way communication that was static and lacked communication with stakeholders. The authors suggest that it’s crucial to evaluate explanation in context and keep it <strong>interactive</strong>. More importantly, since explanations can often be used as an authoritative way, it’s important to <strong>encourage users to avoid over-trusting one interpretation</strong>, which was such a thoughtful insight.</p>
</section>
<section id="robustness" class="level2">
<h2 class="anchored" data-anchor-id="robustness">Robustness</h2>
<p>It’s well known that proxy features can contain information about sensitive attributes such as race and gender. Often people try to remove these spurious features from the dataset. In fact, this practice is not just limited to applications that use datasets with sensitive attributes. ML practitioners drop correlated or spurious features to build more robust models. <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445883">Removing Spurious Features can Hurt Accuracy and Affect Groups Disproportionately</a> shows that <strong>removing these features can lead to performance drop</strong>. The authors mitigated the problem by using a technique called <a href="https://arxiv.org/abs/2002.10716">Robust Self-Training</a>. The authors of this paper also investigated how the performance drop is manifested among different groups in the data.</p>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445910">Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning</a> is an interesting paper that they define the group <strong>fairness in terms of model vulnerability against adversarial attacks</strong>. They first address that in deep learning models, it is possible for a certain subgroup to be more vulnerable to those attacks, and thus the model is less robust for particular subgroups. You can easily picture this as a classification with a decision boundary. If the model is not robust and changes its decision boundary when the attack occurs, samples that are closer to the original boundary may be more likely to be misclassified with the attack. The authors identified that for deep learning models, this is a complex matter because it is about data drift and the learning process of the model. However, as a first attempt, they added a simple regularization term to the model to equalize the robustness bias among the subgroups in some examples.</p>


</section>

 ]]></description>
  <category>conference</category>
  <category>explainability</category>
  <category>causality</category>
  <category>robustness</category>
  <category>responsible AI</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2021-03-09-facct-2021-main2/</guid>
  <pubDate>Tue, 09 Mar 2021 06:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2021-03-09-facct-2021-main2/toolkit.png" medium="image" type="image/png" height="52" width="144"/>
</item>
<item>
  <title>FAccT 2021. AI audit, governance, and trustworthiness</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2021-03-08-facct-2021-main1/</link>
  <description><![CDATA[ 





<p>This year, for the first time, the FAccT conference has more than one track and there are 27 sessions (+80 talks) happening in 3 days. This is a summary of the first day of the main conference. My summary is based on the talks I attended based on my interest.</p>
<section id="keynote-health-technology-and-race" class="level2">
<h2 class="anchored" data-anchor-id="keynote-health-technology-and-race">Keynote: Health, Technology and Race</h2>
<p>As a person who’s been attending FAccT every year, I’ve been familiar with the criticism to the FAccT academic community; many studies appeared here are often too abstract or disconnected from the real world and the impacted communities. The organizers are aware of this and they have been making efforts such as organizing CRAFT (Critiquing and Rethinking Accountability, Fairness and Transparency) sessions where they bring more multidisciplinary stakeholders. I considered the first keynote from today, given by Yeshimabeit “Yeshi” Milner, the executive director and co-founder of <a href="https://d4bl.org/"><em>Data for Black Lives (D4BL)</em></a>, as a signal from the conference saying that they are actively working on this criticism. In this regard, it was an excellent keynote speech to kick off the conference.</p>
<p>At the very beginning of the talk, Yeshi said, “we will never achieve fairness, accountability, and transparency <strong>without first demanding justice, power, and self-determination</strong>.” Again, this directly points out that without a holistic approach where we bring all the stakeholders, achieving justice and making changes isn’t going to happen. Yeshi then talked about racial disparity examples where Black people were discriminated and disproportionately affected in a negative way, especially in healthcare.</p>
<p>She introduced the <a href="https://d4bl.org/covid19-data"><em>COVID-19 Data Tracker Project</em></a> where organizers in the D4BL members compiled a dataset by themselves to highlight the stark disparities in health outcomes related to COVID-19. She mentioned that the larger goal here is to “build the political power of Black communities” as well, which is a critical point. She also emphasized that <strong>any intervention that does not build political power or agency of the marginalized community is liable to harm</strong> rather than to help. Considering that even some tech-for-good projects with good intentions often sideline those who are affected by the system, her remark was poignant.</p>
<p>“We need to think ‘who do we need to be’” At the end of the talk, she mentioned the importance of reflection, and said we need to resist the urge to feel compelled to keep moving forward, but to pause and reflect on ourselves. This involves identifying what we should unlearn and how we need to create new spaces to bring people who are directly impacted. I think this passionate and moving speech from Yeshi sent out this important message to all attendees, especially to academics and tech workers, like myself.</p>
</section>
<section id="algorithm-audit-and-impact-assessment" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-audit-and-impact-assessment">Algorithm Audit and Impact Assessment</h2>
<p>For tech workers who are interested robust and responsible AI like myself, case studies are like gems. We are always curious about how others do things and want to learn from each other. That’s why I was very happy to see a case study about algorithm audit from Wilson et al., <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445928"><em>Building and Auditing Fair Algorithms: A Case Study in Candidate Screening</em></a>.</p>
<p>Researchers from Northeastern University worked with <a href="https://www.pymetrics.ai/">Pymetrics</a>, a company that uses gamification and AI to assist hiring in other companies. This collaboration itself was a unique instance because most audits happen internally or in a permission-less form where the companies being audited do not cooperate with the auditors. The academic researchers seemed to be quite transparent about the process (e.g., they revealed they were paid but the audit was done independently) and disclosed some problems in Pymetric’s system based on the criteria they used. I don’t know the full details yet but it was nice to hear that Pymetrics started working on solutions to fix these.</p>
<p>Speaking of the audit, there was another interesting paper about Amazon’s questionable practice in product recommendation. It’s been known that Amazon created their own private brand and started competing against other brands. The paper, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445944"><em>When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces</em></a> investigates this issue by conducting a systematic audit on Amazon’s recommendation system. They found that Amazon’s private brand has greater exposure in the sponsored ads by comparing related item network (RIN) models between the private brand’s and others’ networks.</p>
<p>Finally, there was a paper dedicated to algorithmic impact assessment. The authors in the paper, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445935"><em>Algorithmic Impact Assessments (AIA) and Accountability: The Co-construction of Impacts</em></a>, pointed out that often poor impact assessment practices neither mitigate harm nor bring effective algorithm governance. They emphasized that impact assessment should be closely related to accountability, especially for both institutions and people who are affected. The paper showcases impact assessment examples from different domains, which will be useful to design an effective and accountable algorithmic impact assessment procedure.</p>
</section>
<section id="trust-in-ai" class="level2">
<h2 class="anchored" data-anchor-id="trust-in-ai">Trust in AI</h2>
<p>Speaking of impact assessment, if we have incredibly thoughtful (which is probably very detailed) impact assessment and documentation of deployed algorithms, does it bring more trust? The paper, <a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445890"><em>The Sanction of Authority: Promoting Public Trust in AI</em></a>, says, perhaps not. The authors bring up the example of aviation industry. When we board on a plane, do we want to see the maintenance log, history of pilot performance to reassure ourselves? Or, do we normally trust that aviation is <strong>“highly regulated, comparatively safe, and any breach of safety will be sanctioned”</strong>? The authors suggest that the latter is how public trust is formed. This is an excellent point. Documentation of deployed AI systems can be as bad as terms of service statements, which usually have extremely poor readability and can be easily manipulated. I liked that the authors emphasized both the importance of regulatory system but also of <strong>externally auditable AI documentation</strong>, which would facilitate the development of AI regulations.</p>
</section>
<section id="explainable-ai" class="level2">
<h2 class="anchored" data-anchor-id="explainable-ai">Explainable AI</h2>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445941"><em>How Can I Choose an Explainer? An Application-Grounded Evaluation of Post-hoc Explanations</em></a> was a case study of comparing various XAI methods (LIME, SHAP, and <a href="https://github.com/andosa/treeinterpreter">TreeInterpreter</a>) and how human participants use explanations to perform fraud detection task. I was hoping to see a discussion of comparison and evaluation of various XAI methods in a standardized way but it was more about evaluating the suitability of these methods using case-specific real-human tasks. I’d like to take a deeper look at the paper to see whether the authors mentioned any suggestions on cases where it is challenging to run those tasks.</p>
</section>
<section id="data-governance" class="level2">
<h2 class="anchored" data-anchor-id="data-governance">Data Governance</h2>
<p>My favorite talk of the day was <a href="https://arxiv.org/pdf/2010.13561.pdf"><em>Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure</em></a> by Hutchinson et al.</p>
<p>It’s not too difficult to meet ML practitioners who are obsessed with state-of-the-art deep learning models and conveniently ignore training data and how their data is preprocessed. These people often have a tendency of even looking down upon data preprocessing process and consider it demeaning. This paper takes a jab at this phenomenon.</p>
<p>The authors first talk about data scapegoating. Even though everyone is well aware of “garbage in, garbage out” relationship between data and ML models, often data is overlooked and undervalued. Many ML practitioners depend on benchmark datasets, which <strong>“reinforce the notion of data as fixed resources.”</strong> In my experience, this tendency intensifies the fanaticism on ML algorithms even though they are just a tiny part of a much bigger picture. With many good off-the-shelf models readily available, frankly speaking, models are rather becoming commodities.</p>
<p>They also point out the lack of good governance practice in data maintenance. Understanding ML model performance often requires good and detailed understanding of training data. However, validation and verification of training data is rarely conducted in ML engineering pipelines.</p>
<p>Finally, the authors didn’t forget to mention the power imbalanced between ML practitioners and data providers. I often feel extremely fortunate that I have experienced the entire end-to-end process of an academic research. I started from scratch and designed a process to collect the data by myself, which required great care and intense labor. In the current ML trend, this step is often conveniently removed and thus creates a bad habit of ignoring labor and responsibility in the data collection and development process.</p>
<p>I really appreciated the simplicity of the authors’ suggestion on this problem; <strong>“acknowledge that datasets are technical infrastructures.”</strong> Since many ML practitioners are familiar with tech infrastructures such as software programs, the authors suggest that we simply apply existing practices to datasets; create <strong>requirements, design, and tests</strong>. Once this formally becomes a part of infrastructure, it also becomes easy to get all the existing stakeholders in the pipeline involved.</p>


</section>

 ]]></description>
  <category>conference</category>
  <category>governance</category>
  <category>explainability</category>
  <category>responsible AI</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2021-03-08-facct-2021-main1/</guid>
  <pubDate>Mon, 08 Mar 2021 06:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2021-03-08-facct-2021-main1/data dev cycle.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>Tutorials at FAccT 2021</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2021-03-04-facct-2021-tutorial/</link>
  <description><![CDATA[ 





<p>Last year during the SciPy 2020 conference, I participated in a mentoring program and I’ve got to know a fellow data scientist, <a href="https://twitter.com/MrHenHan">Henrik Hain</a>. He diligently uploaded a daily update during the conference, which was impressive. Inspired by him, I’ve decided to follow his practice and to write notes during my attendance in the FAccT 2021 this year.</p>
<section id="conference-overview" class="level2">
<h2 class="anchored" data-anchor-id="conference-overview">Conference Overview</h2>
<p>The conference is held virtually this year for the obvious reason. It’s on a platform called <a href="https://circle.so/">circle.so</a>. Everything happens in one platform; video streaming, general announcement, community boards, internal messaging, etc., which is pretty convenient.</p>
<p>As an international <em>virtual</em> conference, the schedule is a bit brutal. For my time zone (UTC-6), the schedule starts at 6 am and lasts until 4-7 pm in the evening. Talks in the main session are pre-recorded and thus attendees can watch at their convenience but some sessions occur in real time. Luckily, the conference organizers set up dedicated watching times for the main session, which I really appreciate.</p>
<p>Today was about tutorials. I attended the following: - Causal Fairness Analysis - Explainable ML in the Wild: When Not to Trust Your Explanations - Responsible AI in Industry: Lessons Learned in Practice - A Behavioral and Economic Approach to Algorithmic Fairness &amp; Human-Centered Mechanism Design - How to Achieve Both Transparency and Accuracy in Predictive Decision Making: an Introduction to Strategic Prediction</p>
<p>Each tutorial was about 90 minutes, which was a bit short for me. Since FATE (fairness, accountability, transparency, and ethics) topics are highly diverse, it makes sense to have many different tutorials. But because they were so short, they weren’t much of a deep-dive and an overview (at least for the ones I attended).</p>
</section>
<section id="causal-fairness-analysis" class="level2">
<h2 class="anchored" data-anchor-id="causal-fairness-analysis">Causal Fairness Analysis</h2>
<p>This tutorial was about conducting fairness analysis through causal framework by using <strong>structural causal models (SCMs)</strong>. The speaker (Elias Bareinboim) started the talk by mentioning that the burden of proof is on the plaintiff in discrimination lawsuit, meaning they have to prove the causal connection if they were treated unfairly. I found this emphasis and motivation from legal lense quite refreshing.</p>
<p>The material was relatively easy to follow in the beginning where Elias explained SCM models and how we evaluate them with given empirical data. However, it got a bit challenging to follow where we started combining fairness and causal models, especially given that fairness metrics are so diverse.</p>
<p>For me the most interesting part was where Elias compared various counterfactual scenarios. I’ve always assumed that the causal DAGs will not change when we switch the group membership to simulate counterfactuals, but obviously there is no guarantee. It’s <strong>possible that we can have indirect and spurious effects for counterfactual scenarios</strong> and he explained that we need to subtract those effects from direct effects.</p>
</section>
<section id="explainable-ml-in-the-wild-when-not-to-trust-your-explanations" class="level2">
<h2 class="anchored" data-anchor-id="explainable-ml-in-the-wild-when-not-to-trust-your-explanations">Explainable ML in the Wild: When Not to Trust Your Explanations</h2>
<p>This tutorial consisted of three parts: overview of explainable AI (XAI) methods, their limitations and ethical/practical challenges. I found the second part most interesting.</p>
<p>The speaker (Chirag Agarwal) introduced four aspects of XAI limitations: <strong>faithfulness, stability, fragility</strong>, and <strong>evaluation gap</strong>. Faithfulness refers to whether explanations change when models changes (and perhaps also <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372836">the Rashomon effect</a>). Stability refers to whether post-hoc explanations are unstable with respect to small non-adversarial input perturbation. For instance, some evidence shows that <a href="https://arxiv.org/abs/1602.04938">LIME</a> explanations may change if we change random seed in certain ML algorithms. We also wouldn’t want our models to chance explanations based on hyperparameters.</p>
<p>Fragility is about whether explanation changes according to data drift in input space. This is closely related to adversarial attack on explanations (i.e., whether small perturbation can change explanation without changing prediction). Finally, in general, it is very difficult to properly evaluate XAI methods and currently there is no ground truth for evaluation.</p>
<p>Perhaps because of these problematic features, the case studies presented in the following session felt very nuanced and complicated. As I’ve seen in other responsible AI talks, XAI methods are closely related to human-in-the-loop systems and the level of trust in human end users.</p>
</section>
<section id="responsible-ai-in-industry-lessons-learned-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="responsible-ai-in-industry-lessons-learned-in-practice">Responsible AI in Industry: Lessons Learned in Practice</h2>
<p>This talk consisted of two parts: overview of responsible AI tools and related case studies. These tools are useful for <strong>model monitoring/inspection, generating explanations, fairness mitigation, error analysis, and counterfactual analysis</strong>. I was surprised that there are already several open-source tools available such as <a href="https://interpret.ml/">InterepretML</a>, <a href="https://fairlearn.org/">Fairlearn</a>, <a href="https://pair-code.github.io/what-if-tool/">What-If Tool</a>.</p>
<p>The live demo from Microsoft was impressive but it was too fast to follow (I personally think Jupyter notebook isn’t the best method for presentation if we have to go back and forth a lot). Also I was not sure whether the examples were from deployed projects or toy datasets especially since the speaker talked about fairness mitigation and I was curious about how the full cycle of mitigation process looked like.</p>
<p>The case study from LinkedIn at the end was interesting (especially since they presented similar material last year at the same conference) but it felt somewhat disconnected for the same reason I just mentioned above; I wasn’t sure how human end users were involved in the fairness mitigation process.</p>
</section>
<section id="a-behavioral-and-economic-approach-to-algorithmic-fairness" class="level2">
<h2 class="anchored" data-anchor-id="a-behavioral-and-economic-approach-to-algorithmic-fairness">A Behavioral and Economic Approach to Algorithmic Fairness</h2>
<p>This talk was about looking at the fairness problem from economic perspective. Different from the traditional computer science approach, this talk suggested that economic approach presents the fairness problem in the form <strong>social welfare functions</strong> where social planner can optimize efficiency (expected outcome of interest among groups) and equity.</p>
<p>What was interesting was that the optimal algorithm isn’t just about prediction function but also about <strong>admission rule</strong>, meaning how social planner can use the predictions to make decisions. Normally, these admission rules are threshold-based; we make a decision on certain group of individuals based on certain thresholds, which is affected by equity preference of social planner. Another interesting aspect was that this equity preference can even affect the prediction function because <strong>decision makers who prefer discrimination may want to use additional features</strong> from data to discriminate protected groups.</p>
</section>
<section id="how-to-achieve-both-transparency-and-accuracy-in-predictive-decision-making-an-introduction-to-strategic-prediction" class="level2">
<h2 class="anchored" data-anchor-id="how-to-achieve-both-transparency-and-accuracy-in-predictive-decision-making-an-introduction-to-strategic-prediction">How to Achieve Both Transparency and Accuracy in Predictive Decision Making: an Introduction to Strategic Prediction</h2>
<p>The whole idea of strategic prediction is very interesting because it’s about <strong>human users trying to game the system if they start understanding the rule of the game</strong>. For instance, if students learn about the criteria universities use for their admission process, they will try to find ways to cross that threshold so that they can be admitted. This tutorial gave an overview of this phenomenon from the perspective each stakeholder group in the process; institution (algorithm designers), individual (data provider), and society (all people as a whole).</p>
<p>Instead of doing a deep dive on a specific topic, the tutorial presented various topics in the domain. <strong>Recourse and incentivization</strong> was one of them. If the deployed algorithm uses a feature that can harm individuals because the feature incentivizes them to behave in a certain way, automated harm at a massive scale will be expected. Another way to look at the strategic prediction is through causality. If we find a true <strong>causal relationship between features and model predictions</strong>, it might be able to utilize the benefit of strategic prediction from the institution’s perspective and to encourage improvement without encouraging gaming, which will be crucial for modellers.</p>


</section>

 ]]></description>
  <category>conference</category>
  <category>causality</category>
  <category>fairness</category>
  <category>explainability</category>
  <category>responsible AI</category>
  <category>ML</category>
  <guid>https://hongsupshin.github.io/posts/2021-03-04-facct-2021-tutorial/</guid>
  <pubDate>Thu, 04 Mar 2021 06:00:00 GMT</pubDate>
</item>
<item>
  <title>Markdown and GitHub for scientific writing</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2020-11-24-github-manuscript-review/</link>
  <description><![CDATA[ 





<p>Last year, I was fortunate to have an opportunity to do a poster presentation at the <a href="https://www.scipy2019.scipy.org/">Scientific Computing in Python (SciPy) conference</a>. Every presenter has an option to submit a written proceedings and I decided to participate. I’ve never had a chance to present my ML work as a paper in industry, and so I thought it would be a good experience.</p>
<p>I was a bit intimated when the conference committees said authors should submit their manuscripts as <strong>pull requests</strong> in their proceedings repository in GitHub. They also said manuscripts should be written in <a href="https://en.wikipedia.org/wiki/ReStructuredText">reStructuredText</a>. I have used git for many years but I have limited experience in collaborating with others on GitHub. Plus, I’ve never used reStructuredText (I’ve only used Markdown) and also have never written an academic manuscript in markup languages.</p>
<p>In short, I eventually became to really enjoy the process of using markup languages for writing manuscripts and using GitHub for review. I will briefly talk about my experience at the SciPy and talk about why I think you should give it a try.</p>
<section id="scientific-writing-in-markup-languages" class="level2">
<h2 class="anchored" data-anchor-id="scientific-writing-in-markup-languages">Scientific writing in markup languages</h2>
<p>There was a bit of learning curve but it wasn’t too difficult to get used to the reStructuredText syntax because there were some similarities between Markdown and reStructuredText (<code>rst</code>). The conference committee also provided <a href="https://raw.githubusercontent.com/hongsups/scipy_proceedings/2019/papers/00_bibderwalt/00_bibderwalt.rst">examples</a>. They also provided me with <a href="https://github.com/hongsups/scipy_proceedings/tree/2019/publisher">a PDF builder</a> that I could run locally. This helped me preview the final version of the PDF when it’s fully rendered. This was useful especially to decide the location of figures. There was a custom LaTeX builder in their repository too, which was invaluable.</p>
<p>Compared with existing word processor softwares such as MS Word or Google Docs, using markup languages pose some challenges to the first-time users. First, you need to learn the syntax. There are some limitations on styling because not everything is rendered automatically (e.g., LaTeX) and sometimes you need to play with html and css files to get a more complex page layout.</p>
<p>However, there are many benefits as well:</p>
<ul>
<li>Every styling is written explicitly (e.g., <code>**</code> for bold in Markdown) and thus it is easily discoverable.</li>
<li>Code snippets can be automatically rendered in a standardized way.</li>
<li>It’s easy and fast to publish the manuscript online, which makes it easy to share.</li>
<li>Finally, with a right document builder, the above-mentioned pain points can be mitigated.</li>
<li>It is much easier and convenient to review the manuscript (see below).</li>
</ul>
</section>
<section id="using-github-for-submission-and-review" class="level2">
<h2 class="anchored" data-anchor-id="using-github-for-submission-and-review">Using GitHub for submission and review</h2>
<p><img src="https://hongsupshin.github.io/posts/2020-11-24-github-manuscript-review/2020-11-24-HS_scipy_example_author_screenshot.png" title="Screenshot of my pull request at the SciPy 2019" class="img-fluid"></p>
<p><img src="https://hongsupshin.github.io/posts/2020-11-24-github-manuscript-review/2020-11-24-HS_scipy_example_reviewer_screenshot.png" title="Screenshot of the review conversation of a pull request I reviewed at the SciPy 2020" class="img-fluid"></p>
<p>Once I finished writing the manuscript, I submitted the main document and image files as a pull request. It was pretty straightforward because I was working alone in my own branch (you can find my pull request <a href="https://github.com/scipy-conference/scipy_proceedings/pull/468">here</a>). A few weeks later, two reviewers started commenting on my pull request. The whole process was quite simple and very similar to a typical review process in academia: reviewers left comments on certain parts of the manuscript and I responded. It also reminded me of a code review process. This year, I volunteered at the SciPy 2020 as a reviewer. I <a href="https://github.com/scipy-conference/scipy_proceedings/pull/550">reviewed a paper that was submitted as a pull request</a>.</p>
</section>
<section id="benefits-of-github-for-reviewing-manuscripts" class="level2">
<h2 class="anchored" data-anchor-id="benefits-of-github-for-reviewing-manuscripts">Benefits of GitHub for reviewing manuscripts</h2>
<p>With these experiences, I learned that markup language with GitHub review process provides many benefits for scientific writing, compared with MS Word or Google Docs which are used in typical (academic) review process:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 16%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Markup + GitHub review</th>
<th>MS Word or Google Docs + Academic review</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Discover exact changes</strong></td>
<td>Yes</td>
<td>Possible but can be confusing when many changes occur in one place</td>
</tr>
<tr class="even">
<td><strong>Change versions easily</strong></td>
<td>Yes</td>
<td>May need multiple versions (separate files)</td>
</tr>
<tr class="odd">
<td><strong>Track communications</strong></td>
<td>Yes</td>
<td>Challenging because comments happen in the margin, which has a very limited space. Otherwise, they are addressed in a separate space (like a letter).</td>
</tr>
<tr class="even">
<td><strong>Text override or unauthorized edits</strong></td>
<td>No (permission needed)</td>
<td>Can happen if tracking option is off or multiple files exist</td>
</tr>
<tr class="odd">
<td><strong>Group communication</strong></td>
<td>Yes</td>
<td>No.&nbsp;Normally reviewers don’t talk to each other. Reviewers communicate individually with authors.</td>
</tr>
<tr class="even">
<td><strong>Immediate responsiveness</strong></td>
<td>High (online comments)</td>
<td>Low. Letters or emails need to be exchanged, which can take months.</td>
</tr>
<tr class="odd">
<td><strong>Communication transparency</strong></td>
<td>High</td>
<td>Almost none (academic review). Normally double-blind. A reviewer can’t normally read other reviews during the review process.</td>
</tr>
<tr class="even">
<td><strong>Public transparency</strong></td>
<td>Yes (public repositories)</td>
<td>Almost none. Very rare to publish the whole conversation between reviewers and authors.</td>
</tr>
<tr class="odd">
<td><strong>One place for everything</strong></td>
<td>Yes</td>
<td>No.&nbsp;Normally non-document type files exist in a different place.</td>
</tr>
</tbody>
</table>
<p>In short, the main benefit of using GitHub for reviewing manuscript comes from that <strong>GitHub is an excellent version control tool</strong>. It prevents reviews (new changes) and the manuscript (original file) from getting mixed up and allows us to track changes meticulously.</p>
<p>Another huge benefit is <strong>transparency</strong>. If our repository and pull requests are public, the entire review process can be seen by anyone. This way, every decision making process is tracked and stays public, which minimizes potential dispute or abuse between authors and reviewers. Given that academia is not capable of perfect self-governance and some academic disciplines have reproducibility problems, transparent review process like this can become a solution.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final thoughts</h2>
<p>It is true that markup languages may not always be perfect for writing a manuscript. Some people are more familiar with traditional word processing softwares where styles are immediately rendered and more style options are available. However, based on my experience with the SciPy conference, especially, being at the both ends of the review process, I can say the benefits of using markup language for scientific writing and using GitHub for review outweighs its caveats. Manuscripts written in markup languages can be easily reviewed in GitHub where we can utilize its excellent built-in tools to achieve <strong>better tracking, communication, and transparency.</strong></p>


</section>

 ]]></description>
  <category>collaboration</category>
  <guid>https://hongsupshin.github.io/posts/2020-11-24-github-manuscript-review/</guid>
  <pubDate>Tue, 24 Nov 2020 06:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2020-11-24-github-manuscript-review/2020-11-24-HS_scipy_example_reviewer_screenshot.png" medium="image" type="image/png" height="79" width="144"/>
</item>
<item>
  <title>Efficient bug discovery with ML for hardware verification</title>
  <dc:creator>Hongsup Shin</dc:creator>
  <link>https://hongsupshin.github.io/posts/2020-09-22-arm-rsh-blog-verification/</link>
  <description><![CDATA[ 





<p>Imagine designing a highly complex machine. In order to be certain that it functions as its design specifies and does not have any bugs, you would need to test every aspect of the design exhaustively. If the machine is controlled by a set of knobs that can be turned on and off, this verification process can get exponentially complex. For instance, with a machine that has 100 binary on-off knobs, then 2<sup>100</sup> tests need to be run to cover all possible combinations. If we assume that a single test takes one second to run, this equates to 1022 years of testing. For present-day microprocessors, it is even more challenging. There can be thousands or tens of thousands of two-state flip-flops in a single microprocessor. Therefore, it is <strong>impossible to verify microprocessor designs exhaustively</strong>.</p>
<p>To work around this problem, hardware verification engineers use a method called <strong>random-constraint testing</strong>. This is more sophisticated than simple random adjustment of knobs – it is <strong>a hybrid approach of manual control and randomization</strong>. Engineers can direct the test behavior to a degree by setting constraints for the tests using adjustable knobs. Once constraints are set, the rest of the verification process depends on randomization; the knobs start subsequent processes that are stochastic, which stimulates various parts of the design. This way, engineers can explore the design randomly under the constraints they have set.</p>
<p>This method works well when engineers have not explored the design much. As engineers start exploring a design, they find and fix bugs, and the more they explore, the fewer bugs are left to be fixed. <strong>Eventually, it becomes very difficult to detect these rare bugs by random probing</strong>. Most of the hardware verification effort goes into finding the few remaining bugs in the design. In fact, this process is so time-consuming that <strong>60-70% of the compute time spent on hardware development goes into verification</strong>.</p>
<p>If the random-constraint testing is not good at specifically targeting bugs in the design, what are the alternatives? My team in <a href="https://www.arm.com/resources/research">Arm Research</a> has been working on this problem since last year. We have analyzed our CPU verification data and successfully trained machine learning (ML) models to solve this problem. We deployed an ML application with these models in collaboration with production engineers at Arm, <a href="https://www.linkedin.com/in/markkoob/">Mark Koob</a> and <a href="https://www.linkedin.com/in/ramachandranswati/">Swati Ramachandran</a>. Our application <strong>uses ML to flag tests that are likely to find bugs</strong>. Verification engineers can feed a large group of prospective tests to our application, and then the application returns a subset of them that are likely to find bugs. This way, <strong>engineers can focus on these tests only, and reduce the number of tests to run, which eventually saves compute costs.</strong></p>
<p>Currently, our application is being <strong>used consistently by Arm verification engineers as a complementary tool to their existing workflow</strong>. On average, it has been shown to be <strong>25% more efficient</strong> than the default verification workflow in terms of finding bugs, and <strong>1.33x more efficient</strong> at finding unique bugs. Through our application, engineers were able to find more unique bugs per test run when compared to their existing verification workflow.</p>
<section id="filtering-prospective-tests-using-ml" class="level2">
<h2 class="anchored" data-anchor-id="filtering-prospective-tests-using-ml">Filtering prospective tests using ML</h2>
<p><img src="https://hongsupshin.github.io/posts/2020-09-22-arm-rsh-blog-verification/2020-09-22-random-constraint-testing-diagram.png" title="Schematic of random-constraint testing in hardware verification" class="img-fluid"></p>
<p>Now let us take a deeper look at how our application works. The main challenge of using ML for hardware verification is that everything revolves around random-constraint testing. Not only is the data collected by this method, but also the testing infrastructure is built to optimize this process. The problem is that random-constraint testing consists of two very different parts:</p>
<ul>
<li><strong>Deterministic knob-control</strong> by verification engineers, and;</li>
<li><strong>Non-deterministic subsequent processes</strong> that are random and intractable.</li>
</ul>
<p>This means two things. Firstly, the only source of the usable data is the knobs. Secondly, we cannot guide tests directly to explore new design space because the whole process is non-deterministic.</p>
<p>Let’s first look at the knob data. Here, each sample is a test that was run in the past. It has <strong>several hundred knob values (input) and a binary output; bug or bug-free</strong>. In our data, bugs were extremely rare (less than 1%). To address this <strong>severe class imbalance</strong>, we adopt two approaches. Firstly, we train a <strong>supervised learning</strong> model that computes a probability of having a bug based on a set of knob values. This model detects tests that may expose bugs similar to the previous ones. Secondly, we train an <strong>unsupervised learning</strong> model that estimates similarity between a new test and the previous tests. If the similarity is low, the test is likely to be novel. Novel knob combinations can probe unexplored design areas and are more likely to expose bugs. In our <a href="http://conference.scipy.org/proceedings/scipy2019/pdfs/Hongsup_Shin.pdf">preliminary results</a>, we found that these models can detect different types of bugs. Since our main goal is to capture as many bugs as possible, we flag a test as a bug provided one of the two models predicts it will be.</p>
<p>To avoid the difficulty in guiding test behavior, we choose a <strong>filtering approach</strong>. We leave the knob values to be generated randomly from the testing infrastructure and filter them afterwards based on ML prediction scores. To do so, we need to provide the ML models with a large group of knob values (test candidates) first. Luckily, this process is computationally cheap. Then, the ML models compute prediction scores (the probability of having a bug) of the candidates. Based on the scores, we select a subset of the candidates that are more likely to find bugs than others.</p>
</section>
<section id="deploying-ml-within-existing-random-constraint-testing-infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="deploying-ml-within-existing-random-constraint-testing-infrastructure">Deploying ML within existing random-constraint testing infrastructure</h2>
<p>Now that we have trained ML models ready, can we completely replace our existing random-constraint testing flow with our application? The answer is no. <strong>The filtering approach, even with the unsupervised learning model, does not completely solve the exploration problem</strong>. That is why the existing flow (random-constraint testing) should remain. The random probing can still be useful for exploration to a degree, and can provide new training data for model update. Thus, we propose two parallel pathways; one with the default randomized testing and the other with ML models, where an additional set of test candidates are provided and then only the tests flagged by the models are filtered and run. This way, it is possible to continue collecting novel data from the default flow for exploration while exploiting the data from previous tests via the ML application.</p>
<p>Some may think that our job is done when the models are delivered, and deployment finally happens. This is not true. For the models, a new journey begins when they are deployed in the production environment, because it is likely that unexpected events are waiting for them. For instance, we have learned that design and testing infrastructure go through daily changes, so test behavior and data generation process may change frequently. This means the models deployed in the beginning do not guarantee good performance as time goes by.</p>
<p>To address this, we have <strong>conducted research into the optimization of model retraining</strong>. We identified how often models need to be retrained and how much data should be used for training. Using this information, we’ve built a retraining module that is automatically invoked periodically or upon verification engineers’ request. During the retraining process, we compare a variety of different models and tune their hyperparameters. This allows for flexibility across changes in data generation processes and various microprocessor designs.</p>
</section>
<section id="towards-end-to-end-ml" class="level2">
<h2 class="anchored" data-anchor-id="towards-end-to-end-ml">Towards end-to-end ML</h2>
<p>When we talk about ML, the focus is often on algorithms. As I mentioned, however, <strong>when it comes to building ML products, the algorithms are only a small part of a much larger pipeline</strong>. Especially with scalable deployment in consideration. Starting from data acquisition all the way to deployment and assessment, every step requires attention of data scientists and ML researchers. It is because developing ML products is different from typical software development. A lot of processes are interconnected and data-dependent, which makes them more challenging to test and evaluate.</p>
<p>Admittedly, we initially approached the development of this ML application as a “throw it over the wall” type of engagement. After developing the core ML modules (data preprocessing and models), we delivered them to production engineers without much engagement afterwards. After the application was deployed in several projects, we occasionally needed to intervene to deal with unexpected model behavior. We soon realized that not engaging in the deployment process makes it very difficult to solve any issues that happen after model deployment.</p>
<p>Recently, in collaboration with our colleagues, it has been made easier to make direct contributions to packages. They have developed a Python package for data science and ML, which has become a core part of our ML application. This allowed us to directly contribute to the ML application easily, and also enabled scalable Python in Arm’s internal compute platform. Our new team member, <a href="https://www.linkedin.com/in/kate-behrman/">Kathrine (Kate) Behrman</a> has already built a new module to fix an existing problem in model validation. With this momentum, we are more engaged in deployment to make our ML application perform better and more reliable, while exploring new research ideas.</p>
<p>Through this process, we learned that <strong>getting more involved in ML product development provides many benefits to researchers</strong>. First, it makes <strong>tech transfer much easier</strong> because making direct contributions to deployed products is more frictionless. This also means that we can <strong>test our new research ideas easily</strong> and measure the effect quickly. In addition, it helps us <strong>fix problems efficiently</strong> because we have better understanding of how the models are served in deployment. <strong>ML products benefit from an evolutionary approach</strong> because there is no absolute guarantee that the data or surrounding infrastructure stays the same. Finally, this process naturally <strong>brings automation and scalability</strong>, which makes our work easily applicable and more impactful.</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>We are currently working on launching a new version of our ML application to accommodate a new CPU project that has launched recently. We expect that the new version can be used in the early stage of verification, which is new territory to us. At the same time, we are exploring various research areas. For example, explainable AI for feature assessment and counterfactuals, various unsupervised learning techniques to target novel bugs more effectively, methods to design ML-friendly test benches, and other verification-related problems such as assessing coverage. We are also putting efforts into standardizing our workflow and automating existing features. We anticipate that our work will bring more consistent and reliable model performance over time. We also expect that it will showcase a successful tech transfer example for ML deployment that can be applied to solving other engineering problems inside Arm with ML.</p>
<p>Learn more about our research in our paper presented at <a href="https://www.dac.com/">DAC (Design Automation Conference) 2020</a>, and please do reach out to me if you have any questions!</p>


</section>

 ]]></description>
  <category>ML</category>
  <category>verification</category>
  <guid>https://hongsupshin.github.io/posts/2020-09-22-arm-rsh-blog-verification/</guid>
  <pubDate>Tue, 22 Sep 2020 05:00:00 GMT</pubDate>
  <media:content url="https://hongsupshin.github.io/posts/2020-09-22-arm-rsh-blog-verification/2020-09-22-random-constraint-testing-diagram.png" medium="image" type="image/png" height="85" width="144"/>
</item>
</channel>
</rss>
