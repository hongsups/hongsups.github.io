{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15097d0b-99a9-4b97-b614-d7e900b49f0f",
   "metadata": {},
   "source": [
    "# Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c2cc2-50ec-4cb8-ac61-fdaffa2a01c1",
   "metadata": {},
   "source": [
    "## How it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e7f68-8d93-4286-8d63-86698d6fa48c",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- objective function\n",
    "- space as a dict\n",
    "    - key: param name\n",
    "    - value: range\n",
    "- `tune.Tuner`\n",
    "\n",
    "How it works\n",
    "```python\n",
    "results=tune.fit()\n",
    "results.get_best_result(metric=\"score\", mode=\"min\").config`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15f8ef-1b00-49bf-9a4c-8f91627d9f77",
   "metadata": {},
   "source": [
    "## Key concepts\n",
    "6 components\n",
    "- search space: passed to trainable\n",
    "- trainable: objective function\n",
    "- search algo: how to tune\n",
    "- schedular: when to stop\n",
    "- trials: run exp (trainable, search algo, scheduler passed to tuner)\n",
    "- analyses: tuner ResultGrid\n",
    "\n",
    "### trainable\n",
    "- use Functional API\n",
    "    - create a function (here called `trainable`) that takes in a dictionary of hyperparameters.\n",
    "    - This function computes a score in a “training loop” and reports this score back to Tune\n",
    "```python\n",
    "from ray import train\n",
    "\n",
    "def objective(x, a, b):  # Define an objective function.\n",
    "    return a * (x**0.5) + b\n",
    "\n",
    "def trainable(config):  # Pass a \"config\" dictionary into your trainable.\n",
    "\n",
    "    for x in range(20):  # \"Train\" for 20 iterations and compute intermediate scores.\n",
    "        score = objective(x, config[\"a\"], config[\"b\"])\n",
    "        train.report({\"score\": score})  # Send the score to Tune.\n",
    "\n",
    "```\n",
    "- `session.report`: report the intermediate score in the training loop\n",
    "- trainable can have a `for` loop = iterations\n",
    "\n",
    "### search space\n",
    "- usually called `config`\n",
    "- bunch of distributions\n",
    "\n",
    "### trials\n",
    "- first arg: trainable\n",
    "- `param_space`: search space config\n",
    "- `Tuner.fit` generates trial objects\n",
    "- *Tune automatically determines how many trials will run in parallel.*\n",
    "    - can specify num_samples: `tune_config=tune.TuneConfig(num_samples=10)`\n",
    "    - or time budget: `time_budget_s`\n",
    "\n",
    "### search algo\n",
    "- random search by default\n",
    "- Tune has Search Algorithms that integrate with many popular optimization libraries, such as HyperOpt or Optuna.\n",
    "- bayesian-optimization\n",
    "    - `pip install bayesian-optimization` first\n",
    "- becomes a part of `tune_config`\n",
    "```python\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "algo = BayesOptSearch(random_search_steps=4)\n",
    "tuner = tune.Tuner(\n",
    "    trainable,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"score\",\n",
    "        mode=\"min\",\n",
    "        search_alg=algo,\n",
    "    ),\n",
    "    run_config=train.RunConfig(stop={\"training_iteration\": 20}),\n",
    "    param_space=search_space,\n",
    ")\n",
    "```\n",
    "\n",
    "### scheduler\n",
    "- more efficient training/tuning\n",
    "    - schedulers can stop, pause, or tweak the hyperparameters of running trials, potentially making your hyperparameter tuning process much faster\n",
    "- allows early stopping\n",
    "    - example: Median Stopping Rule, HyperBand, and `ASHA`.\n",
    "- use a first-in-first-out (FIFO) scheduler by default\n",
    "    - simply passes through the trials selected by your search algorithm in the order they were picked and **does not perform any early stopping**\n",
    "- note: Certain schedulers cannot be used with search algorithms, and certain schedulers require that you implement checkpointing.\n",
    "\n",
    "### ResultGrid\n",
    "- Tuner.fit() returns an ResultGrid object which has methods you can use for analyzing your training.\n",
    "```python\n",
    "best_result = results.get_best_result()  # Get best result object\n",
    "best_config = best_result.config  # Get best trial's hyperparameters\n",
    "best_logdir = best_result.path  # Get best trial's result directory\n",
    "best_checkpoint = best_result.checkpoint  # Get best trial's best checkpoint\n",
    "best_metrics = best_result.metrics  # Get best trial's last results\n",
    "best_result_df = best_result.metrics_dataframe  # Get best result as pandas dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06954b01-f345-432c-9083-805b6a43a868",
   "metadata": {},
   "source": [
    "### others\n",
    "- [passing additional params](https://docs.ray.io/en/latest/tune/faq.html#id10)\n",
    "    - `tuner = tune.Tuner(tune.with_parameters(f, data=data))`\n",
    "- set random seed: use both?\n",
    "```python\n",
    "random.seed(1234)\n",
    "np.random.seed(5678)\n",
    "```\n",
    "- `ray.tune.TuneConfig`: metric, mode (objective goal: `min` or `max`), search_alg, scheduler, num_samples, time_budget_s, \n",
    "- `ray.tune.RunConfig`: Runtime configuration for training and tuning runs.\n",
    "    - storage_path, storage_filesystem, failure_config, checkpoint_config, sync_config, verbose (1=default), stop, callbacks (DeveloperAPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33397ef-8d02-47f5-aef1-7127fd35e8e4",
   "metadata": {},
   "source": [
    "### callbacks\n",
    "- https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html\n",
    "- callbacks are called during various times of the training process\n",
    "- Callbacks can be passed as a parameter to RunConfig\n",
    "```python\n",
    "class MyCallback(Callback):\n",
    "    def on_trial_result(self, iteration, trials, trial, result,\n",
    "                        **info):\n",
    "        print(f\"Got result: {result['metric']}\")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_func,\n",
    "    run_config=train.RunConfig(\n",
    "        callbacks=[MyCallback()]\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27e2c2-6e1f-4309-928f-28cf8c0e7675",
   "metadata": {},
   "source": [
    "## defining trainable \n",
    "- traininable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c19152-73c4-405f-8cd5-e38090a0ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.lightgbm import TuneReportCheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd6079-1930-401b-8d24-818a80f2a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_breast_cancer(config):\n",
    "\n",
    "    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.25)\n",
    "    train_set = lgb.Dataset(train_x, label=train_y)\n",
    "    test_set = lgb.Dataset(test_x, label=test_y)\n",
    "    gbm = lgb.train(\n",
    "        config,\n",
    "        train_set,\n",
    "        valid_sets=[test_set],\n",
    "        valid_names=[\"eval\"],\n",
    "        verbose_eval=False,\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                {\n",
    "                    \"binary_error\": \"eval-binary_error\",\n",
    "                    \"binary_logloss\": \"eval-binary_logloss\",\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    preds = gbm.predict(test_x)\n",
    "    pred_labels = np.rint(preds)\n",
    "    train.report(\n",
    "        {\n",
    "            \"mean_accuracy\": sklearn.metrics.accuracy_score(test_y, pred_labels),\n",
    "            \"done\": True,\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
