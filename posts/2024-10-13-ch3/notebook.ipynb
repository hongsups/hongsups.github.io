{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88458fac-26f7-4831-a475-4b453f20d519",
   "metadata": {},
   "source": [
    "# Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20cde3-e507-44a5-b6ec-75fce8ef5b5c",
   "metadata": {},
   "source": [
    "- simplifiled self-attention\n",
    "- self-attention\n",
    "- causal attention\n",
    "    - the model considers only previous and current inputs in a sequence, ensuring temporal order during the text generation\n",
    "- multi-head attention\n",
    "    - extention of causal\n",
    "    - the model simultaneously attend to information from different representation subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e27817-3df9-4871-9266-c074064ae6fa",
   "metadata": {},
   "source": [
    "## the problem w/ modeling long sequences\n",
    "\n",
    "pre-LLM architectures without attention: can't simply translate a text word by word due to the grammatical structure difference between the source and target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a20cd8-665a-4988-8225-bc97399f5ceb",
   "metadata": {},
   "source": [
    "To address this, it's common to use encoder-decoder architecture\n",
    "- pre-transformers: RNN as a popular choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885d8581-e652-4ed7-955e-bfcc2061be2a",
   "metadata": {},
   "source": [
    "encoder-decoder RNN\n",
    "- encoder\n",
    "    - input -> encoder (processed sequentially)\n",
    "    - updates hidden state (keeps getting updated till the final word)\n",
    "    - updated hidden state also becomes the input (concat)\n",
    "- decoder\n",
    "    - use the final hidden state to generate output one at a time\n",
    "    - its own hidden state gets updated (carrying context)\n",
    "- key idea\n",
    "    - encoder processes the entire input into a hidden state (**memory cell**)\n",
    "    - decoder takes this to produce the output\n",
    "- limitations\n",
    "    - RNN can't directly access earlier hidden state (it just keeps getting updated)\n",
    "    - loss of context especially for longer sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67cde0-b22f-402c-bd1d-153a8faddb12",
   "metadata": {},
   "source": [
    "## capture data dependencies with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e4afb-337c-4412-9513-836769ba61e6",
   "metadata": {},
   "source": [
    "54-64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55c33b-e96d-49d3-96ae-78f6474bbc0b",
   "metadata": {},
   "source": [
    "Bahdanau attention mechanism (2014): decodere can selectively access different parts of input seq at each decoding step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cdc9ae-d193-451f-9f35-e0ccd6952a7b",
   "metadata": {},
   "source": [
    "Transformer's self-attention\n",
    "- inspired by Bahdanau algo\n",
    "- allows each position in the input to consider the relevancy of other positions in the same seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de3b2d1-18d9-4c29-bfc3-9a70a094a849",
   "metadata": {},
   "source": [
    "## attending to different parts of input w/ self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd40a791-8571-4b3e-bda1-366658195d9f",
   "metadata": {},
   "source": [
    "why \"self\"\n",
    "- the mechanism can compute attention within a single input seq\n",
    "- assess relationship and dependencies btw various parts of the input itself\n",
    "- different from traditional attention (even from Bahdanau) where attention involves different seq (e.g., input and output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a8b81-193d-4237-a830-b7ec20294b56",
   "metadata": {},
   "source": [
    "### 1. simple self-attention w/o trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e3480f-adfc-4de2-ac1d-ff2f61da4fdd",
   "metadata": {},
   "source": [
    "goal of self attention: calculate context vectors for each element in the input sentence\n",
    "- context vector\n",
    "    - enriched embedding: incorporate informatoin from all other elements in the seq (crucial for LLM!)\n",
    "    - all inputs x relative weights\n",
    "    - weights are different across items in the input seq (relative weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6061c73-60de-4778-b90c-eee89cf09506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your    (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "     [0.22, 0.58, 0.33], # with    (x^4)\n",
    "     [0.77, 0.25, 0.10], # one     (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step    (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e79051-89c1-4007-b126-7e4a743bf0cb",
   "metadata": {},
   "source": [
    "step 1: compute the attention score, $w$\n",
    "- dot product of the input and query -> scalar (inner product)\n",
    "- dot product = measure of similarity\n",
    "- higher attention score: more similar elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed457795-15c7-4f1f-8e1b-e649840c0ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # second input\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2) # score per input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735d34f-10b1-4bc3-8b32-507ccfd0757f",
   "metadata": {},
   "source": [
    "step 2. normalize the scores: normalized weights, $\\alpha$ (normally we use softmax: always positive, probability/relative importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ff6733-b622-44e4-b5ca-3aebe72c8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8186d54e-a85a-4fe2-ad4a-ae05e18d6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fc5b9-b903-4e2f-b031-7808bb8ae67c",
   "metadata": {},
   "source": [
    "naive implementation -> possibly numerically instable (oevrflow or underflow); so use pytorch\n",
    "- underflow: When integers near zero are rounded to zero, underflow occurs. When the argument is zero instead of a small positive number, many functions act qualitatively differently. (e.g., division by zero)\n",
    "- overflow: When numbers of enormous magnitude are approximated as -\\infty    or \\infty, overflow occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a46841-30ad-4957-a349-397e0b559f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5abca5f4-20b5-45df-a9c8-5b3a935b1cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acceb4f-c449-4ef0-bcab-a422965ead7b",
   "metadata": {},
   "source": [
    "step 3: calculate the context vector (multiply weights to input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661c5a7a-42c1-4348-9d15-5c23351a06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e431b2-25ce-4fe7-adaa-eacf5de55319",
   "metadata": {},
   "source": [
    "### 2. compute attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a0cd2-b21e-4c11-a8ee-92ee1c0ef74d",
   "metadata": {},
   "source": [
    "1. attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6091685-30cf-4546-a2c5-45ca29f23903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901948e-ad15-48b9-b3c8-af0b3cfa874d",
   "metadata": {},
   "source": [
    "for loop is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36229f8d-8866-4aad-bd55-bc5950c7bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b6813-564a-4bd9-93b6-79568c1a739a",
   "metadata": {},
   "source": [
    "2. attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f6ba4-05a0-4b94-ae71-ebce6ac3ceaf",
   "metadata": {},
   "source": [
    "normalize along the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49be725-4fc5-4f28-9aa6-1aa6d59b5a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1) # dim=-1, last dimension\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a289875-38ae-4a83-a30d-57d030b3c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980dc862-da21-4aac-855f-337bd9a406d7",
   "metadata": {},
   "source": [
    "3. context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d331bd9-066a-405c-8359-25f5a1c03e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs # 6x6 * 6*3\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c1e0c0-654f-40a9-990e-8324e84ec39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968a339-8e46-457e-9c3d-2602964f2bae",
   "metadata": {},
   "source": [
    "## Implement self attention w/ trainable weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e1120-26ac-4c6a-808a-282c245f071b",
   "metadata": {},
   "source": [
    "64-74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58543ef8-ba40-406c-8743-78118d62fc8e",
   "metadata": {},
   "source": [
    "self-attention for GPT: **scaled dop-product attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345cae2-63c3-45cf-88aa-79cb7f5c1f6d",
   "metadata": {},
   "source": [
    "weight matrices are updated during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbad07a-67da-497b-8c95-68cf12351556",
   "metadata": {},
   "source": [
    "### 1. compute attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2352765-0900-4d60-8b1b-1e0222090835",
   "metadata": {},
   "source": [
    "3 trainiable weight matrics: $W_q, W_k, W_v$: query, key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b024802-bd90-4650-afbe-0f881f54c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second element\n",
    "d_in = inputs.shape[1] # input embedding size\n",
    "d_out = 2 # output embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15335bb6-6b89-4ce0-a89c-410c85fdb61a",
   "metadata": {},
   "source": [
    "note: usually in GPT-like models, input and output embedding sizes are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89d56c-fabf-4d63-8d7c-62ef11566b12",
   "metadata": {},
   "source": [
    "1. Initialize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f354c857-84ff-4bf4-b1c5-afcb1cd79874",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc5dc9-8f9e-426a-bf2e-7a24d51f7ccc",
   "metadata": {},
   "source": [
    "requires_grad=True -> trainable (for now false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c1c784-4c93-4a2b-80c3-15407bfec4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a667fd7d-54e0-41ca-bbf2-caf2fe271d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "730a6875-97dd-4784-b76f-35e8869a9341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812afa6-ef62-4827-b257-1c34fde9790f",
   "metadata": {},
   "source": [
    "2. compute the attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d877300-4118-4881-ae73-72c3ea28d927",
   "metadata": {},
   "source": [
    "score $w_{22}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b94aa18-4385-451c-a81b-d86f138476dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b61515b-b903-48e8-8c64-e65816ed7d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2549e-23a3-4522-8eef-7e250730256a",
   "metadata": {},
   "source": [
    "3. compute the attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a59e681-9c98-4781-a6fe-7bf593f90c7b",
   "metadata": {},
   "source": [
    "here, scaling: softmax of (scores divided by the sqrt of key dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34a95ed2-3732-4389-bb45-86ad02f8c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb161eb7-93fb-4968-b249-ae6761cc073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.25, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10de7ade-226f-4c01-a10e-f63fadeb2c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1456, 0.2376, 0.2295, 0.1240, 0.0800, 0.1833])\n"
     ]
    }
   ],
   "source": [
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2748ba60-0cf3-477a-ba86-7148b0894997",
   "metadata": {},
   "source": [
    "why scaled-dot product attention\n",
    "- normalizatoin: avoid vanishing gradients\n",
    "- large dot products -> can cause small gradients during backprop (due to softmax)\n",
    "- softmax becomes more like a step function with big dot product\n",
    "- reason why we do the sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b559c64-7947-4033-80ce-63e3dc9e22d8",
   "metadata": {},
   "source": [
    "4. compute the context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1926a1f9-aebf-4bb9-86aa-4d0b14127d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3106, 0.8314])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4815950-dcb5-44c1-902d-6402b4a7ce4b",
   "metadata": {},
   "source": [
    "why query, key, value\n",
    "- terms from information retrieval and database domains\n",
    "- query: \"search query\" = current item that model focuses on = how much attention to pay to them\n",
    "- key: db key for index and search\n",
    "- value: key-value pair; representation of the input items; once the model determines which keys are most relevant to the query, it retrieves the corresponding values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309b22b-eb64-4f93-885b-4256e9d98939",
   "metadata": {},
   "source": [
    "### 2. implement a compact self-attention python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c381854a-d56c-42c5-9512-4cd67602d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T # omega?\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd75a81d-7acd-44f7-82f6-610bdae957d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156dc96-f604-40fd-ba6d-874b51422608",
   "metadata": {},
   "source": [
    "improve this w/ `nn.Linear`\n",
    "- does matmul when the bias are disabled\n",
    "- has an optimized weight initialization scheme (more stable training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44e94b9a-88c2-46f9-98db-3843965f7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdf2fc63-69df-4beb-ba9f-1c7ad40d43c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2626a165-b84c-47dc-9fcb-25ecca32944a",
   "metadata": {},
   "source": [
    "## Hiding future words with causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e1af9-9150-4fe9-8f76-9ffa1ec3a45b",
   "metadata": {},
   "source": [
    "74-78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec6173-3fcc-4096-8c6d-66da3a64e3e6",
   "metadata": {},
   "source": [
    "### 1. Applying a causal attention mask\n",
    "causal attention aka \"masked\" attention\n",
    "- special form of self-attention\n",
    "- model can only access previous and current inputs (not the entirety)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f00bfc-5915-4717-aa13-302ceb5898ac",
   "metadata": {},
   "source": [
    "1. Compute the attention weights (normally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48346d34-abad-43bb-a39c-5ef17b1b81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b1a6e-4400-40ca-948a-f969af8dc95f",
   "metadata": {},
   "source": [
    "2. create and apply the mask (`torch.tril`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d60c0f9-561b-4b9d-962c-6915a9296927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "420738ff-d717-4eb0-a3cc-b7d5c8f1f4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b097877f-0148-4064-9d5d-cae4c47165ff",
   "metadata": {},
   "source": [
    "3. renormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "397a703e-7ae0-4c48-90d5-676ad1ea9601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7b579-043f-4701-892b-0cfcff4f7f28",
   "metadata": {},
   "source": [
    "information leakage\n",
    "- because we first compute the attention weights using all words (before applying masking), it might appear that there's info leakage\n",
    "- but when we apply softmax, it's the same as if we did this without using all words\n",
    "- after masking and renormalization, the distribution of attention weights is as if it was calculated among the unmasked positions to begin with. this ensures no information leakage from future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50679741-ce98-4600-a78e-676a4a2b2540",
   "metadata": {},
   "source": [
    "more efficient way of applying softmax: use the fact that $e^{-\\infty}$=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a49ad956-4119-4cab-8ff3-06c7b0ca2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5d93467-0411-4b7a-ab4d-aee4b28c21e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dac1dc6-631d-4d07-868f-a51a502154a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729bc4d-124e-47da-934b-4cec39a5d29f",
   "metadata": {},
   "source": [
    "### 2. Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679e47e-beac-49e3-883e-0776de06265a",
   "metadata": {},
   "source": [
    "78-88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8686ea0-a399-436d-af28-d33379bbbb07",
   "metadata": {},
   "source": [
    "dropout is applied twice\n",
    "- after calculating the attention weights\n",
    "- after applying the attention weights to the value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cdc4f00-c74e-4682-8d57-0943c6af880a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d4340-e7cf-4c08-aad2-5a96a4588945",
   "metadata": {},
   "source": [
    "50% dropout: half of them are masked, to compensate, half of them are scaled up x2 -> the scaling is crucial to maintain the overall balance of attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4ff4f4e-3fd6-4aea-b9ae-b8c39b4ac96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a573c-c949-41f8-a0ed-eddfb9554385",
   "metadata": {},
   "source": [
    "### 3. Implementing a compact causal attention class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd8717-67a4-4994-a52c-fa224c4ad1d7",
   "metadata": {},
   "source": [
    "write the class that supports batch approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc8522f-e263-42e6-91fe-67ff03f99559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f535f951-2b10-4cdf-a0c6-3a5f3a785f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "        dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # batch size, num token, embedding dim\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d25567-bf0c-4736-86be-d4ef196742a3",
   "metadata": {},
   "source": [
    "advantages of adding `register_buffer` in init: avoid device mismatch error\n",
    "- buffers are automatically moved to the device with the model; relevant when training LLM\n",
    "- no need to manually ensure the tensors are on the same device as model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fdd4038-829e-4ad7-8460-f04f4177ad5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f1118-494a-4ca5-9ca3-f21da98467a5",
   "metadata": {},
   "source": [
    "- 2 sample in the batch\n",
    "- 6 tokens\n",
    "- 2-dim embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eceb2abb-69f6-439a-ad21-5d554eb74c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, d_out, context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a51352-eddd-462f-bb22-672da9fc0d8d",
   "metadata": {},
   "source": [
    "## multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37666c3f-2bc0-4700-817b-34e5b2a39825",
   "metadata": {},
   "source": [
    "= dividing the attention mechanism into multiple “heads,” each operating independently\n",
    "\n",
    "= multiple sets of attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f69286-1d3b-4cd1-a079-d88642ed5660",
   "metadata": {},
   "source": [
    "### 1. Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978ea41-d6d4-4b1e-b0b7-5d687812892f",
   "metadata": {},
   "source": [
    "Create multiple instances of self-attention\n",
    "- each w/ its own weights and combine their outputs\n",
    "- multiple instances of self-attention: compute-intensive\n",
    "- crucial for transformer-based LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd2d41-a207-44e9-9803-2a7b9e2def23",
   "metadata": {},
   "source": [
    "run the attention multiple times in parallel w/ different, learned linear projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a842d7f6-827e-41d5-932e-ef728d372032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(\n",
    "                d_in, d_out, context_length, dropout, qkv_bias\n",
    "            ) for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb05d4-4bd4-4586-b89a-43c0bc3729cb",
   "metadata": {},
   "source": [
    "note that the context vector from different heads are later concatenated: final embedding dim = num_heads x embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53750036-f46b-4576-ae68-5b34675de611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # = number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611e4a8-87f4-4cb4-bb47-8b9b31e25f61",
   "metadata": {},
   "source": [
    "- 2: two input texts\n",
    "- 6: num tokens\n",
    "- 4: final embedding size (num heads x embedding size of original input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7ea27-efe7-4a1c-99de-3379eead6766",
   "metadata": {},
   "source": [
    "in the above example, we processed attention sequentially but we can do it in parallel way by using matmul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d0b4c-399e-40c1-ab7e-d56e16aac59e",
   "metadata": {},
   "source": [
    "### 2. Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff5a20-d0f2-4d97-8d48-b9c4f766ef3b",
   "metadata": {},
   "source": [
    "- split the input into multiple heads by reshaping the project query, key, value\n",
    "- then combine the results from the heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e96aa3-2e3e-42b8-86bf-4b2347766024",
   "metadata": {},
   "source": [
    "86-96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "627c727d-683a-499a-b183-18c91d9f44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 context_length, dropout, num_heads, \n",
    "                 qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # reduce the projection dim to match the desired output dim\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # uses a linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # reshape the matrix (split) by adding num_heads dimension\n",
    "        # then we unroll the last dim (b, num_tokens, d_out) -> b, num_tokens, num_heads, head_dim\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)        \n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(2, 3) # compute the dot product of each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] # masks truncaked to num_tokens\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) # shape: b, num_tokens, n_heads, head_dim\n",
    "        # combines heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # add an optional linear projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce990d-ed08-435e-b54b-be41a3177423",
   "metadata": {},
   "source": [
    "- Stack multiple single-head attention into a multi-head attention\n",
    "- Integrated approach: starts w/ a single multi-head layer, then internally splits this into individual attention heads (n heads) -> splitting of key, value, query tensors\n",
    "- Key operation\n",
    "    - Split `d_out` to `num_heads` and `head_dim` (`head_dim = d_out / num_heads `): `(b, num_token, d_out) -> (b, num_token, num_heads, head_dim)`\n",
    "    - Transpose the last two dims of the tensors and compute the atten_scores: `(b, num_token, num_heads, num_heads)`\n",
    "- More efficient than the sequential approach: just need to do the matmul once\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecf7c305-55a0-43b7-92fb-a4aa83130633",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f732dbed-d06b-40fc-8c5d-5533237fcb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c060adcb-6665-4d7d-9580-7b3040d81495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "80e2a96b-e5f5-4e9c-a2b8-c321e64bd3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7e07e-5a1b-497d-8b26-42f151df33f9",
   "metadata": {},
   "source": [
    "Comparison\n",
    "- smallest GPT-2 model (117M params)\n",
    "    - 12 attention heads\n",
    "    - context vector embedding size of 768\n",
    "- largest GPT-2 model (1.5B params)\n",
    "    - 25 attention heads\n",
    "    - context vector embedding size of 1600\n",
    "    - the embedding sizes of the token inputs and context embedding are the same (d_in = d_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
