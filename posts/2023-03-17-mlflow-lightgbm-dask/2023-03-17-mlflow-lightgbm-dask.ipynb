{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d743a55-2309-48bd-a26e-c7f142a80b5f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Interoperability testing for hyperparameter tuning: MLflow, LightGBM, sklearn, and dask-ml\"\n",
    "date: \"2023-03-17\"\n",
    "description: \"MLflow autologging allows monitoring LightGBM training loss during model training. This behavior is not always expected when we use scikit-learn and dask to tune LightGBM models. This notebook describes how the unexpected behavior manifests and explains some gotchas when using these tools together.\"\n",
    "categories:\n",
    "  - ML\n",
    "  - MLOps\n",
    "  - MLflow\n",
    "  - Interoperability\n",
    "image: \"images/index.png\"\n",
    "author: Hongsup Shin\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "---\n",
    "\n",
    "There are numerous open source ML packages in python ecosystem. Developers do their best to maximize interoperability in relation to other main ML packages but it's not possible to check every possible combination. That's why I think some of the responsibility of interoperability lies on users. MLflow's autologging method is quite handy because with a single line of code (`mlflow.autologging`), we obtain useful metrics of model behavior such as confusion matrix, feature importance, or training loss over epochs. However, this is not always guaranteed when we apply model tuning on top by using scikit-learn and dask.\n",
    "\n",
    "In this notebook, I first demonstrated what MLflow autologging method did particularly for LightGBM models. Then, I tried the same autologging in model tuning frameworks of scikit-learn and Dask-ML backend, and how the autologging method behaves. Check `environment.yml` to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9100bba-fd42-4807-bb14-6e85953611a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Hongsup Shin\n",
      "\n",
      "Last updated: 2023-03-19 23:00:45\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.16\n",
      "IPython version      : 8.10.0\n",
      "\n",
      "Git hash: 0eaf0c3c88c3e909b76c36af9b13fea1f04d7c08\n",
      "\n",
      "Git repo: https://github.com/hongsupshin/hongsupshin.github.io.git\n",
      "\n",
      "Git branch: 1-mlflow\n",
      "\n",
      "lightgbm: 3.3.2\n",
      "sklearn : 1.2.0\n",
      "numpy   : 1.24.0\n",
      "mlflow  : 2.1.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.client import MlflowClient\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, PredefinedSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from dask_ml.model_selection import RandomizedSearchCV as dask_RandomizedSearchCV\n",
    "from distributed import Client\n",
    "\n",
    "seed = 97531\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -d -t -u -v -g -r -b -iv -a \"Hongsup Shin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60587416-ac76-4996-8741-55df7659dd50",
   "metadata": {},
   "source": [
    "<center><img src=\"images/index.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a3a19-5ba9-4cad-adc4-d0cf49f8744e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00e69886-2d48-4361-80d8-4ae0fcb153a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e46620d-0f70-4382-b866-ebc715669247",
   "metadata": {},
   "source": [
    "### MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f30756b-a1d8-44e3-8868-959735e7dbcc",
   "metadata": {},
   "source": [
    "MLflow comes with a tracking UI, which you can launch by running `mlflow ui`. By default, you can see the UI http://localhost:5000. Here, I assumed that you ran `mlflow ui` before the following cell where experiment location was defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3741ce1c-7adb-4fdd-83f4-a7a024abd900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/618881380489654419', creation_time=1679103932467, experiment_id='618881380489654419', last_update_time=1679103932467, lifecycle_stage='active', name='mlflow_tune_demo', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"mlflow_tune_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e4507b-1ba9-4c04-acd6-00a366f80584",
   "metadata": {},
   "source": [
    "`mlflow.autolog()` should be called before running training but this enables all supported libraries that are imported. Thus, specific autologging is recommened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c14ad0-99a1-45f6-b313-50b1c76af24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.lightgbm.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4693f9-3e97-46b8-a071-a60c704d57b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965cd82-2fa5-47a2-aefb-f8aa32f748c1",
   "metadata": {},
   "source": [
    "For this walkthrough, I used the breast cancer dataset from scikit-learn (`sklearn.datasets.load_breast_cancer()`), which is a binary classificaiton problem. For training, I split the dataset into train (50%), validation (25%) and test sets (25%). The validation set was used for model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3fda29c-d030-4411-a2e6-a10732b2f839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.33, random_state=seed)\n",
    "\n",
    "train_set = lgb.Dataset(X_train, label=y_train)\n",
    "valid_set = lgb.Dataset(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3087a0-d3a5-461d-82f9-34073db6f22c",
   "metadata": {},
   "source": [
    "Instead of `lightgbm.LGBMClassifier`, the scikit-learn API, we use the native LightGBM (`lightgbm.train`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b84ac5-ac42-457f-b1b5-84ff21ddbef5",
   "metadata": {},
   "source": [
    "## LightGBM autologging for a single training run (no tuning)\n",
    "\n",
    "To test the limits of autologging and make things more interesting, I set up the following:\n",
    "- Apply an early-stopping callback\n",
    "- Track two types of metrics: log-loss (`\"binary_logloss\"`) and AUROC (`\"auc\"`)\n",
    "- Track two types of datasets: training and validation\n",
    "- Log test metrics in addition to the autologged metrics using `mlflow.log_metrics`\n",
    "\n",
    "and passed artbitrary hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b3ca48-0642-4007-a3c1-23b3872347e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": [\"binary_logloss\", \"auc\"],\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 1.0,\n",
    "    \"seed\": seed,\n",
    "    \"num_iterations\": 10,\n",
    "    \"early_stopping_round\": 5,\n",
    "    \"first_metric_only\": True,\n",
    "    \"force_col_wise\":True,\n",
    "    \"verbosity\": -1,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29875de3-ddac-4c67-9673-cb62634d446e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/mlflow_tune/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's binary_logloss: 0.587637\ttrain's auc: 0.986343\tval's binary_logloss: 0.577365\tval's auc: 0.942092\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\ttrain's binary_logloss: 0.525352\ttrain's auc: 0.989106\tval's binary_logloss: 0.52163\tval's auc: 0.963598\n",
      "[3]\ttrain's binary_logloss: 0.473652\ttrain's auc: 0.990591\tval's binary_logloss: 0.47546\tval's auc: 0.978383\n",
      "[4]\ttrain's binary_logloss: 0.427402\ttrain's auc: 0.992129\tval's binary_logloss: 0.428912\tval's auc: 0.983647\n",
      "[5]\ttrain's binary_logloss: 0.388029\ttrain's auc: 0.994553\tval's binary_logloss: 0.392357\tval's auc: 0.985551\n",
      "[6]\ttrain's binary_logloss: 0.355106\ttrain's auc: 0.995543\tval's binary_logloss: 0.361549\tval's auc: 0.986335\n",
      "[7]\ttrain's binary_logloss: 0.323011\ttrain's auc: 0.996247\tval's binary_logloss: 0.330934\tval's auc: 0.990031\n",
      "[8]\ttrain's binary_logloss: 0.297144\ttrain's auc: 0.996247\tval's binary_logloss: 0.309573\tval's auc: 0.990255\n",
      "[9]\ttrain's binary_logloss: 0.272297\ttrain's auc: 0.996508\tval's binary_logloss: 0.286207\tval's auc: 0.990927\n",
      "[10]\ttrain's binary_logloss: 0.250728\ttrain's auc: 0.996455\tval's binary_logloss: 0.265777\tval's auc: 0.991823\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\ttrain's binary_logloss: 0.250728\ttrain's auc: 0.996455\tval's binary_logloss: 0.265777\tval's auc: 0.991823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 23:00:57 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/opt/anaconda3/envs/mlflow_tune/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"lgb_single\") as run:\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=train_set,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=5), lgb.log_evaluation()],\n",
    "        valid_sets=[train_set, valid_set],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "    )\n",
    "    \n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    loss = log_loss(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"test-logloss\":loss,\n",
    "            \"test-auc\": roc_auc,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b289dea-5393-4443-b73c-8e17a6bdb298",
   "metadata": {},
   "source": [
    "When training was done, the UI showed the autologged metrics such as feature importance scores and plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c468e8-796c-4aa4-9dde-e903ae65893e",
   "metadata": {},
   "source": [
    "![](images/lightgbm_autologging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c1e1f-f6ba-45fe-9c0d-db100d082352",
   "metadata": {},
   "source": [
    "The UI also shoed other metrics I defined when setting up the training. This information is under \"Metrics\" section in the run. When I selected `train-binary_logloss`, it showed a log-loss vs. iteration curve. I could overlay `val-binary_logloss` on top of it, which would be useful to identify model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9db2c0-b642-4e3f-8019-9656602560b7",
   "metadata": {},
   "source": [
    "![](images/train-val-binary_logloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931d655-cf25-4ab8-aa0f-146e00e97f58",
   "metadata": {},
   "source": [
    "I could fetch all logged metrics via `mlflow.client.MlflowClient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf1bf8c-23fd-4ced-b627-d29a49f2ea33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train-auc': 0.9964553794829024, 'train-binary_logloss': 0.2507277280941103, 'val-binary_logloss': 0.2657767821072834, 'test-auc': 0.9735537190082645, 'stopped_iteration': 10.0, 'val-auc': 0.9918234767025089, 'best_iteration': 10.0, 'test-logloss': 0.30723647532041254}\n"
     ]
    }
   ],
   "source": [
    "mlflow_client = MlflowClient()\n",
    "run_id = run.info.run_id\n",
    "mlflow_run = mlflow_client.get_run(run_id)\n",
    "print(mlflow_run.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5662d9-fcea-401f-b653-79e71499db74",
   "metadata": {
    "tags": []
   },
   "source": [
    "This confirms that with `mlflow.lightgbm.autolog`, the following metrics were logged in the UI:\n",
    "- Optimization loss over iterations\n",
    "- Metrics from train and validation datasets\n",
    "- Feature importance scores and plots\n",
    "- Additional metrics logged by `mlflow.log_metrics`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05705c1a-5f8d-45fb-8953-6a0b5dbd5a6a",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and MLflow autologging\n",
    "After some testing, I learned that the autologging behavior changed depending on tuner and autologging types. I tested scikit-learn and LightGBM autologging, and scikit-learn and Dask-ML tuners. This resulted in the following four combinations to test:\n",
    "\n",
    "| Test # | LightGBM autologging | scikit-learn autologging | Tuner backend |\n",
    "|--|--|--|--|\n",
    "|1|No|Yes|scikit-learn|\n",
    "|2|No|Yes|dask-ml|\n",
    "|3|Yes|Yes|scikit-learn|\n",
    "|4|Yes|Yes|dask-ml|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220df15-2953-4d02-b326-f7e6a0d99e7e",
   "metadata": {},
   "source": [
    "### Test 1. sklearn autolog and sklearn tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e891c-a808-4a06-b614-b2744693dbb0",
   "metadata": {},
   "source": [
    "To reduce the interaction btw `mlflow.lightgbm.autolog` and `mlflow.sklearn.autolog`, I turned the former first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ab5e10-3192-4d76-beaa-4325b511fedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.lightgbm.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6daf717-37b6-4348-a2f8-e9005b9aabc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(max_tuning_runs=None) # log all runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec9a4ca-8076-4a4a-b125-d24d2a7aa052",
   "metadata": {},
   "source": [
    "Here, I also used `PredefinedSplit` instead of k-fold to match the datasets for a hyperparameter search and evaluation parameters in LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033d7833-cbfb-40e3-b0d4-b8f5da4a2349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed)\n",
    "\n",
    "n_val_size = 100\n",
    "n_train_size = X_train_val.shape[0] - n_val_size\n",
    "ps = PredefinedSplit(test_fold=[0]*n_val_size + [-1]*n_train_size)\n",
    "\n",
    "for train_index, val_index in ps.split():\n",
    "    X_train = X_train_val[train_index, :]\n",
    "    X_val = X_train_val[val_index, :]\n",
    "    y_train = y_train_val[train_index]\n",
    "    y_val = y_train_val[val_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f7d4d-e0f6-465f-b8fc-581a9ad6e4b4",
   "metadata": {},
   "source": [
    "Additionally, to be consistent with the autologging and tuner types, I used the scikit-learn API version of LightGBM (`LGBMClassifier`). For this tuning example, I chose `learning_rate` and `subsample` hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82c861f2-d74a-4792-ba0a-e80460bcd864",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tval's binary_logloss: 0.624749\n",
      "[2]\tval's binary_logloss: 0.568357\n",
      "[3]\tval's binary_logloss: 0.520761\n",
      "[4]\tval's binary_logloss: 0.480421\n",
      "[5]\tval's binary_logloss: 0.442099\n",
      "[6]\tval's binary_logloss: 0.410214\n",
      "[7]\tval's binary_logloss: 0.382292\n",
      "[8]\tval's binary_logloss: 0.357785\n",
      "[9]\tval's binary_logloss: 0.335433\n",
      "[10]\tval's binary_logloss: 0.31392\n",
      "[1]\tval's binary_logloss: 0.637054\n",
      "[2]\tval's binary_logloss: 0.589451\n",
      "[3]\tval's binary_logloss: 0.547508\n",
      "[4]\tval's binary_logloss: 0.511241\n",
      "[5]\tval's binary_logloss: 0.478765\n",
      "[6]\tval's binary_logloss: 0.448812\n",
      "[7]\tval's binary_logloss: 0.423736\n",
      "[8]\tval's binary_logloss: 0.398193\n",
      "[9]\tval's binary_logloss: 0.377066\n",
      "[10]\tval's binary_logloss: 0.356542\n",
      "[1]\tval's binary_logloss: 0.653833\n",
      "[2]\tval's binary_logloss: 0.618878\n",
      "[3]\tval's binary_logloss: 0.586995\n",
      "[4]\tval's binary_logloss: 0.558061\n",
      "[5]\tval's binary_logloss: 0.531579\n",
      "[6]\tval's binary_logloss: 0.507355\n",
      "[7]\tval's binary_logloss: 0.485109\n",
      "[8]\tval's binary_logloss: 0.463636\n",
      "[9]\tval's binary_logloss: 0.444977\n",
      "[10]\tval's binary_logloss: 0.427695\n",
      "[1]\tval's binary_logloss: 0.620853\n",
      "[2]\tval's binary_logloss: 0.56225\n",
      "[3]\tval's binary_logloss: 0.512275\n",
      "[4]\tval's binary_logloss: 0.464753\n",
      "[5]\tval's binary_logloss: 0.426894\n",
      "[6]\tval's binary_logloss: 0.390463\n",
      "[7]\tval's binary_logloss: 0.358157\n",
      "[8]\tval's binary_logloss: 0.330555\n",
      "[9]\tval's binary_logloss: 0.305165\n",
      "[10]\tval's binary_logloss: 0.282671\n"
     ]
    }
   ],
   "source": [
    "n_search = 3\n",
    "\n",
    "with mlflow.start_run(run_name=\"test_1\") as run:\n",
    "    \n",
    "    clf = lgb.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        metric=\"binary_logloss\",\n",
    "        seed=seed,\n",
    "        class_weight=\"balanced\",\n",
    "        n_estimators=10,\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline([(\"clf\", clf)])\n",
    "    param_space = {\n",
    "        \"clf__learning_rate\": np.linspace(0.05, 0.1, 10),\n",
    "        \"clf__subsample\": np.linspace(0.1, 1, 10),\n",
    "    }\n",
    "    \n",
    "    search_cv = RandomizedSearchCV(pipe, param_space, cv=ps, n_iter=n_search)\n",
    "    search_cv.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        clf__eval_set=[(X_val, y_val)],\n",
    "        clf__eval_names=['val'],\n",
    "        clf__eval_metric=['binary_logloss'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5535e6c-4f30-48d3-8851-901604b8afab",
   "metadata": {},
   "source": [
    "The UI showed that 1 parent run and `n_search` (3) child runs were created, where the parent run had the autologged metrics such as confusion matrix, ROC curve, and PR curve:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7796e8f-aaa0-4fab-be46-a296f230358c",
   "metadata": {},
   "source": [
    "![](images/test1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663444aa-1020-4eaf-b901-ec95dba3d855",
   "metadata": {},
   "source": [
    "`cv_results` was also returned and the logged metrics from all child runs were similar to `cv_results`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8bc85-c704-4e0b-9c26-7c6d6324373e",
   "metadata": {},
   "source": [
    "### Test 2. sklearn autolog and dask-ml tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bbff4d0-39d3-4832-8711-1e82a11916d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client(processes=False, threads_per_worker=4, n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc7954c5-4d19-4b2c-94fc-8d2b25f8dd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tval's binary_logloss: 0.6207[1]\tval's binary_logloss: 0.6207\n",
      "[1]\tval's binary_logloss: 0.64539\n",
      "\n",
      "[2]\tval's binary_logloss: 0.603905\n",
      "[2]\tval's binary_logloss: 0.561648\n",
      "[2]\tval's binary_logloss: 0.561648\n",
      "[3]\tval's binary_logloss: 0.566663\n",
      "[3]\tval's binary_logloss: 0.51229\n",
      "[3]\tval's binary_logloss: 0.51229\n",
      "[4]\tval's binary_logloss: 0.533792\n",
      "[4]\tval's binary_logloss: 0.470843\n",
      "[4]\tval's binary_logloss: 0.470843\n",
      "[5]\tval's binary_logloss: 0.504106\n",
      "[5]\tval's binary_logloss: 0.43163\n",
      "[5]\tval's binary_logloss: 0.43163\n",
      "[6]\tval's binary_logloss: 0.476199\n",
      "[6]\tval's binary_logloss: 0.399279\n",
      "[6]\tval's binary_logloss: 0.399279\n",
      "[7]\tval's binary_logloss: 0.452345\n",
      "[7]\tval's binary_logloss: 0.371128\n",
      "[7]\tval's binary_logloss: 0.371128\n",
      "[8]\tval's binary_logloss: 0.430183\n",
      "[8]\tval's binary_logloss: 0.345312\n",
      "[8]\tval's binary_logloss: 0.345312\n",
      "[9]\tval's binary_logloss: 0.409486\n",
      "[9]\tval's binary_logloss: 0.323905\n",
      "[9]\tval's binary_logloss: 0.323905\n",
      "[10]\tval's binary_logloss: 0.388993\n",
      "[10]\tval's binary_logloss: 0.303382\n",
      "[10]\tval's binary_logloss: 0.303382\n",
      "[1]\tval's binary_logloss: 0.642694\n",
      "[2]\tval's binary_logloss: 0.599408\n",
      "[3]\tval's binary_logloss: 0.5599\n",
      "[4]\tval's binary_logloss: 0.525228\n",
      "[5]\tval's binary_logloss: 0.490788\n",
      "[6]\tval's binary_logloss: 0.461964\n",
      "[7]\tval's binary_logloss: 0.433798\n",
      "[8]\tval's binary_logloss: 0.409936\n",
      "[9]\tval's binary_logloss: 0.385757\n",
      "[10]\tval's binary_logloss: 0.365051\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"test_2\") as run:\n",
    "    \n",
    "    search_cv = dask_RandomizedSearchCV(pipe, param_space, cv=ps, n_iter=n_search)\n",
    "    search_cv.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        clf__eval_set=[(X_val, y_val)],\n",
    "        clf__eval_names=[\"val\"],\n",
    "        clf__eval_metric=[\"binary_logloss\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968390d2-f863-4ca5-846f-f8746d5f6970",
   "metadata": {},
   "source": [
    "`mlflow.sklearn.autolog` still created confusion matrix, ROC curve, and PR curve but **only a single run is returned,** and all child runs are now missing. Besides, the UI also didn't log `cv_results`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837048c-21b7-4ebc-8de8-a614ed3e31d0",
   "metadata": {},
   "source": [
    "<img src=\"images/test2.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3128de4-1621-4a45-884e-83b2e5219e90",
   "metadata": {},
   "source": [
    "#### Is the only logged run the best run?\n",
    "Here was where the behavior of `mlflow.sklearn.autolog` changed. It was supposed to return a single parent run and multiple child runs but when `dask-ml` was used as tuner, it only logged a single run. I didn't know whether thi was the best run or not, so I decided to compare the MLflow logged result with the actual search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2977fc6e-2d8b-4e50-9201-84d886a080cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow_run = mlflow_client.get_run(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a31b2c59-a15e-41d1-9c6f-d10948db8417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert str(search_cv.best_estimator_.get_params()['clf__learning_rate']) == \\\n",
    "mlflow_run.data.params['clf__learning_rate']\n",
    "assert str(search_cv.best_estimator_.get_params()['clf__subsample']) == \\\n",
    "mlflow_run.data.params['clf__subsample']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735ddb1a-8153-46eb-9bcf-5fe5026acf91",
   "metadata": {},
   "source": [
    "Luckily, the assertions have passed, meaning that the single recorded run by MLflow was the best run. Except that users can't see the child runs in the UI, this behavior seems acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9c581-c9de-4454-afa5-773615931407",
   "metadata": {},
   "source": [
    "### Test 3. lightgbm+sklearn autolog and sklearn tuner \n",
    "This test idea came to my mind becasue I imagined it would be very convenient if one could use autologging on top of a sklearn tuner. Thus, I decided to turn on lightgbm autologging in addition to the sklearn autologging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "388183c5-ffaf-4d93-bf23-ce499ed243d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow.lightgbm.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a629ef7-4a01-406e-af8a-eb6ccd74353f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tval's binary_logloss: 0.64539\n",
      "[2]\tval's binary_logloss: 0.603905\n",
      "[3]\tval's binary_logloss: 0.566663\n",
      "[4]\tval's binary_logloss: 0.533792\n",
      "[5]\tval's binary_logloss: 0.504106\n",
      "[6]\tval's binary_logloss: 0.476199\n",
      "[7]\tval's binary_logloss: 0.452345\n",
      "[8]\tval's binary_logloss: 0.430183\n",
      "[9]\tval's binary_logloss: 0.409486\n",
      "[10]\tval's binary_logloss: 0.388993\n",
      "[1]\tval's binary_logloss: 0.6207\n",
      "[2]\tval's binary_logloss: 0.561648\n",
      "[3]\tval's binary_logloss: 0.51229\n",
      "[4]\tval's binary_logloss: 0.470843\n",
      "[5]\tval's binary_logloss: 0.43163\n",
      "[6]\tval's binary_logloss: 0.399279\n",
      "[7]\tval's binary_logloss: 0.371128\n",
      "[8]\tval's binary_logloss: 0.345312\n",
      "[9]\tval's binary_logloss: 0.323905\n",
      "[10]\tval's binary_logloss: 0.303382\n",
      "[1]\tval's binary_logloss: 0.632926\n",
      "[2]\tval's binary_logloss: 0.582412\n",
      "[3]\tval's binary_logloss: 0.538314\n",
      "[4]\tval's binary_logloss: 0.500565\n",
      "[5]\tval's binary_logloss: 0.467006\n",
      "[6]\tval's binary_logloss: 0.436234\n",
      "[7]\tval's binary_logloss: 0.410759\n",
      "[8]\tval's binary_logloss: 0.384813\n",
      "[9]\tval's binary_logloss: 0.361938\n",
      "[10]\tval's binary_logloss: 0.342656\n",
      "[1]\tval's binary_logloss: 0.642694\n",
      "[2]\tval's binary_logloss: 0.599408\n",
      "[3]\tval's binary_logloss: 0.5599\n",
      "[4]\tval's binary_logloss: 0.525228\n",
      "[5]\tval's binary_logloss: 0.490788\n",
      "[6]\tval's binary_logloss: 0.461964\n",
      "[7]\tval's binary_logloss: 0.433798\n",
      "[8]\tval's binary_logloss: 0.409936\n",
      "[9]\tval's binary_logloss: 0.385757\n",
      "[10]\tval's binary_logloss: 0.365051\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='test_3') as run:\n",
    "    \n",
    "    search_cv = RandomizedSearchCV(pipe, param_space, cv=ps, n_iter=n_search)\n",
    "    search_cv.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        clf__eval_set=[(X_val, y_val)],\n",
    "        clf__eval_names=[\"val\"],\n",
    "        clf__eval_metric=[\"binary_logloss\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb7308-7584-467a-85f5-9f2ae94ab097",
   "metadata": {},
   "source": [
    "This time, I found that sklearn autologging behaved normally but lightgbm autologging didn't work at all. First, lightgbm autologging metrics such as feature importance scores were missing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e929c5d-7915-4ed4-a72e-75378cf47742",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/test3.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7078b871-f4b5-42fd-9050-224b74d8dd65",
   "metadata": {},
   "source": [
    "Second, `training-log_loss` wasn't logged for every iteration but it was logged as a single numeric value, and thus was visualized as a bar graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4527a-5f3c-4535-8105-9e18cea31723",
   "metadata": {},
   "source": [
    "![](images/test3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7c020-38b0-4ba5-86af-2d12d371389c",
   "metadata": {},
   "source": [
    "### Test 4. lightgbm+sklearn autolog and dask-ml tuner\n",
    "Finally, I used the dask-ml tuner, lightgbm and sklearn autologging altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d789316-6f32-425b-8082-c3cf605d3df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tval's binary_logloss: 0.632926[1]\tval's binary_logloss: 0.653833\n",
      "\n",
      "[1]\tval's binary_logloss: 0.632926\n",
      "[2]\tval's binary_logloss: 0.582412\n",
      "[2]\tval's binary_logloss: 0.618878\n",
      "[2]\tval's binary_logloss: 0.582412\n",
      "[3]\tval's binary_logloss: 0.586995\n",
      "[3]\tval's binary_logloss: 0.538314\n",
      "[3]\tval's binary_logloss: 0.538314\n",
      "[4]\tval's binary_logloss: 0.558061\n",
      "[4]\tval's binary_logloss: 0.500565\n",
      "[4]\tval's binary_logloss: 0.500565\n",
      "[5]\tval's binary_logloss: 0.531579\n",
      "[5]\tval's binary_logloss: 0.467006\n",
      "[5]\tval's binary_logloss: 0.467006\n",
      "[6]\tval's binary_logloss: 0.507355\n",
      "[6]\tval's binary_logloss: 0.436234\n",
      "[6]\tval's binary_logloss: 0.436234\n",
      "[7]\tval's binary_logloss: 0.485109\n",
      "[7]\tval's binary_logloss: 0.410759\n",
      "[7]\tval's binary_logloss: 0.410759\n",
      "[8]\tval's binary_logloss: 0.463636\n",
      "[8]\tval's binary_logloss: 0.384813\n",
      "[8]\tval's binary_logloss: 0.384813\n",
      "[9]\tval's binary_logloss: 0.444977\n",
      "[9]\tval's binary_logloss: 0.361938\n",
      "[9]\tval's binary_logloss: 0.361938\n",
      "[10]\tval's binary_logloss: 0.427695\n",
      "[10]\tval's binary_logloss: 0.342656\n",
      "[10]\tval's binary_logloss: 0.342656\n",
      "[1]\tval's binary_logloss: 0.651621\n",
      "[2]\tval's binary_logloss: 0.615177\n",
      "[3]\tval's binary_logloss: 0.581242\n",
      "[4]\tval's binary_logloss: 0.550942\n",
      "[5]\tval's binary_logloss: 0.522853\n",
      "[6]\tval's binary_logloss: 0.494685\n",
      "[7]\tval's binary_logloss: 0.470868\n",
      "[8]\tval's binary_logloss: 0.447008\n",
      "[9]\tval's binary_logloss: 0.42506\n",
      "[10]\tval's binary_logloss: 0.406164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 23:02:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during sklearn autologging: The following failures occurred while performing one or more logging operations: [MlflowException('Failed to perform one or more operations on the run with ID 4f0ea51beb0748139aa4364c5d332279. Failed operations: [MlflowException(\"API request to http://127.0.0.1:5000/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host=\\'127.0.0.1\\', port=5000): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError(\\'too many 500 error responses\\'))\")]')]\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='test_4') as run:\n",
    "    \n",
    "    search_cv = dask_RandomizedSearchCV(pipe, param_space, cv=ps, n_iter=n_search)\n",
    "    search_cv.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        clf__eval_set=[(X_val, y_val)],\n",
    "        clf__eval_names=[\"val\"],\n",
    "        clf__eval_metric=[\"binary_logloss\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b629be-7222-448f-abf5-0948893be37b",
   "metadata": {},
   "source": [
    "This time, similar to Test 2, a single run was returned but it seemed that lightgbm autologging actually worked because the UI generated images from both sklearn and lightgbm autologging methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e806085-bfbc-408c-bb05-a742b9b7689a",
   "metadata": {},
   "source": [
    "<img src=\"images/test4.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f324008-d3e5-4af9-b592-5ff4d4addea3",
   "metadata": {},
   "source": [
    "However, only single run was returned, again like in Test 2. Unfortunately, this time, this single run didn't pass the assertion test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fbc0c8d-a207-4fbc-a084-e7e86a1a8b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlflow_run = mlflow_client.get_run(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "527128e5-6c5e-4e62-8e0f-7616560f7701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(search_cv\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mget_params()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf__learning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \\\n\u001b[1;32m      2\u001b[0m mlflow_run\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(search_cv\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mget_params()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf__subsample\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \\\n\u001b[1;32m      4\u001b[0m mlflow_run\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert str(search_cv.best_estimator_.get_params()['clf__learning_rate']) == \\\n",
    "mlflow_run.data.params['learning_rate']\n",
    "assert str(search_cv.best_estimator_.get_params()['clf__subsample']) == \\\n",
    "mlflow_run.data.params['subsample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d93e952b-4be8-46d7-9afd-7176dbc5ace4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 0.07777777777777778\n",
      "0.5 0.7000000000000001\n"
     ]
    }
   ],
   "source": [
    "print(str(search_cv.best_estimator_.get_params()['clf__learning_rate']),\n",
    "      mlflow_run.data.params['learning_rate'])\n",
    "print(str(search_cv.best_estimator_.get_params()['clf__subsample']),\n",
    "      mlflow_run.data.params['subsample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d10aac-ce5a-4383-a8a6-00128e8c7a28",
   "metadata": {},
   "source": [
    "This means that when dask-ml, sklearn autolog, and lightgbm autologgin are used at once, we cannot trust the MLflow tracking UI becasue the single set of represented hyperparameters in the UI are not the best estimator's hyperparameters. This means this combination gives unreliable results, which we should avoid at all costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4ff5db1-d59e-4445-adfc-be8a9f9c5594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb2b5f-868b-4595-a074-164cdef3a5da",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0265c-737a-40c1-8561-da569e81a1de",
   "metadata": {},
   "source": [
    "In this notebook, I demonstrated how different combinations of autologging and tuners could produce different results. Some of these changed behaviors were simple omissions but I found a more troubling combination as well where the results were just simply wrong. This suggests that when it comes to testing interoperability, we should not only check whether they work together but also whether the returned results are accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
