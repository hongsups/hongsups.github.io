---
title: Thompson sampling 
author: Hongsup Shin
date: 2025-01-18
description: None
categories: [ML, verification]
---

I have been doing research and building ML application to increase efficiency of hardware verification process. My main project has been finding bugs efficiently while exploring the vast hardware design space. Naturally, this is often formulated as an exploration-exploitation problem. Recently, I had a discussion with a colleague, who suggested a multi-armed bandit (MAB) approach to balance the load between exploration and exploitation. I proposed Thompson sampling, as a first approach to investigate this further because its simple, straightforward, and handles the load balancing elegantly.

## Premise

As I described in my previous papers, in our ML application, we train a supervised learning model to recommend top-k test candidates that are more bug-prone. Our ML application powered by this model runs along with the default random-constraint verification process, which randomly explores a bounded design space set by verification engineers. In short, the ML arm exploits historic data while the random arm continues exploration. Currently, the compute (load) allocated to the ML arm is fixed. An MAB approach can control the load based on model performance in a data-driven way.

## Thompson sampling

Thompson sampling uses probabilistic reward functions for choosing actions in a MAB setting. In a vanilla setup, we start with two Beta(1, 1) distribution (uniform) and draw a random sample from each distribution. We then select the arm with the higher value, and update the parameters based on the outcome. Thus, it naturally addresses the uncertainty through the probabilistic distribution update.

If we have two arms with binary outcomes (successes and failures or bugs and no bugs), we can model each arm's reward as a beta distribution. In a beta distribution, $\alpha$ and $\beta$ represent the number of successes + 1, and failures + 1, and thus the parameter update is straightforward.

This helps maintain exploration (of arms) through probabilistic sampling, and the exploration gradually decreases as we sample more data (i.e., the distribution of both arms become narrower). It is simple to implement, and we don't have to manually control the number of times each arm should be pulled.

```python
import numpy as np
from scipy.stats import beta

class ThompsonSampling:
    def __init__(self):
        # Initialize the beta distribution for both arms (uniform)
        self.alphas = [1, 1] 
        self.betas = [1, 1] 
    
    def select_arm(self):
        # Sample from both arms and return the index of the better arm
        samples = [beta.rvs(a, b, size=1)[0] for a, b in zip(self.alphas, self.betas)]
        return np.argmax(samples)

    def action(self, chosen_arm):
        # Pull the chosen arm and collect the reward
        if chosen_arm == 0:
            reward = PullArm0()
        else:
            reward = PullArm1()
        return reward
    
    def update(self):
        # Update the chosen arm's parameters based on the reward
        chosen_arm = self.select_arm()
        reward = self.action(chosen_arm)        
        if reward == 1:
            self.alphas[chosen_arm] += 1
        else:
            self.betas[chosen_arm] += 1
```

Now let's discuss some modified versions of Thomson sampling to address practical challenges.

## Reducing overconfidence of arms

When statistics literature refers to exploration-exploitation dilemma in MAB, the exploration refers to exploring different arms. In our case, the supervised arm represents exploitation and the random arm represents exploration of a *design space*, so note that the definition of exploration is different here. As we collect more data, the supervised arm can start dominating and the majority of the load is directed to this arm. This poses high risk in a stage where bugs are extremely rare because finding novel bugs here mainly requires exploration.

There are several ways to handle this. The simplest is to set *minimum exploration rate*. This guarantees a fixed amount of load dedicated to a specific ram, the exploration arm in our case.

```python
class ThompsonWithMinExplore:
    def __init__(self, ..., min_explore_rate=0.2):
        self.min_explore_rate = min_explore_rate

    def select_arm(self):

        if np.random.random() < self.min_explore_rate:            
            return 1 # always return a specific arm index

        samples = [beta.rvs(a, b, size=1)[0] for a, b in zip(self.alphas, self.betas)]
        return np.argmax(samples)        
```

We can also manipulate the probability distribution of the dominant arm. Setting upper bounds of the parameters prevent their updates once they reach the threshold. 

```python
class ThompsonWithUpperBound:

    def update(self, alpha_upper_bound, beta_upper_bound):
        chosen_arm = self.select_arm()
        reward = self.action(chosen_arm)        
        if reward == 1:
            if chosen_arm == 0 and self.alphas[0] < alpha_upper_bound:
                self.alphas[0] += 1
            elif chosen_arm == 1:
                self.alphas[1] += 1
        else:
            if chosen_arm == 0 and self.betas[0] < beta_upper_bound:
                self.betas[0] += 1
            elif chosen_arm == 1:
                self.betas[1] += 1
```

Similarly, setting a time-decay of parameters prevents any arms from being overly confident. We can consider this as gradually reducing the confidence of older observations.

```python
class ThompsonWithTimeDecay:
    def __init__(self, decay_rate):
        self.decay_rate = decay_rate
        
    def update(self):
        chosen_arm = self.select_arm()
        reward = self.action(chosen_arm)        

        # Decay before update
        self.alphas = [a * self.decay_rate for a in self.alphas]
        self.betas = [b * self.decay_rate for b in self.betas]        
        if reward == 1:
            self.alphas[chosen_arm] += 1
        else:
            self.betas[chosen_arm] += 1
```

## Dealing with rare events

Another common challenge rises when reward frequency is too low. In the binary reward setup, low frequency produces a beta distribution that is highly compressed towards zero with a long tail on the right side. 

## Starting with previous knowledge

## Using non-binary reward

## Batch Thompson sampling